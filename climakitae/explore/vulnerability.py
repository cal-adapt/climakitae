"""Tools for CAVA vulnerability assessment pilot"""

import warnings
import xarray as xr
import numpy as np
import pandas as pd
import param

from climakitae.util.utils import (
    get_closest_gridcell,
    add_dummy_time_to_wl,
    stack_sims_across_locs,
)
from climakitae.core.data_export import export
from climakitae.core.data_interface import DataParameters
from climakitae.core.data_load import load
from climakitae.explore import warming_levels
from climakitae.explore.threshold_tools import get_block_maxima, get_return_value
from climakitae.tools.batch import batch_select
from dask.array.core import PerformanceWarning

# Ignore specific warnings about division and performance to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")
warnings.filterwarnings("ignore", category=PerformanceWarning)


def _export_no_e(da, filename, format):
    """Exports a file but doesn't throw an exception if a file already exists."""
    try:
        export(
            da,
            filename=filename,
            format=format,
        )
    except Exception as e:
        print(e)


def _clean_wl_data(data, downscaling_method, separate_files):
    """Cleans WL data to conform with time-based data for rest of function."""
    # Remove SSP descriptions
    if separate_files:
        data["all_sims"] = [
            sim_name.item().split("--")[0].strip() for sim_name in data.all_sims
        ]
    else:
        sim_names = []
        for sim_name in data.all_sims:
            split_vals = sim_name.item().split("--")
            _, lat, lon = split_vals[1].split("_")
            new_name = f"{split_vals[0].strip()}_{lat}_{lon}"
            sim_names.append(new_name)
        data["all_sims"] = sim_names
    data = data.rename({"all_sims": "simulation"})
    return data


def _filter_ba_models(data, downscaling_method, wrf_bias_adjust, historical_data):
    """Filters the data for the bias adjusted simulations, if desired."""
    if (
        downscaling_method == "Dynamical"
        and wrf_bias_adjust
        and historical_data != "Historical Reconstruction"
    ):
        # Filter only for BC-WRF models
        # This check is to see which simulations have these BC models as the root part of their simulation name, which is to accomodate for simulation names being the same across SSPs.
        # Not sure why this is selecting the right sims for batch mode but not leaving the lat/lons on their names
        data = data.sel(
            simulation=[
                sim
                for sim in data.simulation.values
                if any(
                    s in sim
                    for s in [
                        "WRF_EC-Earth3_r1i1p1f1",
                        "WRF_MPI-ESM1-2-HR_r3i1p1f1",
                        "WRF_TaiESM1_r1i1p1f1",
                        "WRF_MIROC6_r1i1p1f1",
                    ]
                )
            ]
        )
    return data


def _metric_agg(
    da, approach, metric, heat_idx_threshold, one_in_x, percentile, name_of_calc, distr
):
    """
    Calculate a metric on the given DataArray (da) using the specified metric type
    and heat index threshold.

    Parameters:
    -----------
    da : xarray.DataArray
        Input DataArray to compute metric and treshold on.
    metric : str
        The metric type to calculate. Supported options are "min", "max".
    heat_idx_threshold : float
        The heat index threshold for comparison.
    one_in_x : int
        Return period for 1-in-X events.
    percentile : int
        Percentile / quantile for calculating "likely".

    Returns:
    --------
    metric_data : xarray.DataArray
        Resulting DataArray from the calculated metric and heat index threshold applied.
    """
    metric_map = {
        "min": np.min,
        "max": np.max,
    }

    def _apply_metric(da):
        """Applies the hard-coded heat index threshold or percentile to the DataArray"""
        if (
            heat_idx_threshold or heat_idx_threshold == 0
        ):  # If a threshold is passed and/or if the `heat_idx_threshold` passed in is 0, since that is a valid threshold.
            calc_val = (
                (da.resample(time="1D").reduce(metric_map[metric]) > heat_idx_threshold)
                .resample(time="1Y")
                .sum()
                .mean(dim="time")
            )  # This line counts the average number of high heat days per year over this time interval

        elif one_in_x:  # Calculate 1-in-X events

            # Get 1-in-x event values
            return_vals = []
            for (
                sim
            ) in (
                da.simulation.values
            ):  # Can use xr.apply_ufunc, not just a for loop here
                ams = get_block_maxima(
                    da.sel(simulation=sim),
                    extremes_type=metric,
                    groupby=(1, "day"),
                    check_ess=False,
                ).squeeze()  # `get_return_value` is expecting a 1-D array
                return_values = get_return_value(
                    ams,
                    return_period=one_in_x,
                    multiple_points=False,
                    distr=distr,
                )["return_value"]
                return_vals.append(return_values)
            calc_val = xr.concat(return_vals, dim="simulation")

            # Changing attributes for thresholds DataArray because of export issue later on
            old_group = calc_val.attrs["groupby"]
            calc_val.attrs["groupby"] = f"{old_group[0]} {old_group[1]}"

        elif percentile or percentile == 0:

            calc_val = (
                da.resample(time="1D")
                .reduce(metric_map[metric])
                .quantile(percentile / 100, dim="time")
                .assign_coords(
                    lat=da.lat,
                    lon=da.lon,
                    quantile=percentile,  # Coords are dropped during quantile method application
                )
                .rename({"quantile": "percentile"})
            )

        calc_val.name = name_of_calc
        return calc_val

    # If the data passed in is warming level data, then a dummy timestamp must be added to act as the current WL 'time' dimension
    if approach == "warming_level":
        da = add_dummy_time_to_wl(da)

    metric_data = _apply_metric(da)
    return metric_data


class CavaParams(param.Parameterized):
    """
    Validates parameters for the `cava_data` function, and returns transformed parameters, if needed.
    Transformed parameters returned: `ssp_selected`, `variable_type`.
    """

    input_locations = param.DataFrame(
        doc="Input coordinates must have 'lat' and 'lon' columns."
    )
    time_start_year = param.Integer(bounds=(1900, 2100), doc="Start year", default=1981)
    time_end_year = param.Integer(bounds=(1900, 2100), doc="End year", default=2010)

    # Need to find all valid units by variable name
    units = param.String(default="Celsius", doc="Units for temperature measurement")

    variable = param.ObjectSelector(
        default="Air Temperature at 2m",
        objects=["Air Temperature at 2m", "Precipitation (total)", "NOAA Heat Index"],
        doc="Variable to analyze",
    )

    metric_calc = param.ObjectSelector(
        default="max",
        objects=["min", "max", "mean", "median"],
        doc="Metric calculation method",
    )
    percentile = param.Number(
        default=None, bounds=(0, 100), allow_None=True, doc="Percentile for calculation"
    )
    heat_idx_threshold = param.Number(
        default=None, allow_None=True, doc="Threshold for heat index"
    )
    one_in_x = param.Integer(default=None, allow_None=True, doc="1-in-x year event")
    season = param.ObjectSelector(
        default="all", objects=["summer", "winter", "all"], doc="Season to analyze"
    )
    downscaling_method = param.ObjectSelector(
        default="Dynamical",
        objects=["Dynamical", "Statistical"],
        doc="Downscaling method",
    )
    approach = param.ObjectSelector(
        default="time", objects=["time", "warming_level"], doc="Approach to use"
    )
    warming_level = param.ObjectSelector(
        default="1.5",
        objects=[None, "1.5", "2.0", "2.5", "3.0", "4.0"],
        doc="Warming level for analysis",
    )
    wrf_bias_adjust = param.Boolean(default=True, doc="Flag for WRF bias adjustment")
    historical_data = param.ObjectSelector(
        default="Historical Climate",
        objects=["Historical Climate", "Historical Reconstruction"],
        doc="Type of historical data",
    )
    ssp_data = param.ListSelector(
        default=["SSP3-7.0"],
        objects=["SSP2-4.5", "SSP3-7.0", "SSP5-8.5"],
        doc="SSP data scenarios to use",
    )
    export_method = param.ObjectSelector(
        default="both",
        objects=["raw", "calculate", "both", "None"],
        doc="Export method",
    )
    separate_files = param.Boolean(
        default=False,
        doc="Whether to separate climate variable information into separate files",
    )
    file_format = param.String(default="NetCDF", doc="Format of the output files")
    batch_mode = param.Boolean(default=False, doc="Flag for batch mode operation")
    distr = param.ObjectSelector(
        default="gev", objects=["gev", "genpareto"], doc="Distribution function"
    )

    def __init__(self, **params):
        super().__init__(**params)
        self.validate_params()

    def validate_params(self):
        errors = []

        # Check if input_locations is a DataFrame with lat/lon columns
        if isinstance(self.input_locations, pd.DataFrame):
            if (
                "lat" not in self.input_locations.columns
                or "lon" not in self.input_locations.columns
            ):
                errors.append("Input coordinates must have `lat` and `lon` columns.")
            elif not pd.api.types.is_numeric_dtype(
                self.input_locations["lat"]
            ) or not pd.api.types.is_numeric_dtype(self.input_locations["lon"]):
                errors.append("Input lat/lon columns must be float64 or int64 types.")

        # Validating that time start year < time end year
        if self.time_start_year > self.time_end_year:
            errors.append(
                "Start year must come before, or be the same year as, the end year."
            )

        # Validate year range for historical reconstruction
        if (
            self.historical_data == "Historical Reconstruction"
            and self.time_end_year > 2022
        ):
            errors.append(
                "End year for Historical Reconstruction data must be 2022 or earlier."
            )

        # Validate time approach with Historical Reconstruction
        if (
            self.historical_data == "Historical Reconstruction"
            and self.approach != "time"
        ):
            errors.append(
                "Historical Reconstruction data can only be retrieved using a time-based approach."
            )

        # Validating warming level inputs
        if self.approach == "warming_level" and self.warming_level not in [
            "1.5",
            "2.0",
            "2.5",
            "3.0",
            "4.0",
        ]:
            errors.append("Warming level must be '1.5', '2.0', '2.5', '3.0', or '4.0'")

        # Validating that only one of heat_idx_threshold, percentile, or one_in_x is passed
        if (
            (self.heat_idx_threshold is not None and self.percentile is not None)
            or (self.heat_idx_threshold is not None and self.one_in_x is not None)
            or (self.percentile is not None and self.one_in_x is not None)
        ):
            errors.append(
                "Only one of heat_idx_threshold, one_in_x, and percentile can be non-None."
            )

        # Check that not all 3 customizable metric arguments are none.
        if (
            self.heat_idx_threshold is None
            and self.percentile is None
            and self.one_in_x is None
        ):
            errors.append(
                "`heat_idx_threshold`, `percentile`, and `one_in_x` cannot all be None."
            )

        # Make sure the metric is 'min' or 'max' if percentile is passed in
        if self.percentile is not None and self.metric_calc not in ["min", "max"]:
            errors.append(
                "Metric calculation must be 'min' or 'max' for percentile calculations."
            )

        # Validating variable and downscaling method combo
        if (
            self.variable == "NOAA Heat Index"
            and self.downscaling_method == "Statistical"
        ):
            errors.append(
                "`NOAA Heat Index` cannot be used with Statistical downscaling method."
            )

        # Only allow for SSP 3-7.0 for WRF data
        if self.downscaling_method == "Dynamical" and self.approach == "time":
            if len(self.ssp_data) > 1 or self.ssp_data[0] != "SSP3-7.0":
                errors.append(
                    "Can only use SSP 3-7.0 data for a time-based approach using WRF data."
                )

        # Raise errors if any
        if errors:
            raise ValueError("Parameter validation errors:\n\n" + "\n".join(errors))

    def get_names(self):

        # Re-assigning `variable` in case it needs to be re-instantiated later.
        variable = self.variable

        # Determine variable type
        if variable in ["Air Temperature at 2m", "Precipitation (total)"]:
            variable_type = "Variable"
        else:
            variable_type = "Derived Index"

        # Adjust variable name for LOCA2 data
        if (
            variable == "Air Temperature at 2m"
            and self.downscaling_method == "Statistical"
        ):
            if self.metric_calc == "max":
                variable = "Maximum air temperature at 2m"
            elif self.metric_calc == "min":
                variable = "Minimum air temperature at 2m"

        # Adding time period or warming level to filename
        if self.approach == "time":
            approach_str = f"{self.time_start_year}_to_{self.time_end_year}"
            approach_var_name = f"from {approach_str}"
        elif self.approach == "warming_level":
            wl_str = (
                str(int(float(self.warming_level)))
                if self.warming_level not in ["1.5", "2.5"]
                else self.warming_level.replace(".", "pt")
            )
            approach_str = f"{wl_str}degreeWL"
            approach_var_name = f"for Warming Level {self.warming_level}°C"

        # Retrieving the name of the calculation about to occur
        if self.percentile is not None:

            def ordinal(n):
                """Find ordinal name for a number (1 -> 1st, 2 -> 2nd, 3 -> 3rd)"""
                return f"{n}{('th' if 11 <= n % 100 <= 13 else {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th'))}"

            var_name = f"{ordinal(self.percentile)} percentile of Daily {self.metric_calc.capitalize()} of {variable} for {self.season} seasons {approach_var_name}"
            raw_name = "likely_seasonal_temperature_raw_data"
            calc_name = (
                f"likely_seasonal_{self.season}_{self.metric_calc}_{approach_str}"
            )

        elif self.heat_idx_threshold is not None:
            var_name = f"Days per year with {self.metric_calc.capitalize()} Daily {variable} above a heat index threshold of {self.heat_idx_threshold} {self.units} for {self.season} seasons {approach_var_name}"
            raw_name = "heat_index_raw_data"
            calc_name = f"heat_index_thresh{self.heat_idx_threshold}_{approach_str}"

        elif self.one_in_x is not None:
            var_name = f"1-in-{self.one_in_x} year {self.metric_calc.capitalize()} {variable} for {self.season} seasons {approach_var_name}"
            raw_name = f"one_in_{self.one_in_x}_temperature_raw_data"
            calc_name = f"one_in_{self.one_in_x}_temperature_{approach_str}"

        ssp_mapping = {
            "SSP2-4.5": "SSP 2-4.5 -- Middle of the Road",
            "SSP3-7.0": "SSP 3-7.0 -- Business as Usual",
            "SSP5-8.5": "SSP 5-8.5 -- Burn it All",
        }

        return {
            "ssp_selected": [ssp_mapping[ssp] for ssp in self.ssp_data],
            "variable": variable,
            "variable_type": variable_type,
            "var_name": var_name,
            "raw_name": raw_name,
            "calc_name": calc_name,
        }


def cava_data(
    input_locations,  # csv file
    variable,  # must eventually accept temp, precip, or heat index
    units=None,
    approach="time",  # default for now
    downscaling_method="Dynamical",  # default for now
    time_start_year=1981,  # default setting, optional
    time_end_year=2010,  # default setting, optional
    historical_data="Historical Climate",  # default for now
    ssp_data=["SSP3-7.0"],  # default for now
    warming_level="1.5",  # default for now
    metric_calc="max",  # default for now
    heat_idx_threshold=None,
    one_in_x=None,
    percentile=None,
    season="all",  # default to all so no subsetting occurs unless called
    wrf_bias_adjust=True,
    export_method="both",  # raw, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format="NetCDF",
    batch_mode=False,
    distr="gev",
):
    """Retrieve, process, and export climate data based on inputs.

    Designed for CAVA reports.

    Parameters
    ----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    variable : str
        Type of climate variable to retrieve and calculate.
    approach : str
        Approach to follow, default is "time".
    downscaling_method : str
        Method of downscaling, default is "Dynamical".
    time_start_year : int, optional
        Starting year for data selection.
    time_end_year : int, optional
        Ending year for data selection.
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    metric_calc : str, optional
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics. Default is "max"
    heat_idx_threshold : float
        Heat index threshold for counting events.
    one_in_x : int, optional
        Return period for 1-in-X events.
    percentile : int, optional
        Percentile for calculating "likely" event occurrence.
    season : str, optional
        Season to subset time dimension on (e.g., 'summer', 'winter', 'all'). Default is 'all'.
    units : str, optional
        Units for the retrieved data.
    wrf_bias_adjust : str, optional
        Flag to subset the WRF data for the bias-adjusted models. Default is True.
    export_method : str, optional
        Export method, options are 'raw', 'calculate', 'both', default is 'both'.
    file_format : str, optional
        Export file format options.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.

    Returns
    -------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises
    ------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    with param.exceptions_summarized():
        params = CavaParams(**locals())
        clean_params = params.get_names()

        months_map = {
            "winter": [12, 1, 2],
            "summer": [6, 7, 8],
            "all": np.arange(1, 13),
        }

        locations = input_locations[["lat", "lon"]].values

        print(f"Calculating {clean_params['var_name']}")

        # display(Markdown(f"**Calculating {clean_params['var_name']}**"))

        print(f"--- Selecting Data Points --- \n")
        if approach == "time":

            selections = DataParameters()
            selections.data_type = "Gridded"
            selections.downscaling_method = downscaling_method
            selections.scenario_ssp = (
                clean_params["ssp_selected"]
                if historical_data != "Historical Reconstruction"
                else []
            )
            selections.timescale = (
                "hourly" if downscaling_method == "Dynamical" else "daily"
            )  # Toggling time-resolution for WRF and LOCA data respectively
            selections.variable_type = clean_params["variable_type"]
            selections.variable = clean_params["variable"]
            selections.resolution = "3 km"
            selections.units = units
            selections.scenario_historical = [historical_data]
            selections.time_slice = (time_start_year, time_end_year)

            if batch_mode:
                separate_files = False
                data_pts = batch_select(selections, locations, "time")

            else:

                data_pts = []
                for index, loc in input_locations.iterrows():

                    print(f"Selecting data for {loc['lat'], loc['lon']}")
                    lat, lon = loc["lat"], loc["lon"]
                    selections.latitude = (
                        lat - 0.02,
                        lat + 0.02,
                    )
                    selections.longitude = (lon - 0.02, lon + 0.02)

                    data = selections.retrieve()

                    # Remove leap days, if applicable
                    data = data.sel(
                        time=~((data.time.dt.month == 2) & (data.time.dt.day == 29))
                    )

                    # Filter for specified season
                    data = data.sel(time=data.time.dt.month.isin(months_map[season]))

                    # Filter for BA models
                    data = _filter_ba_models(
                        data, downscaling_method, wrf_bias_adjust, historical_data
                    )

                    # Dropping dimensions as approriate
                    if historical_data == "Historical Reconstruction":
                        data = data.squeeze(dim="scenario")

                    data = get_closest_gridcell(data, lat, lon, print_coords=False)
                    data = stack_sims_across_locs(data, "simulation")

                    data_pts.append(data)

        elif approach == "warming_level":
            wl = warming_levels()
            wl.wl_params.timescale = (
                "hourly" if downscaling_method == "Dynamical" else "daily"
            )  # Toggling time-resolution for WRF and LOCA data respectively
            wl.wl_params.downscaling_method = downscaling_method
            wl.wl_params.variable_type = clean_params["variable_type"]
            wl.wl_params.variable = clean_params["variable"]
            wl.wl_params.warming_levels = [
                warming_level
            ]  # Calvin- default, only allow for 1 warming level to be passed in.
            wl.wl_params.units = units
            wl.wl_params.resolution = "3 km"
            wl.wl_params.anom = "No"
            wl.wl_params.months = months_map[season]

            # Band-aid
            if batch_mode and downscaling_method == "Statistical":
                print(
                    "Batch mode for Statistical data in Warming Level approach is not optimized for multiple locations (in development). Resetting `batch_mode` to False. This may take some time because each location is retrieved and returned separately.\n"
                )
                batch_mode = False

            if batch_mode:

                wl.wl_params.load_data = False
                separate_files = False
                data_pts = batch_select(wl, locations, "warming_level")

                # Clean WL data before using
                data_pts = _clean_wl_data(data_pts, downscaling_method, separate_files)

                # Filter for BA models
                data_pts = _filter_ba_models(
                    data_pts, downscaling_method, wrf_bias_adjust, historical_data
                )

            else:

                data_pts = []
                for index, loc in input_locations.iterrows():
                    print(f"Selecting data for {loc['lat'], loc['lon']}")
                    lat, lon = loc["lat"], loc["lon"]

                    wl.wl_params.latitude = (lat - 0.02, lat + 0.02)
                    wl.wl_params.longitude = (lon - 0.02, lon + 0.02)
                    wl.calculate()

                    # Clean WL data before using
                    data = _clean_wl_data(
                        wl.sliced_data[warming_level],
                        downscaling_method,
                        separate_files,
                    )

                    # Filter for BA models
                    data = _filter_ba_models(
                        data, downscaling_method, wrf_bias_adjust, historical_data
                    )

                    # # Dropping dimensions as approriate
                    # if historical_data == "Historical Reconstruction":
                    #     data = data.squeeze(dim="scenario")

                    data = get_closest_gridcell(data, lat, lon, print_coords=False)
                    data = stack_sims_across_locs(data, "simulation")

                    data_pts.append(data)

        # Determining how to concatenate or load data, if necessary, depending on `approach` and `separate_files` parameters.
        if approach == "time":
            print("\n--- Loading Data into Memory ---\n")

            if separate_files:
                loaded_data = []
                for point in data_pts:
                    print(
                        f"Point ({point.lat.compute().item()}, {point.lon.compute().item()})"
                    )
                    data = load(point, progress_bar=True)
                    loaded_data.append(data)

            else:
                data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
                loaded_data = load(data_pts, progress_bar=True)

        elif approach == "warming_level":

            # Don't load or print anything since warming levels are already loaded
            loaded_data = data_pts

        # Export raw data if desired
        if export_method == "raw" or export_method == "both":

            print("\n--- Exporting Raw Data ---")

            if downscaling_method == "Statistical":
                print(
                    "\nExporting Statistical data in most granular time availability (daily)\n"
                )

            if separate_files:
                for pt_idx in range(len(loaded_data)):
                    _export_no_e(
                        loaded_data[pt_idx],
                        filename=clean_params["raw_name"],
                        format=file_format,
                    )
            else:
                _export_no_e(
                    loaded_data, filename="combined_raw_data", format=file_format
                )

            if export_method == "raw":
                return loaded_data

        print("\n--- Calculating Metrics ---")
        print("Calculating... ", end="", flush=True)

        if separate_files:
            metric_data = []
            # Calculate and export into separate files
            for point in loaded_data:
                calc_val = _metric_agg(
                    point,
                    approach,
                    metric_calc,
                    heat_idx_threshold,
                    one_in_x,
                    percentile,
                    clean_params["var_name"],
                    distr,
                )
                metric_data.append(calc_val)

        # Export a combined file of all metrics
        else:
            metric_data = _metric_agg(
                loaded_data,
                approach,
                metric_calc,
                heat_idx_threshold,
                one_in_x,
                percentile,
                clean_params["var_name"],
                distr,
            )
        print("Complete!")

        # Export the data
        print("\n--- Exporting Metric Data ---")

        if export_method == "calculate" or export_method == "both":

            if separate_files:
                for pt_idx in range(len(metric_data)):
                    _export_no_e(
                        metric_data[pt_idx],
                        filename=f"{clean_params['calc_name']}_{str(metric_data[pt_idx].lat.item()).replace('.', '')}N_{str(metric_data[pt_idx].lon.item()).replace('.', '')}W",
                        format=file_format,
                    )  # Will need to include naming convention for calculated file too.

            else:
                _export_no_e(metric_data, filename="metric_data", format=file_format)

            # Returning values to user
            if len(metric_data) == 1:
                metric_data = metric_data[0]
            if len(loaded_data) == 1:
                loaded_data = loaded_data[0]

            if export_method == "calculate":
                return {"calc_data": metric_data}
            elif export_method == "both":
                return {"calc_data": metric_data, "raw_data": loaded_data}

        elif export_method == "None":  # Specific for table generation
            print("Data export selection set to 'None'; no data is exported!")
            return {"calc_data": metric_data}
