"""Tools for CAVA vulnerability assessment pilot"""

from climakitae.util.utils import get_closest_gridcell
from climakitae.core.data_load import load
from climakitae.core.data_interface import Select
from climakitae.core.data_export import export
import xarray as xr
import numpy as np
import pandas as pd
import warnings
from climakitae.explore import warming_levels

# Ignore specific warnings about division errors to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")


def metric_agg(da, approach, metric, threshold):
    """
    Calculate a metric on the given DataArray (da) using the specified metric type
    and threshold.

    Parameters:
    -----------
    da : xarray.DataArray
        Input DataArray to compute metric and treshold on.
    metric : str
        The metric type to calculate. Supported options are "min", "max",
        "mean", and "median".
    threshold : float
        The threshold value for comparison.

    Returns:
    --------
    metric_data : xarray.DataArray
        Resulting DataArray from the calculated metric and threshold applied.
    """
    metric_map = {
        "min": np.min,
        "max": np.max,
        "mean": np.mean,
        "median": np.median,
    }

    def apply_metric(da):
        """Applies the hard-coded metric to the DataArray"""
        return (
            (da.resample(time="1D").reduce(metric_map[metric]) > threshold)
            .resample(time="1Y")
            .sum()
        )

    if approach == "time":
        metric_data = apply_metric(da)

    # If the data passed in is warming level data, the data must be manipulated to re-create a dummy time index
    elif approach == "warming_level":

        ### Creating dummy timestamps and renaming time dimension to replace current WL "time" dimension

        # Finding the name of the dimension that is not `simulation`, that should be named `hours/days/months_from_center`
        time_dim_name = [dim for dim in da.dims if dim != "simulation"][0].split("_")[0]

        # Creating dummy timestamps and assigning to the DataArray
        name_to_freq = {"hours": "H", "days": "D", "months": "M"}
        timestamps = pd.date_range(
            "2000-01-01",
            periods=len(da[time_dim_name]),
            freq=name_to_freq[time_dim_name],
        )
        da = da.assign_coords(hours_from_center=timestamps).rename(
            hours_from_center="time"
        )

        # Computing metric
        metric_data = apply_metric(da)

        # Reverting time dimension back to WL framework and rename dimension
        metric_data = metric_data.assign_coords(
            time=np.concatenate(
                [
                    np.arange(-len(metric_data.time) / 2, 0),
                    np.arange(1, len(metric_data.time) / 2 + 1),
                ]
            )
        ).rename(time="years_from_center")

    return metric_data


def check_and_set_params(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    metric_calc,
    threshold,
    downscaling_method="Dynamical",  # default for now ## mandatory
    approach="time",  # GWL to follow on ## mandatory
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=["SSP 3-7.0 -- Business as Usual"],
    variable="METRIC HERE",  ## mandatory, must eventually accept temp, precip, or heat index
    export_method="both",  # off-ramp, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
):
    """
    Checks params and returns new param values if required.
    """
    # Validating lat/lon
    if "lat" not in input_locations.columns or "lon" not in input_locations.columns:
        raise ValueError("Input coordinates must have `lat` and `lon` columns.")
    elif (
        (input_locations.dtypes.lat != "float64")
        and (input_locations.dtypes.lat != "int64")
    ) or (
        (input_locations.dtypes.lon != "float64")
        and (input_locations.dtypes.lon != "int64")
    ):
        raise ValueError("Input lat/lon columns must be float64 or int64 types.")

    # Validating metric
    if metric_calc not in ["min", "max", "mean", "median"]:
        raise ValueError("Passed in metric must be min, max, median, or mean.")

    # Validating export method
    if export_method not in ["off-ramp", "calculate", "both"]:
        raise ValueError(
            "Passed in export method must be 'off-ramp', 'calculate', or 'both'"
        )

    # Validating variable
    if variable not in [
        "Air Temperature at 2m",
        "Precipitation (total)",
        "NOAA Heat Index",
    ]:
        raise ValueError(
            "Passed in variable must be `Air Temperature at 2m`, `Precipitation (total)`, or `NOAA Heat Index`"
        )

    # Calvin- Add unit validation for variable (can't pull degF for Precip)

    # Map SSP names from user inputs to selections SSPs
    ssp_mapping = {
        "SSP2-4.5": "SSP 2-4.5 -- Middle of the Road",
        "SSP3-7.0": "SSP 3-7.0 -- Business as Usual",
        "SSP5-8.5": "SSP 5-8.5 -- Burn it All",
    }
    ssp_selected = [ssp_mapping[ssp] for ssp in ssp_data]

    # Determining if passed in variable is a climate variable or a derived index
    if variable == "Air Temperature at 2m" or variable == "Precipitation (total)":
        variable_type = "Variable"
    elif variable == "NOAA Heat Index":
        variable_type = "Derived Index"

    return {"ssp_selected": ssp_selected, "variable_type": variable_type}


def cava_data(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    metric_calc,
    threshold,
    downscaling_method="Dynamical",  # default for now ## mandatory
    approach="time",  # GWL to follow on ## mandatory, Q: Why is anom required for non BC WRF data?
    warming_level="1.5",
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=["SSP 3-7.0 -- Business as Usual"],
    variable="METRIC HERE",  ## mandatory, must eventually accept temp, precip, or heat index
    export_method="both",  # off-ramp, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
):
    """
    Retrieves, processes, and exports climate data based on inputs, designed for CAVA reports.

    Parameters:
    -----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    downscaling_method : str, optional
        Method of downscaling, default is "Dynamical".
    approach : str, optional
        Approach to follow, default is "time".
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    time_start_year : int, optional
        Starting year for data selection.
    time_end_year : int, optional
        Ending year for data selection.
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    variable : str
        Type of climate variable to retrieve and calculate.
    units : str
        Units for the retrieved data, default is "degF".
    raw_export : str, optional
        Export option, default is "offramp".

    Returns:
    --------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises:
    -------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    clean_params = check_and_set_params(
        **locals()
    )  # This may not be best practice. Also to note, this line MUST be the first line (or at least before other local vars are defined), or else they will also be passed to `check_and_set_params`.

    print("--- Selecting Data Points --- \n")

    data_pts = []
    for index, loc in input_locations.iterrows():

        print(f"Selecting data for {loc['lat'], loc['lon']}")
        lat, lon = loc["lat"], loc["lon"]

        if approach == "time":

            selections = Select()
            selections.data_type = "Gridded"
            selections.downscaling_method = downscaling_method
            selections.scenario_historical = [historical_data]
            selections.scenario_ssp = clean_params["ssp_selected"]
            selections.timescale = "hourly"
            selections.variable_type = clean_params["variable_type"]
            selections.variable = variable
            selections.latitude = (
                lat - 0.02,
                lat + 0.02,
            )  # Q: What padding to put for lat/lon values?
            selections.longitude = (lon - 0.02, lon + 0.02)
            selections.time_slice = (time_start_year, time_end_year)
            selections.resolution = "3 km"
            selections.units = units

            data = selections.retrieve()

        elif approach == "WL":

            wl = warming_levels()
            wl.wl_params.timescale = "hourly"
            wl.wl_params.downscaling_method = downscaling_method
            wl.wl_params.variable_type = "Derived Index"
            wl.wl_params.variable = "NOAA Heat Index"
            wl.wl_params.latitude = (lat - 0.02, lat + 0.02)
            wl.wl_params.longitude = (lon - 0.02, lon + 0.02)
            wl.wl_params.warming_levels = [
                warming_level
            ]  # Calvin- default, only allow for 1 warming level to be passed in.
            wl.wl_params.units = units
            wl.wl_params.resolution = "3 km"
            wl.wl_params.anom = "No"

            wl.calculate()

            data = wl.sliced_data[warming_level]

            # Renaming simulation dimension (Calvin- will only work when models are unique across SSPs, Q: will this work with LOCA data too?)
            data["all_sims"] = [
                "_".join(x.split("_")[:3])
                for x in wl.sliced_data[warming_level].all_sims.to_numpy()
            ]
            data = data.rename({"all_sims": "simulation"})

            # Elevate dimensions, if needed
            if "x" not in data.dims and downscaling_method == "Dynamical":
                data = data.expand_dims(dim="x")
            if "y" not in data.dims and downscaling_method == "Dynamical":
                data = data.expand_dims(dim="y")

        # Filter only for BC-WRF models
        data = data.sel(
            simulation=[
                "WRF_EC-Earth3_r1i1p1f1",
                "WRF_MPI-ESM1-2-HR_r3i1p1f1",
                "WRF_TaiESM1_r1i1p1f1",
                "WRF_MIROC6_r1i1p1f1",
            ]
        )

        # Find the closest gridcell from the data to the lat/lon point
        closest_cell = get_closest_gridcell(
            data, lat, lon, print_coords=False
        ).squeeze()

        # Storing data differently if all data points should be in one file vs each data point in its own file
        if not separate_files:
            # Renaming gridcell so that it can be concatenated with other lat/lon gridcells
            closest_cell["simulation"] = [
                "{}_{}_{}".format(
                    sim_name, closest_cell.lat.item(), closest_cell.lon.item()
                )
                for sim_name in closest_cell.simulation
            ]

        data_pts.append(closest_cell)

    # Loading the data differently if you want all data points in a single file vs separate files

    print("\n--- Loading Data into Memory ---\n")

    if separate_files:
        loaded_data = [
            (
                lambda point: (
                    print(f"Point ({data_pt.lat.item()}, {data_pt.lon.item()})"),
                    load(data_pt),
                    print("\n"),
                )
            )(data_pt)[
                1
            ]  # Prints the data, loads the data, and only stores the loaded data into `loaded_data`
            for data_pt in data_pts
        ]

    else:
        # Combine data points into a single xr.Dataset to load in and calculate metrics on
        data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
        loaded_data = load(data_pts)

    # Export raw data if desired
    if export_method == "off-ramp" or export_method == "both":

        print("\n--- Offramp Exporting Raw Data ---")

        if separate_files:
            for point in loaded_data:
                export(
                    point,
                    filename=f"{variable}_{point.lat.item()}_{point.lon.item()}",  # Q: Do we want to include decimal points in filenames? Also, .nc or .csv?
                )
        else:
            export(loaded_data, filename="combined_raw_data")

        if export_method == "off-ramp":
            return loaded_data

    print("--- Calculating Metrics ---")

    # Run calculations or threshold counts if desired
    if (
        export_method == "calculate" or export_method == "both"
    ):  # Calvin- I think this condition is always true if you get to this part of the code

        if separate_files:

            metric_data = []
            # Calculate and export into separate files
            for point in loaded_data:
                calc_val = metric_agg(point, approach, metric_calc, threshold)
                calc_val.name = f"Days per year with {metric_calc.capitalize()} Daily {variable} above threshold of {threshold} {units}"
                metric_data.append(calc_val)

        # Export a combined file of all metrics
        else:
            metric_data = metric_agg(loaded_data, approach, metric_calc, threshold)
            metric_data.name = f"Days per year with {metric_calc.capitalize()} Daily {variable} above threshold of {threshold} {units}"

    # Export the data
    print("\n--- Exporting Metric Data ---")

    if separate_files:
        for point in metric_data:
            export(
                point,
                filename=f"{metric_calc.capitalize()}_{variable}_{threshold}_{point.lat.item()}_{point.lon.item()}",
            )  # Q: How do we want the file names to be stored?

    else:
        export(metric_data, filename="metric_data")

    # Return data based on export method, Q: How should we return the points? Dictionary of points to DataArrays?
    if export_method == "calculate":
        return metric_data
    elif export_method == "both":
        return loaded_data, metric_data
