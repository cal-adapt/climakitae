"""Tools for CAVA vulnerability assessment pilot"""

import warnings
import xarray as xr
import numpy as np
import pandas as pd
import param

from climakitae.util.utils import (
    get_closest_gridcell,
    add_dummy_time_to_wl,
)
from climakitae.core.data_export import export
from climakitae.core.data_interface import DataParameters
from climakitae.core.data_load import load
from climakitae.explore import warming_levels
from climakitae.explore.threshold_tools import (
    get_block_maxima,
    get_return_value,
    get_ks_stat,
)
from climakitae.tools.batch import batch_select
from dask.array.core import PerformanceWarning

# Ignore specific warnings about division and performance to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")
warnings.filterwarnings("ignore", category=PerformanceWarning)


def _export_no_e(da, filename, format):
    """Exports a file but doesn't throw an exception if a file already exists."""
    try:
        export(
            da,
            filename=filename,
            format=format,
        )
    except Exception as e:
        print(e)


def _filter_ba_models(data, downscaling_method, wrf_bias_adjust, historical_data):
    """Filters the data for the bias adjusted simulations, if desired."""
    if (
        downscaling_method == "Dynamical"
        and wrf_bias_adjust
        and historical_data != "Historical Reconstruction"
    ):
        # Filter only for BC-WRF models
        # This check is to see which simulations have these BC models as the root part of their simulation name, which is to accomodate for simulation names being the same across SSPs.
        # Not sure why this is selecting the right sims for batch mode but not leaving the lat/lons on their names
        data = data.sel(
            simulation=[
                sim
                for sim in data.simulation.values
                if any(
                    s in sim
                    for s in [
                        "WRF_EC-Earth3_r1i1p1f1",
                        "WRF_MPI-ESM1-2-HR_r3i1p1f1",
                        "WRF_TaiESM1_r1i1p1f1",
                        "WRF_MIROC6_r1i1p1f1",
                        "WRF_EC-Earth3-Veg_r1i1p1f1",
                    ]
                )
            ]
        )
    return data


def _metric_agg(
    da,
    variable,
    months,
    approach,
    metric,
    name_of_calc,
    heat_idx_threshold,
    percentile,
    one_in_x,
    event_duration,
    distr,
):
    """
    Calculate a metric on the given DataArray (da) using the specified metric type
    and heat index threshold.

    Parameters:
    -----------
    da : xarray.DataArray
        Input DataArray to compute metric and treshold on.
    metric : str
        The metric type to calculate. Supported options are "min", "max".
    heat_idx_threshold : float
        The heat index threshold for comparison.
    one_in_x : int
        Return period for 1-in-X events.
    percentile : int
        Percentile / quantile for calculating "likely".

    Returns:
    --------
    metric_data : xarray.DataArray
        Resulting DataArray from the calculated metric and heat index threshold applied.
    """
    metric_map = {
        "min": np.min,
        "max": np.max,
    }

    def _apply_metric(da):
        """Applies the hard-coded metrics to the DataArray"""
        if (
            heat_idx_threshold or heat_idx_threshold == 0
        ):  # If a threshold is passed and/or if the `heat_idx_threshold` passed in is 0, since that is a valid threshold.
            calc_val = (
                (da.resample(time="1D").reduce(metric_map[metric]) > heat_idx_threshold)
                .resample(time="1Y")
                .sum()
                .mean(dim="time")
            )  # This line counts the average number of high heat days per year over this time interval

        elif one_in_x:  # Calculate 1-in-X events

            print(
                f"Goodness of fit for 1-in-{one_in_x}, {'-'.join(map(str, event_duration))} {variable} events: "
            )

            # Get 1-in-x event values
            return_vals, p_vals = [], []
            for (
                sim
            ) in (
                da.simulation.values
            ):  # Can use xr.apply_ufunc, not just a for loop here

                one_da = da.sel(simulation=sim)

                # Removing small amounts of noise and 0-precip days from precipitation data
                if variable == "Precipitation (total)":
                    one_da = one_da.resample(time="1D").sum()

                    # Resampling can cause initially filtered out dates to reappear. Re-filtered in case that occurs.
                    one_da = one_da.sel(time=one_da.time.dt.month.isin(months))

                    one_da = one_da.where(one_da > 10e-10, drop=True)

                # Computing block maxima
                groupby, duration = None, None
                if event_duration == (1, "day"):
                    groupby = event_duration
                elif event_duration[1] == "hour":
                    duration = event_duration

                ams = get_block_maxima(
                    one_da,
                    extremes_type=metric,
                    duration=duration,
                    groupby=groupby,
                    check_ess=False,
                ).squeeze()  # `get_return_value` is expecting a 1-D array

                # Calculating 1-in-X return value
                return_values = get_return_value(
                    ams,
                    return_period=one_in_x,
                    multiple_points=False,
                    distr=distr,
                )["return_value"]
                return_vals.append(return_values)

                # Determining goodness-of-fit for the given distribution
                d_statistic, p_value = [
                    float(v)
                    for v in get_ks_stat(
                        ams, distr=distr, multiple_points=False
                    ).data_vars.values()
                ]
                p_vals.append(p_value)

                # Printing out p-values
                p_val_print = (
                    format(p_value, ".3e") if p_value < 0.05 else round(p_value, 4)
                )
                to_print = f"\nThe simulation {sim} fitted with a {distr} distribution has a p-value of {p_val_print}."

                if p_value < 0.05:
                    to_print += " Since the p-value is <0.05, the selected distribution does not fit the data well and therefore is not a good fit (see guidance)."
                print(to_print)

            # Creating output 1-in-X object with p-values
            calc_val = xr.concat(return_vals, dim="simulation")
            calc_val = calc_val.assign_coords(p_values=("simulation", p_vals))

            # Changing attributes for thresholds DataArray because of export issue later on
            if groupby:
                old_group = calc_val.attrs["groupby"]
                calc_val.attrs["groupby"] = f"{old_group[0]} {old_group[1]}"
            elif duration:
                old_duration = calc_val.attrs["duration"]
                calc_val.attrs["duration"] = f"{old_duration[0]} {old_duration[1]}"

            # Add distribution to attributes
            calc_val = calc_val.assign_attrs(
                fitted_distr=distr,
                sample_size=len(ams),
            )

        elif percentile or percentile == 0:

            calc_val = (
                da.resample(time="1D")
                .reduce(metric_map[metric])
                .quantile(percentile / 100, dim="time")
                .assign_coords(
                    lat=da.lat,
                    lon=da.lon,
                    quantile=percentile,  # Coords are dropped during quantile method application
                )
                .rename({"quantile": "percentile"})
            )

        calc_val.name = name_of_calc
        return calc_val

    # If the data passed in is warming level data, then a dummy timestamp must be added to act as the current WL 'time' dimension
    if approach == "Warming Level":
        da = add_dummy_time_to_wl(da)

    metric_data = _apply_metric(da)
    return metric_data


class CavaParams(param.Parameterized):
    """
    Validates parameters for the `cava_data` function, and returns transformed parameters, if needed.
    Transformed parameters returned: `ssp_selected`, `variable_type`.
    """

    input_locations = param.DataFrame(
        doc="Input coordinates must have 'lat' and 'lon' columns."
    )
    time_start_year = param.Integer(bounds=(1900, 2100), doc="Start year", default=1981)
    time_end_year = param.Integer(bounds=(1900, 2100), doc="End year", default=2010)

    # Need to find all valid units by variable name
    units = param.String(default="Celsius", doc="Units for temperature measurement")

    variable = param.ObjectSelector(
        default="Air Temperature at 2m",
        objects=["Air Temperature at 2m", "Precipitation (total)", "NOAA Heat Index"],
        doc="Variable to analyze",
    )

    metric_calc = param.ObjectSelector(
        default="max",
        objects=["min", "max", "mean", "median"],
        doc="Metric calculation method",
    )
    percentile = param.Number(
        default=None, bounds=(0, 100), allow_None=True, doc="Percentile for calculation"
    )
    heat_idx_threshold = param.Number(
        default=None, allow_None=True, doc="Threshold for heat index"
    )
    one_in_x = param.Integer(default=None, allow_None=True, doc="1-in-x year event")
    season = param.ObjectSelector(
        default="all", objects=["summer", "winter", "all"], doc="Season to analyze"
    )
    downscaling_method = param.ObjectSelector(
        default="Dynamical",
        objects=["Dynamical", "Statistical"],
        doc="Downscaling method",
    )
    approach = param.ObjectSelector(
        default="Time", objects=["Time", "Warming Level"], doc="Approach to use"
    )
    warming_level = param.ObjectSelector(
        default=1.5,
        objects=[None, 1.5, 2.0, 2.5, 3.0],
        doc="Warming level for analysis",
    )
    wrf_bias_adjust = param.Boolean(default=True, doc="Flag for WRF bias adjustment")
    historical_data = param.ObjectSelector(
        default="Historical Climate",
        objects=["Historical Climate", "Historical Reconstruction"],
        doc="Type of historical data",
    )
    ssp_data = param.ListSelector(
        default=["SSP3-7.0"],
        objects=["SSP2-4.5", "SSP3-7.0", "SSP5-8.5"],
        doc="SSP data scenarios to use",
    )
    export_method = param.ObjectSelector(
        default="both",
        objects=["raw", "calculate", "both", "None"],
        doc="Export method",
    )
    separate_files = param.Boolean(
        default=False,
        doc="Whether to separate climate variable information into separate files",
    )
    file_format = param.String(default="NetCDF", doc="Format of the output files")
    batch_mode = param.Boolean(default=False, doc="Flag for batch mode operation")
    distr = param.ObjectSelector(
        default="gev",
        objects=["gev", "genpareto", "gamma"],
        doc="Distribution function",
    )

    def __init__(self, **params):
        super().__init__(**params)
        self.validate_params()

    def validate_params(self):
        errors = []

        # Check if input_locations is a DataFrame with lat/lon columns
        if isinstance(self.input_locations, pd.DataFrame):
            if (
                "lat" not in self.input_locations.columns
                or "lon" not in self.input_locations.columns
            ):
                errors.append("Input coordinates must have `lat` and `lon` columns.")
            elif not pd.api.types.is_numeric_dtype(
                self.input_locations["lat"]
            ) or not pd.api.types.is_numeric_dtype(self.input_locations["lon"]):
                errors.append("Input lat/lon columns must be float64 or int64 types.")

        # Validating that time start year < time end year
        if self.time_start_year > self.time_end_year:
            errors.append(
                "Start year must come before, or be the same year as, the end year."
            )

        # Validate year range for historical reconstruction
        if (
            self.historical_data == "Historical Reconstruction"
            and self.time_end_year > 2022
        ):
            errors.append(
                "End year for Historical Reconstruction data must be 2022 or earlier."
            )

        # Validate time approach with Historical Reconstruction
        if (
            self.historical_data == "Historical Reconstruction"
            and self.approach != "Time"
        ):
            errors.append(
                "Historical Reconstruction data can only be retrieved using a time-based approach."
            )

        # Validating warming level inputs
        if self.approach == "Warming Level" and self.warming_level not in [
            1.5,
            2.0,
            2.5,
            3.0,
        ]:
            errors.append("Warming level must be 1.5, 2.0, 2.5, or 3.0.")

        # Validating that only one of heat_idx_threshold, percentile, or one_in_x is passed
        if (
            (self.heat_idx_threshold is not None and self.percentile is not None)
            or (self.heat_idx_threshold is not None and self.one_in_x is not None)
            or (self.percentile is not None and self.one_in_x is not None)
        ):
            errors.append(
                "Only one of heat_idx_threshold, one_in_x, and percentile can be non-None."
            )

        # Check that not all 3 customizable metric arguments are none.
        if (
            self.heat_idx_threshold is None
            and self.percentile is None
            and self.one_in_x is None
        ):
            errors.append(
                "`heat_idx_threshold`, `percentile`, and `one_in_x` cannot all be None."
            )

        # Make sure the metric is 'min' or 'max' if percentile is passed in
        if self.percentile is not None and self.metric_calc not in ["min", "max"]:
            errors.append(
                "Metric calculation must be 'min' or 'max' for percentile calculations."
            )

        # Validating variable and downscaling method combo
        if (
            self.variable == "NOAA Heat Index"
            and self.downscaling_method == "Statistical"
        ):
            errors.append(
                "`NOAA Heat Index` cannot be used with Statistical downscaling method."
            )

        # Only allow for SSP 3-7.0 for WRF data
        if self.downscaling_method == "Dynamical" and self.approach == "Time":
            if len(self.ssp_data) > 1 or self.ssp_data[0] != "SSP3-7.0":
                errors.append(
                    "Can only use SSP 3-7.0 data for a time-based approach using WRF data."
                )

        # Only allow `hour` or `daily` frequencies for 1-in-X event counts
        dur_len, dur_type = self.event_duration
        if dur_type != "hour" and dur_type != "day":
            raise ValueError(
                "Current specifications not implemented. `event_duration` options only implemented for `hour` or `daily` frequency."
            )

        # Raise errors if any
        if errors:
            raise ValueError("Parameter validation errors:\n\n" + "\n".join(errors))

    def get_names(self):

        # Re-assigning `variable` in case it needs to be re-instantiated later.
        variable = self.variable

        # Determine variable type
        if variable in ["Air Temperature at 2m", "Precipitation (total)"]:
            variable_type = "Variable"
        else:
            variable_type = "Derived Index"

        # Adjust variable name for LOCA2 data
        if (
            variable == "Air Temperature at 2m"
            and self.downscaling_method == "Statistical"
        ):
            if self.metric_calc == "max":
                variable = "Maximum air temperature at 2m"
            elif self.metric_calc == "min":
                variable = "Minimum air temperature at 2m"

        # Adding time period or warming level to filename
        if self.approach == "Time":
            approach_str = f"{self.time_start_year}_to_{self.time_end_year}"
            approach_var_name = f"from {approach_str}"
        elif self.approach == "Warming Level":
            wl_str = (
                str(int(self.warming_level))
                if self.warming_level not in [1.5, 2.5]
                else str(self.warming_level).replace(".", "pt")
            )
            approach_str = f"{wl_str}degreeWL"
            approach_var_name = f"for Warming Level {self.warming_level}Â°C"

        # Retrieving the name of the calculation about to occur
        if self.percentile is not None:

            def ordinal(n):
                """Find ordinal name for a number (1 -> 1st, 2 -> 2nd, 3 -> 3rd)"""
                return f"{n}{('th' if 11 <= n % 100 <= 13 else {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th'))}"

            var_name = f"{ordinal(self.percentile)} percentile of Daily {self.metric_calc.capitalize()} of {variable} for {self.season} seasons {approach_var_name}"
            raw_name = "likely_seasonal_temperature_raw_data"
            calc_name = (
                f"likely_seasonal_{self.season}_{self.metric_calc}_{approach_str}"
            )

        elif self.heat_idx_threshold is not None:
            var_name = f"Days per year with {self.metric_calc.capitalize()} Daily {variable} above a heat index threshold of {self.heat_idx_threshold} {self.units} for {self.season} seasons {approach_var_name}"
            raw_name = "heat_index_raw_data"
            calc_name = f"heat_index_thresh{self.heat_idx_threshold}_{approach_str}"

        elif self.one_in_x is not None:
            var_name = f"1-in-{self.one_in_x} year, {'-'.join(map(str, self.event_duration))} {self.metric_calc.capitalize()} {variable} for {self.season} seasons {approach_var_name}"

            # Making names for raw and calculated data
            dur_len, dur_type = self.event_duration
            shortname_event = f"{dur_len}_{dur_type if dur_type == 'day' else 'hr'}"
            if variable == "Precipitation (total)":
                raw_name = f"one_in_{self.one_in_x}_precipitation_raw_data"
                calc_name = f"one_in_{self.one_in_x}_{shortname_event}_precipitation_{approach_str}"
            elif variable == "Air Temperature at 2m" or "Maximum air temperature at 2m":
                raw_name = f"one_in_{self.one_in_x}_temperature_raw_data"
                calc_name = f"one_in_{self.one_in_x}_{shortname_event}_temperature_{approach_str}"

        ssp_mapping = {
            "SSP2-4.5": "SSP 2-4.5 -- Middle of the Road",
            "SSP3-7.0": "SSP 3-7.0 -- Business as Usual",
            "SSP5-8.5": "SSP 5-8.5 -- Burn it All",
        }

        return {
            "ssp_selected": [ssp_mapping[ssp] for ssp in self.ssp_data],
            "variable": variable,
            "variable_type": variable_type,
            "var_name": var_name,
            "raw_name": raw_name,
            "calc_name": calc_name,
        }


def cava_data(
    input_locations,  # csv file
    variable,  # must eventually accept temp, precip, or heat index
    units=None,
    approach="Time",  # default for now
    downscaling_method="Dynamical",  # default for now
    time_start_year=1981,  # default setting, optional
    time_end_year=2010,  # default setting, optional
    historical_data="Historical Climate",  # default for now
    ssp_data=["SSP3-7.0"],  # default for now
    warming_level=1.5,  # default for now
    metric_calc="max",  # default for now
    heat_idx_threshold=None,
    one_in_x=None,
    event_duration=(1, "day"),
    percentile=None,
    season="all",  # default to all so no subsetting occurs unless called
    wrf_bias_adjust=True,
    export_method="both",  # raw, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format="NetCDF",
    batch_mode=False,
    distr="gev",
):
    """Retrieve, process, and export climate data based on inputs.

    Designed for CAVA reports.

    Parameters
    ----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    variable : str
        Type of climate variable to retrieve and calculate.
    approach : str
        Approach to follow, default is "Time".
    downscaling_method : str
        Method of downscaling, default is "Dynamical".
    time_start_year : int, optional
        Starting year for data selection.
    time_end_year : int, optional
        Ending year for data selection.
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    metric_calc : str, optional
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics. Default is "max"
    heat_idx_threshold : float
        Heat index threshold for counting events.
    one_in_x : int, optional
        Return period for 1-in-X events.
    percentile : int, optional
        Percentile for calculating "likely" event occurrence.
    season : str, optional
        Season to subset time dimension on (e.g., 'summer', 'winter', 'all'). Default is 'all'.
    units : str, optional
        Units for the retrieved data.
    wrf_bias_adjust : str, optional
        Flag to subset the WRF data for the bias-adjusted models. Default is True.
    export_method : str, optional
        Export method, options are 'raw', 'calculate', 'both', default is 'both'.
    file_format : str, optional
        Export file format options.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.

    Returns
    -------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises
    ------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    with param.exceptions_summarized():
        params = CavaParams(**locals())
        clean_params = params.get_names()

        months_map = {
            "winter": [12, 1, 2],
            "summer": [6, 7, 8],
            "all": range(1, 13),
        }

        locations = input_locations[["lat", "lon"]].values

        print(f"Calculating {clean_params['var_name']}")

        print(f"--- Selecting Data Points --- \n")

        selections = DataParameters()
        selections.approach = approach
        selections.data_type = "Gridded"
        selections.downscaling_method = downscaling_method
        selections.timescale = (
            "hourly" if downscaling_method == "Dynamical" else "daily"
        )
        selections.variable_type = clean_params["variable_type"]
        selections.variable = clean_params["variable"]
        selections.resolution = "3 km"
        selections.units = units
        selections.time_slice = (time_start_year, time_end_year)

        # Setting WL vs time-based attributes
        if approach == "Warming Level":
            selections.warming_level = [float(warming_level)]
            selections.warming_level_months = months_map[season]
            selections.scenario_ssp = ["n/a"]
            selections.scenario_historical = ["n/a"]

        else:
            selections.scenario_ssp = (
                clean_params["ssp_selected"]
                if historical_data != "Historical Reconstruction"
                else []
            )
            selections.scenario_historical = [historical_data]

        if batch_mode and downscaling_method == "Statistical":
            print(
                "Batch mode for Statistical data in Warming Level approach is not optimized for multiple locations (in development). Resetting `batch_mode` to False. This may take some time because each location is retrieved and returned separately.\n"
            )
            batch_mode = False

        if batch_mode:
            separate_files = False
            data_pts = batch_select(approach, selections, locations)

            data = _filter_ba_models(
                data_pts, downscaling_method, wrf_bias_adjust, historical_data
            )

        else:
            data_pts = []
            for idx, loc in input_locations.iterrows():

                print(f"Selecting data for {loc['lat'], loc['lon']}")
                lat, lon = loc["lat"], loc["lon"]

                selections.latitude = (lat - 0.02, lat + 0.02)
                selections.longitude = (lon - 0.02, lon + 0.02)
                data = selections.retrieve()

                if approach == "Time":
                    # Remove leap days, if applicable
                    data = data.sel(
                        time=~((data.time.dt.month == 2) & (data.time.dt.day == 29))
                    )

                data = get_closest_gridcell(data, lat, lon, print_coords=False)

                data = _filter_ba_models(
                    data, downscaling_method, wrf_bias_adjust, historical_data
                )

                data_pts.append(data)

        if historical_data == "Historical Reconstruction":
            data = data.squeeze(dim="scenario")

        # Determining how to concatenate or load data, if necessary, depending on `approach` and `separate_files` parameters.
        print("\n--- Loading Data into Memory ---\n")

        if separate_files:
            loaded_data = []
            for point in data_pts:
                print(
                    f"Point ({point.lat.compute().item()}, {point.lon.compute().item()})"
                )
                data = load(point, progress_bar=True)
                loaded_data.append(data)

        else:
            data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
            loaded_data = load(data_pts, progress_bar=True)

        # Export raw data if desired
        if export_method == "raw" or export_method == "both":

            print("\n--- Exporting Raw Data ---")

            if downscaling_method == "Statistical":
                print(
                    "\nExporting Statistical data in most granular time availability (daily)\n"
                )

            if separate_files:
                for pt_idx in range(len(loaded_data)):
                    _export_no_e(
                        loaded_data[pt_idx],
                        filename=clean_params["raw_name"],
                        format=file_format,
                    )
            else:
                _export_no_e(
                    loaded_data, filename="combined_raw_data", format=file_format
                )

            if export_method == "raw":
                return loaded_data

        print("\n--- Calculating Metrics ---")
        print("Calculating...\n")  # , end="", flush=True)

        if separate_files:
            metric_data = []
            # Calculate and export into separate files
            for point in loaded_data:
                calc_val = _metric_agg(
                    point,
                    variable,
                    months_map[season],
                    approach,
                    metric_calc,
                    clean_params["var_name"],
                    heat_idx_threshold,
                    percentile,
                    one_in_x,
                    event_duration,
                    distr,
                )
                metric_data.append(calc_val)

        # Export a combined file of all metrics
        else:
            metric_data = _metric_agg(
                loaded_data,
                variable,
                months_map[season],
                approach,
                metric_calc,
                clean_params["var_name"],
                heat_idx_threshold,
                percentile,
                one_in_x,
                event_duration,
                distr,
            )

        print("\nComplete!")

        # Export the data
        print("\n--- Exporting Metric Data ---")

        if export_method == "calculate" or export_method == "both":

            if separate_files:
                for pt_idx in range(len(metric_data)):
                    _export_no_e(
                        metric_data[pt_idx],
                        filename=f"{clean_params['calc_name']}_{str(metric_data[pt_idx].lat.item()).replace('.', '')}N_{str(metric_data[pt_idx].lon.item()).replace('.', '')}W",
                        format=file_format,
                    )  # Will need to include naming convention for calculated file too.

            else:
                _export_no_e(metric_data, filename="metric_data", format=file_format)

            # Returning values to user
            if len(metric_data) == 1:
                metric_data = metric_data[0]
            if len(loaded_data) == 1:
                loaded_data = loaded_data[0]

            if export_method == "calculate":
                return {"calc_data": metric_data}
            elif export_method == "both":
                return {"calc_data": metric_data, "raw_data": loaded_data}

        elif export_method == "None":  # Specific for table generation
            print("Data export selection set to 'None'; no data is exported!")
            return {"calc_data": metric_data}


def _get_sce_3km_latlon():
    """Get all 3 km lat/lon coordinates for SCE service territory"""
    # Retrieve SCE territory data
    selections = DataParameters()
    selections.downscaling_method = "Dynamical"
    selections.timescale = "monthly"
    selections.time_slice = (2010, 2011)
    selections.resolution = "3 km"
    selections.area_subset = "CA Electric Load Serving Entities (IOU & POU)"
    selections.cached_area = ["Southern California Edison"]
    ds = selections.retrieve()

    # Find only values within the boxed area that are within the service territory
    single_slice = ds.isel(time=0).isel(simulation=0).squeeze().compute()

    single_dim = single_slice.stack(spatial=["y", "x"])
    non_null = single_dim[~single_dim.isnull()]

    df = pd.DataFrame({"lat": non_null.lat.values, "lon": non_null.lon.values})

    return df
