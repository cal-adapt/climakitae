"""Tools for CAVA vulnerability assessment pilot"""

from climakitae.util.utils import get_closest_gridcell
from climakitae.core.data_load import load
from climakitae.core.data_interface import Select
from climakitae.core.data_export import export
import xarray as xr
import numpy as np
import warnings

# Ignore specific warnings about division errors to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")


def metric_agg(da, metric, threshold):
    """
    Calculate a metric on the given DataArray (da) using the specified metric type
    and threshold.

    Parameters:
    -----------
    da : xarray.DataArray
        Input DataArray to compute metric and treshold on.
    metric : str
        The metric type to calculate. Supported options are "min", "max",
        "mean", and "median".
    threshold : float
        The threshold value for comparison.

    Returns:
    --------
    metric_data : xarray.DataArray
        Resulting DataArray from the calculated metric and threshold applied.
    """
    metric_map = {
        "min": np.min,
        "max": np.max,
        "mean": np.mean,
        "median": np.median,
    }

    metric_data = (
        (da.resample(time="1D").reduce(metric_map[metric]) > threshold)
        .resample(time="1Y")
        .sum()
    )

    return metric_data


def check_and_set_params(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    metric_calc,
    threshold,
    downscaling_method="Dynamical",  # default for now ## mandatory
    approach="time",  # GWL to follow on ## mandatory
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=["SSP 3-7.0 -- Business as Usual"],
    variable="METRIC HERE",  ## mandatory, must eventually accept temp, precip, or heat index
    export_method="both",  # off-ramp, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
):
    """
    Validates parameters for the `cava_data` function, and returns transformed parameters, if needed.
    Transformed parameters returned: `ssp_selected`, `variable_type`.
    """
    # Validating lat/lon
    if "lat" not in input_locations.columns or "lon" not in input_locations.columns:
        raise ValueError("Input coordinates must have `lat` and `lon` columns.")
    elif (
        (input_locations.dtypes.lat != "float64")
        and (input_locations.dtypes.lat != "int64")
    ) or (
        (input_locations.dtypes.lon != "float64")
        and (input_locations.dtypes.lon != "int64")
    ):
        raise ValueError("Input lat/lon columns must be float64 or int64 types.")

    # Validating metric
    if metric_calc not in ["min", "max", "mean", "median"]:
        raise ValueError("Passed in metric must be min, max, median, or mean.")

    # Validating export method
    if export_method not in ["off-ramp", "calculate", "both"]:
        raise ValueError(
            "Passed in export method must be 'off-ramp', 'calculate', or 'both'"
        )

    # Validating variable
    if variable not in [
        "Air Temperature at 2m",
        "Precipitation (total)",
        "NOAA Heat Index",
    ]:
        raise ValueError(
            "Passed in variable must be `Air Temperature at 2m`, `Precipitation (total)`, or `NOAA Heat Index`"
        )

    # Calvin- Add unit validation for variable (can't pull degF for Precip)

    # Map SSP names from user inputs to selections SSPs
    ssp_mapping = {
        "SSP2-4.5": "SSP 2-4.5 -- Middle of the Road",
        "SSP3-7.0": "SSP 3-7.0 -- Business as Usual",
        "SSP5-8.5": "SSP 5-8.5 -- Burn it All",
    }
    ssp_selected = [ssp_mapping[ssp] for ssp in ssp_data]

    # Determining if passed in variable is a climate variable or a derived index
    if variable == "Air Temperature at 2m" or variable == "Precipitation (total)":
        variable_type = "Variable"
    elif variable == "NOAA Heat Index":
        variable_type = "Derived Index"

    return {"ssp_selected": ssp_selected, "variable_type": variable_type}


def cava_data(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    variable,  ## mandatory, must eventually accept temp, precip, or heat index
    metric_calc,
    threshold,
    downscaling_method="Dynamical",  # default for now ## mandatory
    approach="time",  # GWL to follow on ## mandatory
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=["SSP 3-7.0 -- Business as Usual"],
    export_method="both",  # off-ramp, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
):
    """
    Retrieves, processes, and exports climate data based on inputs, designed for CAVA reports.

    Parameters:
    -----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    time_start_year : int
        Starting year for data selection.
    time_end_year : int
        Ending year for data selection.
    units : str
        Units for the retrieved data.
    variable : str
        Type of climate variable to retrieve and calculate.
    metric_calc : str
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics.
    threshold : float
        Threshold value for counting events.
    downscaling_method : str, optional
        Method of downscaling, default is "Dynamical".
    approach : str, optional
        Approach to follow, default is "time".
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    export_method : str, optional
        Export method, options are 'off-ramp', 'calculate', 'both', default is 'both'.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.

    Returns:
    --------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises:
    -------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    clean_params = check_and_set_params(
        **locals()
    )  # This may not be best practice. Also to note, this line MUST be the first line (or at least before other local vars are defined), or else they will also be passed to `check_and_set_params`.

    print("--- Selecting Data Points --- \n")

    data_pts = []
    for index, loc in input_locations.iterrows():

        print(f"Selecting data for {loc['lat'], loc['lon']}")
        lat, lon = loc["lat"], loc["lon"]

        selections = Select()
        selections.data_type = "Gridded"
        selections.downscaling_method = downscaling_method
        selections.scenario_historical = [historical_data]
        selections.scenario_ssp = clean_params["ssp_selected"]
        selections.timescale = "hourly"
        selections.variable_type = clean_params["variable_type"]
        selections.variable = variable
        selections.latitude = (
            lat - 0.02,
            lat + 0.02,
        )  # Q: What padding to put for lat/lon values?
        selections.longitude = (lon - 0.02, lon + 0.02)
        selections.time_slice = (time_start_year, time_end_year)
        selections.resolution = "3 km"
        selections.units = units

        data = selections.retrieve()

        # Filter only for BC-WRF models
        data = data.sel(
            simulation=[
                "WRF_EC-Earth3_r1i1p1f1",
                "WRF_MPI-ESM1-2-HR_r3i1p1f1",
                "WRF_TaiESM1_r1i1p1f1",
                "WRF_MIROC6_r1i1p1f1",
            ]
        )

        # Find the closest gridcell from the data to the lat/lon point
        closest_cell = get_closest_gridcell(
            data, lat, lon, print_coords=False
        ).squeeze()

        # Storing data differently if all data points should be in one file vs each data point in its own file
        if not separate_files:
            # Renaming gridcell so that it can be concatenated with other lat/lon gridcells
            closest_cell["simulation"] = [
                "{}_{}_{}".format(
                    sim_name, closest_cell.lat.item(), closest_cell.lon.item()
                )
                for sim_name in closest_cell.simulation
            ]

        data_pts.append(closest_cell)

    # Loading the data differently if you want all data points in a single file vs separate files

    print("\n--- Loading Data into Memory ---\n")

    if separate_files:
        loaded_data = [
            (
                lambda point: (
                    print(f"Point ({data_pt.lat.item()}, {data_pt.lon.item()})"),
                    load(data_pt),
                    print("\n"),
                )
            )(data_pt)[
                1
            ]  # Prints the data, loads the data, and only stores the loaded data into `loaded_data`
            for data_pt in data_pts
        ]

    else:
        # Combine data points into a single xr.Dataset to load in and calculate metrics on
        data_pts = xr.concat(data_pts, dim="simulation").chunk(
            (1, len(closest_cell.time))
        )
        loaded_data = load(data_pts)

    # Export raw data if desired
    if export_method == "off-ramp" or export_method == "both":

        print("\n--- Offramp Exporting Raw Data ---")

        if separate_files:
            for point in loaded_data:
                export(
                    point,
                    filename=f"{variable.replace(' ', '_')}_{point.lat.item()}_{point.lon.item()}",  # Q: Do we want to include decimal points in filenames? Also, .nc or .csv?
                )
        else:
            export(loaded_data, filename="combined_raw_data")

        if export_method == "off-ramp":
            return loaded_data

    print("\n--- Calculating Metrics ---")

    # Run calculations or threshold counts if desired
    if (
        export_method == "calculate" or export_method == "both"
    ):  # Calvin- I think this condition is always true if you get to this part of the code

        if separate_files:

            metric_data = []
            # Calculate and export into separate files
            for point in loaded_data:
                calc_val = metric_agg(point, metric_calc, threshold)
                calc_val.name = f"Days per year with {metric_calc.capitalize()} Daily {variable} above threshold of {threshold} {units}"
                metric_data.append(calc_val)

        # Export a combined file of all metrics
        else:
            metric_data = metric_agg(loaded_data, metric_calc, threshold)
            metric_data.name = f"Days per year with {metric_calc.capitalize()} Daily {variable} above threshold of {threshold} {units}"

    # Export the data
    print("\n--- Exporting Metric Data ---")

    if separate_files:
        for point in metric_data:
            export(
                point,
                filename=f"{metric_calc.capitalize()}_{variable.replace(' ', '_')}_{threshold}_{point.lat.item()}_{point.lon.item()}",
            )  # Q: How do we want the file names to be stored?

    else:
        export(metric_data, filename="metric_data")

    # Return data based on export method, Q: How should we return the points? Dictionary of points to DataArrays?
    if export_method == "calculate":
        return metric_data
    elif export_method == "both":
        return loaded_data, metric_data
