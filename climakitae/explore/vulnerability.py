"""Tools for CAVA vulnerability assessment pilot"""

import warnings
import xarray as xr
import numpy as np
import pandas as pd
import param
from typing import Union

from climakitae.util.utils import (
    get_closest_gridcell,
    add_dummy_time_to_wl,
)
from climakitae.core.data_export import export
from climakitae.core.data_interface import DataParameters
from climakitae.core.data_load import load
from climakitae.core.constants import (
    WRF_BA_MODELS,
    SSPS,
    WRF_NO_0PT8_GWL_MODELS,
    LOCA_NO_0PT8_GWL_MODELS,
    WARMING_LEVELS,
)
from climakitae.explore import warming_levels
from climakitae.explore.threshold_tools import (
    get_block_maxima,
    get_return_value,
    get_ks_stat,
)
from climakitae.tools.batch import batch_select
from dask.array.core import PerformanceWarning

# Ignore specific warnings about division and performance to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")
warnings.filterwarnings("ignore", category=PerformanceWarning)


def _export_no_e(da, filename, format):
    """Exports a file but doesn't throw an exception if a file already exists."""
    try:
        export(
            da,
            filename=filename,
            format=format,
        )
    except Exception as e:
        print(e)


def _filter_ba_models(data, downscaling_method, wrf_bias_adjust, historical_data):
    """Filters the WRF data for the bias adjusted simulations, if desired."""
    if (
        downscaling_method == "Dynamical"
        and wrf_bias_adjust
        and historical_data != "Historical Reconstruction"
    ):
        # Filter only for BC-WRF models
        # This check is to see which simulations have these BC models as the root part of their simulation name, which is to accomodate for simulation names being the same across SSPs.
        # Not sure why this is selecting the right sims for batch mode but not leaving the lat/lons on their names
        data = data.sel(
            simulation=[
                sim
                for sim in data.simulation.values
                if any(s in sim for s in WRF_BA_MODELS)
            ]
        )
        print(f"Filtering WRF data for bias-adjusted simulations.")
    return data


def _filter_hist_gwl_models(data, downscaling_method, approach, warming_level):
    """Filters the data to remove simulation(s) that do not reach 0.8°C in GWL selection"""

    # Filter for viable 0.8°C simulations
    match downscaling_method:
        case "Dynamical":
            data = data.sel(
                simulation=[
                    sim
                    for sim in data.simulation.values
                    if sim not in WRF_NO_0PT8_GWL_MODELS
                ]
            )
        case "Statistical":
            data = data.sel(
                simulation=[
                    sim
                    for sim in data.simulation.values
                    if sim not in LOCA_NO_0PT8_GWL_MODELS
                ]
            )
        case _:
            raise ValueError(
                'downscaling_method needs to be either "Dynamical" or "Statistical"'
            )
    print(
        f"Not all {downscaling_method} simulations have data for {warming_level}°C. Removing invalid simulations. Please see Guidance materials for more information."
    )  # guidance forthcoming
    return data


def _metric_agg(
    da,
    variable,
    months,
    approach,
    metric,
    name_of_calc,
    heat_idx_threshold,
    percentile,
    one_in_x,
    event_duration,
    distr,
):
    """
    Calculate a metric on the given DataArray (da) using the specified metric type
    and heat index threshold.

    Parameters:
    -----------
    da : xarray.DataArray
        Input DataArray to compute metric and treshold on.
    metric : str
        The metric type to calculate. Supported options are "min", "max".
    heat_idx_threshold : float
        The heat index threshold for comparison.
    one_in_x : int
        Return period for 1-in-X events.
    percentile : int
        Percentile / quantile for calculating "likely".

    Returns:
    --------
    metric_data : xarray.DataArray
        Resulting DataArray from the calculated metric and heat index threshold applied.
    """
    metric_map = {
        "min": np.min,
        "max": np.max,
    }

    def _apply_metric(da):
        """Applies the hard-coded metrics to the DataArray"""
        if (
            heat_idx_threshold or heat_idx_threshold == 0
        ):  # If a threshold is passed and/or if the `heat_idx_threshold` passed in is 0, since that is a valid threshold.
            calc_val = (
                (da.resample(time="1D").reduce(metric_map[metric]) > heat_idx_threshold)
                .resample(time="1YE")
                .sum()
                .mean(dim="time")
            )  # This line counts the average number of high heat days per year over this time interval

        elif not any(
            x is None for x in one_in_x
        ):  # This is done since `one_in_x` param could hold multiple X values

            x_vals = [f"1-in-{x}" for x in one_in_x]
            print(
                f"Goodness of fit for {", ".join(x_vals)}, {'-'.join(map(str, event_duration))} {variable} events: "
            )

            # Get 1-in-x event values
            return_vals, p_vals = [], []
            for (
                sim
            ) in (
                da.simulation.values
            ):  # Can use xr.apply_ufunc, not just a for loop here

                one_da = da.sel(simulation=sim)
                print(f"Running on simulation: {sim}")

                # Removing small amounts of noise and 0-precip days from precipitation data
                if variable == "Precipitation (total)":
                    one_da = one_da.resample(time="1D").sum()

                    # Resampling can cause initially filtered out dates to reappear. Re-filtered in case that occurs.
                    one_da = one_da.sel(time=one_da.time.dt.month.isin(months))

                    # Remove trace precipitation, using a small finite limit
                    one_da = one_da.where(one_da > 10e-10, drop=True)

                # Computing block maxima
                groupby, duration = None, None
                if event_duration == (1, "day"):
                    groupby = event_duration
                elif event_duration[1] == "hour":
                    duration = event_duration

                ams = get_block_maxima(
                    one_da,
                    extremes_type=metric,
                    duration=duration,
                    groupby=groupby,
                    check_ess=False,
                ).squeeze()

                # Calculating 1-in-X return value
                return_values = get_return_value(
                    ams,
                    return_period=one_in_x,
                    multiple_points=False,
                    distr=distr,
                )["return_value"]
                return_vals.append(return_values)

                # Determining goodness-of-fit for the given distribution
                d_statistics, p_value = get_ks_stat(
                    ams, distr=distr, multiple_points=False
                ).data_vars.values()
                p_vals.append(p_value)

                # Printing out p-values
                p_val_print = (
                    format(p_value.item(), ".3e")
                    if p_value.item() < 0.05
                    else round(p_value.item(), 4)
                )
                to_print = f"The simulation {sim} fitted with a {distr} distribution has a p-value of {p_val_print}.\n"

                if p_value.item() < 0.05:
                    to_print += " Since the p-value is <0.05, the selected distribution does not fit the data well and therefore is not a good fit (see guidance)."
                print(to_print)

            # Creating output 1-in-X object with p-values
            ret_vals = xr.concat(return_vals, dim="simulation")
            p_vals = xr.concat(p_vals, dim="simulation")

            # Removing any potential `None` attributes in the 1 in X or p-value DataArrays (i.e. from duration=None)
            ret_vals.attrs = {k: v for k, v in ret_vals.attrs.items() if v is not None}
            p_vals.attrs = {k: v for k, v in p_vals.attrs.items() if v is not None}

            calc_val = xr.Dataset({"return_value": ret_vals, "p_values": p_vals})

            # Changing attributes for thresholds DataArray because of export issue later on
            num, size = event_duration
            calc_val.attrs["groupby"] = f"{num} {size}"

            # Add distribution to attributes
            calc_val = calc_val.assign_attrs(
                fitted_distr=distr,
                sample_size=len(ams),
            )

            # # Change `arg_value` dimension coordinates to `1-in-X` parameter and rename to `one_in_x`
            # calc_val = calc_val.assign_coords(arg_value=one_in_x).rename(
            #     {"arg_value": "one_in_x"}
            # )

        elif percentile or percentile == 0:

            calc_val = (
                da.resample(time="1D")
                .reduce(metric_map[metric])
                .quantile(percentile / 100, dim="time")
                .assign_coords(
                    lat=da.lat,
                    lon=da.lon,
                    quantile=percentile,  # Coords are dropped during quantile method application
                )
                .rename({"quantile": "percentile"})
            )

        if isinstance(calc_val, xr.DataArray):
            calc_val.name = name_of_calc

        return calc_val

    # If the data passed in is warming level data, then a dummy timestamp must be added to act as the current WL 'time' dimension
    if approach == "Warming Level":
        da = add_dummy_time_to_wl(da)

    metric_data = _apply_metric(da)
    return metric_data


class CavaParams(param.Parameterized):
    """
    Validates parameters for the `cava_data` function, and returns transformed parameters, if needed.
    Transformed parameters returned: `ssp_selected`, `variable_type`.
    """

    input_locations = param.DataFrame(
        doc="Input coordinates must have 'lat' and 'lon' columns."
    )
    time_start_year = param.Integer(bounds=(1900, 2100), doc="Start year", default=1981)
    time_end_year = param.Integer(bounds=(1900, 2100), doc="End year", default=2010)

    # Need to find all valid units by variable name
    units = param.String(default="Celsius", doc="Units for temperature measurement")

    variable = param.ObjectSelector(
        default="Air Temperature at 2m",
        objects=["Air Temperature at 2m", "Precipitation (total)", "NOAA Heat Index"],
        doc="Variable to analyze",
    )
    metric_calc = param.ObjectSelector(
        default="max",
        objects=["min", "max", "mean", "median"],
        doc="Metric calculation method",
    )
    percentile = param.Number(
        default=None, bounds=(0, 100), allow_None=True, doc="Percentile for calculation"
    )
    heat_idx_threshold = param.Number(
        default=None, allow_None=True, doc="Threshold for heat index"
    )
    one_in_x = param.Parameter(
        default=None, allow_None=True, doc="1-in-x year event(s) — number or list"
    )
    season = param.ObjectSelector(
        default="all", objects=["summer", "winter", "all"], doc="Season to analyze"
    )
    downscaling_method = param.ObjectSelector(
        default="Dynamical",
        objects=["Dynamical", "Statistical"],
        doc="Downscaling method",
    )
    approach = param.ObjectSelector(
        default="Time", objects=["Time", "Warming Level"], doc="Approach to use"
    )
    warming_level = param.Number(
        default=1.0, bounds=(0, 7)
    )  # Bounds will be further enforced once `DataParameters` is replaced with `get_data`
    wrf_bias_adjust = param.Boolean(default=True, doc="Flag for WRF bias adjustment")
    historical_data = param.ObjectSelector(
        default="Historical Climate",
        objects=["Historical Climate", "Historical Reconstruction"],
        doc="Type of historical data",
    )
    ssp_data = param.ListSelector(
        default=["SSP 3-7.0"],
        objects=["SSP 2-4.5", "SSP 3-7.0", "SSP 5-8.5"],
        doc="SSP data scenarios to use",
    )
    export_method = param.ObjectSelector(
        default="both",
        objects=["raw", "calculate", "both", "None"],
        doc="Export method",
    )
    separate_files = param.Boolean(
        default=False,
        doc="Whether to separate climate variable information into separate files",
    )
    file_format = param.String(default="NetCDF", doc="Format of the output files")
    batch_mode = param.Boolean(default=False, doc="Flag for batch mode operation")
    distr = param.ObjectSelector(
        default="gev",
        objects=["gev", "genpareto", "gamma"],
        doc="Distribution function",
    )
    event_duration = param.Tuple(default=(1, "day"))

    def __init__(self, **params):
        super().__init__(**params)
        self.validate_params()

    def validate_params(self):
        errors = []

        # Change `one_in_x` type before param validation and arg generation
        one_in_x = self.one_in_x
        match one_in_x:
            case one_in_x if not isinstance(one_in_x, (list)):
                self.one_in_x = np.array([one_in_x])
            case one_in_x if isinstance(one_in_x, list):
                self.one_in_x = np.array(one_in_x)

        # Check if input_locations is a DataFrame with lat/lon columns
        if isinstance(self.input_locations, pd.DataFrame):
            if (
                "lat" not in self.input_locations.columns
                or "lon" not in self.input_locations.columns
            ):
                errors.append("Input coordinates must have `lat` and `lon` columns.")
            elif not pd.api.types.is_numeric_dtype(
                self.input_locations["lat"]
            ) or not pd.api.types.is_numeric_dtype(self.input_locations["lon"]):
                errors.append("Input lat/lon columns must be float64 or int64 types.")

        # Validating that time start year < time end year
        if self.time_start_year > self.time_end_year:
            errors.append(
                "Start year must come before, or be the same year as, the end year."
            )

        # Validate year range for historical reconstruction
        if (
            self.historical_data == "Historical Reconstruction"
            and self.time_end_year > 2022
        ):
            errors.append(
                "End year for Historical Reconstruction data must be 2022 or earlier."
            )

        # Validate time approach with Historical Reconstruction
        if (
            self.historical_data == "Historical Reconstruction"
            and self.approach != "Time"
        ):
            errors.append(
                "Historical Reconstruction data can only be retrieved using a time-based approach."
            )

        # Validating that only one of heat_idx_threshold, percentile, or one_in_x is passed
        if (
            (self.heat_idx_threshold is not None and self.percentile is not None)
            or (
                self.heat_idx_threshold is not None
                and not any(x is None for x in self.one_in_x)
            )
            or (
                self.percentile is not None
                and not any(x is None for x in self.one_in_x)
            )
        ):
            errors.append(
                "Only one of heat_idx_threshold, one_in_x, and percentile can be non-None."
            )

        # Check that not all 3 customizable metric arguments are none.
        if (
            self.heat_idx_threshold is None
            and self.percentile is None
            and all(x is None for x in self.one_in_x)
        ):
            errors.append(
                "`heat_idx_threshold`, `percentile`, and `one_in_x` cannot all be None."
            )

        # Make sure the metric is 'min' or 'max' if percentile is passed in
        if self.percentile is not None and self.metric_calc not in ["min", "max"]:
            errors.append(
                "Metric calculation must be 'min' or 'max' for percentile calculations."
            )

        # Validating variable and downscaling method combo
        if (
            self.variable == "NOAA Heat Index"
            and self.downscaling_method == "Statistical"
        ):
            errors.append(
                "`NOAA Heat Index` cannot be used with Statistical downscaling method."
            )

        # Only allow for SSP 3-7.0 for WRF data
        if self.downscaling_method == "Dynamical" and self.approach == "Time":
            if len(self.ssp_data) > 1 or self.ssp_data[0] != "SSP 3-7.0":
                errors.append(
                    "Can only use SSP 3-7.0 data for a time-based approach using WRF data."
                )

        # Only allow `hour` or `daily` frequencies for 1-in-X event counts
        dur_len, dur_type = self.event_duration
        if dur_type != "hour" and dur_type != "day":
            raise ValueError(
                "Current specifications not implemented. `event_duration` options only implemented for `hour` or `daily` frequency."
            )

        # Raise errors if any
        if errors:
            raise ValueError("Parameter validation errors:\n\n" + "\n".join(errors))

    def get_names(self):

        # Re-assigning `variable` in case it needs to be re-instantiated later.
        variable = self.variable

        # Determine variable type
        if variable in ["Air Temperature at 2m", "Precipitation (total)"]:
            variable_type = "Variable"
        else:
            variable_type = "Derived Index"

        # Adjust variable name for LOCA2 data
        if (
            variable == "Air Temperature at 2m"
            and self.downscaling_method == "Statistical"
        ):
            match self.metric_calc:
                case "max":
                    variable = "Maximum air temperature at 2m"
                case "min":
                    variable = "Minimum air temperature at 2m"
                case _:
                    raise ValueError('metric_calc needs to be either "max" or "min"')

        # Reducing potential decimal places for warming level
        self.warming_level = round(self.warming_level, 3)

        # Adding time period or warming level to filename
        match self.approach:
            case "Time":
                approach_str = f"{self.time_start_year}_to_{self.time_end_year}"
                approach_var_name = f"from {approach_str}"
            case "Warming Level":
                if float(self.warming_level).is_integer():
                    wl_str = str(int(self.warming_level))
                else:
                    wl_str = str(self.warming_level).replace(".", "pt")

                approach_str = f"{wl_str}degreeWL"
                approach_var_name = f"for Warming Level {self.warming_level}°C"
            case _:
                raise ValueError(
                    'approach needs to be either "Time" or "Warming Level"'
                )
        # Retrieving the name of the calculation about to occur
        if self.percentile is not None:

            def ordinal(n):
                """Find ordinal name for a number (1 -> 1st, 2 -> 2nd, 3 -> 3rd)"""
                return f"{n}{('th' if 11 <= n % 100 <= 13 else {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th'))}"

            var_name = f"{ordinal(self.percentile)} percentile of Daily {self.metric_calc.capitalize()} of {variable} for {self.season} seasons {approach_var_name}"
            raw_name = "likely_seasonal_temperature_raw_data"
            calc_name = (
                f"likely_seasonal_{self.season}_{self.metric_calc}_{approach_str}"
            )

        elif self.heat_idx_threshold is not None:
            var_name = f"Days per year with {self.metric_calc.capitalize()} Daily {variable} above a heat index threshold of {self.heat_idx_threshold} {self.units} for {self.season} seasons {approach_var_name}"
            raw_name = "heat_index_raw_data"
            calc_name = f"heat_index_thresh{self.heat_idx_threshold}_{approach_str}"

        elif not any(x is None for x in self.one_in_x):

            x_names = [
                f"1-in-{x} year, {'-'.join(map(str, self.event_duration))} {self.metric_calc.capitalize()} {variable} for {self.season} seasons {approach_var_name}"
                for x in self.one_in_x
            ]
            var_name = " and ".join(x_names)

            # Making names for raw and calculated data
            x_vals_str = "-".join(map(str, self.one_in_x))

            dur_len, dur_type = self.event_duration
            shortname_event = f"{dur_len}_{dur_type if dur_type == 'day' else 'hr'}"
            match variable:
                case "Precipitation (total)":
                    raw_name = f"one_in_{x_vals_str}_precipitation_raw_data"
                    calc_name = f"one_in_{x_vals_str}_{shortname_event}_precipitation_{approach_str}"
                case "Air Temperature at 2m" | "Maximum air temperature at 2m":
                    raw_name = f"one_in_{x_vals_str}_temperature_raw_data"
                    calc_name = f"one_in_{x_vals_str}_{shortname_event}_temperature_{approach_str}"
                case _:
                    raise ValueError(
                        'variable needs to be either "Precipitation (total)", "Air Temperature at 2m", or "Maximum air temperature at 2m"'
                    )

        return {
            "ssp_selected": self.ssp_data,
            "variable": variable,
            "variable_type": variable_type,
            "var_name": var_name,
            "raw_name": raw_name,
            "calc_name": calc_name,
        }


def cava_data(
    input_locations,  # csv file
    variable,
    units=None,
    approach="Time",  # default
    downscaling_method="Dynamical",  # default
    time_start_year=1981,  # default, optional
    time_end_year=2010,  # default, optional
    historical_data="Historical Climate",  # default
    ssp_data=["SSP 3-7.0"],  # default
    warming_level=1.5,  # default
    metric_calc="max",  # default
    heat_idx_threshold=None,
    one_in_x=None,
    event_duration=(1, "day"),
    percentile=None,
    season="all",  # default to all so no subsetting occurs unless called
    wrf_bias_adjust=True,
    export_method="both",  # raw, full calculate, both
    separate_files=True,  # toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format="NetCDF",
    batch_mode=False,
    distr="gev",
):
    """Retrieve, process, and export climate data based on inputs.

    Designed for CAVA reports.

    Parameters
    ----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    variable : str
        Type of climate variable to retrieve and calculate.
    approach : str
        Approach to follow, default is "Time".
    downscaling_method : str
        Method of downscaling, default is "Dynamical".
    time_start_year : int, optional
        Starting year for data selection.
    time_end_year : int, optional
        Ending year for data selection.
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP 3-7.0".
    warming_level : str, optional
        Global warming levels, default is 1.5°C.
    metric_calc : str, optional
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics. Default is "max"
    heat_idx_threshold : float
        Heat index threshold for counting events.
    one_in_x : int, optional
        Return period for 1-in-X events.
    event_duration: tuple (int, str), optional
        Duration of event and time unit (e.g. (1, "day"))
    percentile : int, optional
        Percentile for calculating "likely" event occurrence.
    season : str, optional
        Season to subset time dimension on (e.g., 'summer', 'winter', 'all'). Default is 'all'.
    units : str, optional
        Units for the retrieved data.
    wrf_bias_adjust : str, optional
        Flag to subset the WRF data for the bias-adjusted models. Default is True.
    export_method : str, optional
        Export method, options are 'raw', 'calculate', 'both', default is 'both'.
    file_format : str, optional
        Export file format options.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.

    Returns
    -------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises
    ------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    with param.exceptions_summarized():
        params = CavaParams(**locals())
        clean_params = params.get_names()

        # Convert string input to list
        # ssp_data input requires a list even if the user is just inputting a single value
        # i.e. ssp_data = "SSP 3-7.0" is NOT valid; needs to be ssp_data = ["SSP 3-7.0]
        # Here, we catch that error for the user and just convert the string to a list :D
        if isinstance(ssp_data, str):
            ssp_data = [ssp_data]

        # Convert `one_in_x` param to a np.ndarray object instead of a list, if it is currently a list
        if not isinstance(one_in_x, (list, np.ndarray)):
            one_in_x = np.array([one_in_x])
        elif isinstance(one_in_x, list):
            one_in_x = np.array(one_in_x)

        months_map = {
            "winter": [12, 1, 2],
            "summer": [6, 7, 8],
            "all": list(np.arange(1, 13)),
        }

        locations = input_locations[["lat", "lon"]].values

        print(f"Calculating {clean_params['var_name']}")

        print(f"--- Selecting Data Points --- \n")

        selections = DataParameters()
        selections.approach = approach
        selections.data_type = "Gridded"
        selections.downscaling_method = downscaling_method
        selections.timescale = (
            "hourly" if downscaling_method == "Dynamical" else "daily"
        )
        selections.variable_type = clean_params["variable_type"]
        selections.variable = clean_params["variable"]
        selections.resolution = "3 km"
        selections.units = units

        # Setting WL vs time-based attributes
        if approach == "Warming Level":
            selections.warming_level = [float(warming_level)]
            selections.warming_level_months = months_map[season]
            selections.scenario_ssp = ["n/a"]
            selections.scenario_historical = ["n/a"]

        else:
            selections.scenario_ssp = (
                clean_params["ssp_selected"]
                if historical_data != "Historical Reconstruction"
                else []
            )
            selections.scenario_historical = [historical_data]
            selections.time_slice = (time_start_year, time_end_year)

        if batch_mode and downscaling_method == "Statistical":
            print(
                "Batch mode for Statistical data in Warming Level approach is not optimized for multiple locations (in development). Resetting `batch_mode` to False. This may take some time because each location is retrieved and returned separately.\n"
            )
            batch_mode = False

        if batch_mode:
            separate_files = False
            data_pts = batch_select(approach, selections, locations)

            data = _filter_ba_models(
                data_pts, downscaling_method, wrf_bias_adjust, historical_data
            )

            if approach == "Warming Level" and warming_level == 0.8:
                # Filter out inappropriate models for historical GWL baseline
                data = _filter_hist_gwl_models(
                    data, downscaling_method, approach, warming_level
                )

        else:
            data_pts = []
            for idx, loc in input_locations.iterrows():

                lat, lon = float(loc["lat"]), float(loc["lon"])
                print(f"Selecting data for {lat, lon}")

                selections.latitude = (lat - 0.02, lat + 0.02)
                selections.longitude = (lon - 0.02, lon + 0.02)
                data = selections.retrieve()

                if approach == "Time":
                    # Remove leap days, if applicable
                    data = data.sel(
                        time=~((data.time.dt.month == 2) & (data.time.dt.day == 29))
                    )

                data = get_closest_gridcell(data, lat, lon, print_coords=False)

                data = _filter_ba_models(
                    data, downscaling_method, wrf_bias_adjust, historical_data
                )

                if approach == "Warming Level" and warming_level == 0.8:
                    # Filter out inappropriate models for historical GWL baseline
                    data = _filter_hist_gwl_models(
                        data, downscaling_method, approach, warming_level
                    )
                data_pts.append(data)

        if historical_data == "Historical Reconstruction":
            data = data.squeeze(dim="scenario")

        # Determining how to concatenate or load data, if necessary, depending on `approach` and `separate_files` parameters.
        print("\n--- Loading Data into Memory ---\n")

        if separate_files:
            loaded_data = []
            for point in data_pts:
                print(
                    f"Point ({point.lat.compute().item()}, {point.lon.compute().item()})"
                )
                data = load(point, progress_bar=True)
                loaded_data.append(data)

        else:
            data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
            loaded_data = load(data_pts, progress_bar=True)

        # Export raw data if desired
        if export_method == "raw" or export_method == "both":

            print("\n--- Exporting Raw Data ---")

            if downscaling_method == "Statistical":
                print(
                    "\nExporting Statistical data in most granular time availability (daily)\n"
                )

            if separate_files:
                for pt_idx in range(len(loaded_data)):
                    _export_no_e(
                        loaded_data[pt_idx],
                        filename=f"{clean_params['raw_name']}_{str(round(loaded_data[pt_idx].lat.item(), 3)).replace('.', '')}N_{str(round(loaded_data[pt_idx].lon.item(), 3)).replace('.', '')}W",
                        format=file_format,
                    )
            else:
                _export_no_e(
                    loaded_data, filename="combined_raw_data", format=file_format
                )

            if export_method == "raw":
                return loaded_data

        print("\n--- Calculating Metrics ---")
        print("Calculating...\n")  # , end="", flush=True)

        if separate_files:
            metric_data = []
            # Calculate and export into separate files
            for point in loaded_data:
                calc_val = _metric_agg(
                    point,
                    variable,
                    months_map[season],
                    approach,
                    metric_calc,
                    clean_params["var_name"],
                    heat_idx_threshold,
                    percentile,
                    one_in_x,
                    event_duration,
                    distr,
                )
                metric_data.append(calc_val)

        # Export a combined file of all metrics
        else:
            metric_data = _metric_agg(
                loaded_data,
                variable,
                months_map[season],
                approach,
                metric_calc,
                clean_params["var_name"],
                heat_idx_threshold,
                percentile,
                one_in_x,
                event_duration,
                distr,
            )

        print("\nComplete!")

        # Export the data
        print("\n--- Exporting Metric Data ---")

        match export_method:
            case "calculate" | "both":
                if separate_files:
                    for pt_idx in range(len(metric_data)):
                        _export_no_e(
                            metric_data[pt_idx],
                            filename=f"{clean_params['calc_name']}_{str(round(metric_data[pt_idx].lat.item(), 3)).replace('.', '')}N_{str(round(metric_data[pt_idx].lon.item(), 3)).replace('.', '')}W",
                            format=file_format,
                        )  # Will need to include naming convention for calculated file too.
                else:
                    _export_no_e(
                        metric_data, filename="metric_data", format=file_format
                    )
                # Returning values to user
                if len(metric_data) == 1:
                    metric_data = metric_data[0]
                if len(loaded_data) == 1:
                    loaded_data = loaded_data[0]

                match export_method:
                    case "calculate":
                        return {"calc_data": metric_data}
                    case "both":
                        return {"calc_data": metric_data, "raw_data": loaded_data}

            case "None":  # Specific for table generation
                print("Data export selection set to 'None'; no data is exported!")
                return {"calc_data": metric_data}
