"""Tools for CAVA vulnerability assessment pilot"""

from climakitae.util.utils import get_closest_gridcell
from climakitae.core.data_load import load
from climakitae.core.data_interface import Select
from climakitae.core.data_export import export
import xarray as xr
import warnings

# Ignore specific warnings about division errors to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")


def cava_data(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    downscaling_method="Dynamical",  # default for now ## mandatory
    approach="time",  # GWL to follow on ## mandatory
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=[
        "SSP 3-7.0 -- Business as Usual"
    ],  # Q: We have this encoded differently in AE, should we have an internal mapping?
    variable="METRIC HERE",  ## mandatory, Q: Should we also include a threshold input?
    raw_export="offramp",  # off-ramp, full calculate, both
):
    """
    Retrieves, processes, and exports climate data based on inputs, designed for CAVA reports.

    Parameters:
    -----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    downscaling_method : str, optional
        Method of downscaling, default is "Dynamical".
    approach : str, optional
        Approach to follow, default is "time".
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    time_start_year : int, optional
        Starting year for data selection.
    time_end_year : int, optional
        Ending year for data selection.
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    variable : str
        Type of climate variable to retrieve and calculate.
    units : str
        Units for the retrieved data, default is "degF".
    raw_export : str, optional
        Export option, default is "offramp".

    Returns:
    --------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises:
    -------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """

    if "lat" not in input_locations.columns or "lon" not in input_locations.columns:
        raise ValueError("Input coordinates must have `lat` and `lon` columns.")
    elif (
        (input_locations.dtypes.lat != "float64")
        and (input_locations.dtypes.lat != "int64")
    ) or (
        (input_locations.dtypes.lon != "float64")
        and (input_locations.dtypes.lon != "int64")
    ):
        raise ValueError("Input lat/lon columns must be float64 or int64 types.")

    data_pts = []

    print("--- Selecting Data Points --- \n")

    for index, loc in input_locations.iterrows():

        print(f"Selecting data for {loc['lat'], loc['lon']}")
        lat, lon = loc["lat"], loc["lon"]

        selections = Select()
        selections.data_type = "Gridded"
        selections.downscaling_method = downscaling_method
        selections.scenario_historical = [historical_data]
        selections.timescale = "hourly"
        selections.variable_type = "Derived Index"
        selections.variable = (
            "NOAA Heat Index"  # Q: How does this get passed in as `variable`?
        )
        selections.scenario_ssp = ssp_data
        selections.latitude = (
            lat - 0.02,
            lat + 0.02,
        )  # Q: What padding to put for lat/lon values?
        selections.longitude = (lon - 0.02, lon + 0.02)
        selections.time_slice = (time_start_year, time_end_year)
        selections.resolution = "3 km"
        selections.units = units

        data = selections.retrieve()

        # Filter only for BC-WRF models
        data = data.sel(
            simulation=[
                "WRF_EC-Earth3_r1i1p1f1",
                "WRF_MPI-ESM1-2-HR_r3i1p1f1",
                "WRF_TaiESM1_r1i1p1f1",
                "WRF_MIROC6_r1i1p1f1",
            ]
        )

        # Find the closest gridcell from the data to the lat/lon point
        closest_cell = get_closest_gridcell(
            data, lat, lon, print_coords=False
        ).squeeze()

        # Renaming gridcell so that it can be concatenated with other lat/lon gridcells
        closest_cell["simulation"] = [
            "{}_{}_{}".format(
                sim_name, closest_cell.lat.item(), closest_cell.lon.item()
            )
            for sim_name in closest_cell.simulation
        ]

        data_pts.append(closest_cell)

    # Combine data points into a single xr.Dataset to load in and calculate metrics on
    data_pts = xr.concat(data_pts, dim="simulation").chunk((1, len(closest_cell.time)))
    print("\n--- Loading Data into Memory ---")
    loaded_data = load(data_pts)

    # Calculate metrics -- (hard coded for now)
    print("\n--- Calculating Metrics ---")
    hi_threshold = 91
    metric_data = (
        (loaded_data.resample(time="1D").max() > hi_threshold).resample(time="1Y").sum()
    )
    metric_data.name = "Days per year above Heat Index threshold of {}Â°F".format(
        hi_threshold
    )

    # Export the data
    print("\n--- Exporting Data ---")
    export(metric_data, filename="metric_data")

    return metric_data
