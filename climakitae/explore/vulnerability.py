"""Tools for CAVA vulnerability assessment pilot"""

import warnings

import numpy as np
import pandas as pd
import param
import xarray as xr
from dask.array.core import PerformanceWarning

from climakitae.core.constants import (
    LOCA_NO_0PT8_GWL_MODELS,
    UNSET,
    WRF_BA_MODELS,
    WRF_NO_0PT8_GWL_MODELS,
)
from climakitae.core.data_export import export
from climakitae.core.data_interface import DataParameters
from climakitae.core.data_load import load
from climakitae.explore.threshold_tools import (
    get_block_maxima,
    get_ks_stat,
    get_return_value,
)
from climakitae.tools.batch import batch_select
from climakitae.util.utils import add_dummy_time_to_wl, get_closest_gridcell

# Ignore specific warnings about division and performance to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")
warnings.filterwarnings("ignore", category=PerformanceWarning)


def _export_no_e(da: xr.DataArray | xr.Dataset, filename: str, file_format: str):
    """
    Export a file but doesn't throw an exception if a file already exists.

    Parameters
    ----------
    data : xr.DataArray or xr.Dataset
        Data to export, as output by e.g. `DataParameters.retrieve()`.
    filename : str, optional
        Output file name (without file extension, i.e. "my_filename" instead
        of "my_filename.nc"). The default is "dataexport".
    format : str, optional
        File format ("Zarr", "NetCDF", "CSV"). The default is "NetCDF".

    Returns
    -------
     None
    """
    try:
        export(
            da,
            filename=filename,
            format=file_format,
        )
    except Exception as e:
        print(e)


def _filter_ba_models(
    data: xr.DataArray,
    downscaling_method: str,
    wrf_bias_adjust: bool,
    historical_data: str,
) -> xr.DataArray:
    """
    Filter data to include only bias-adjusted WRF models when specific conditions are met.
    This function filters the input data to retain only simulations that correspond to
    bias-adjusted Weather Research and Forecasting (WRF) models when dynamical
    downscaling with bias adjustment is requested and historical reconstruction is
    not being used.

    Parameters
    ----------
    data : xr.DataArray
        Input climate data array containing simulation data across multiple models.
    downscaling_method : str
        The downscaling method being used. Must be "Dynamical" for filtering to occur.
    wrf_bias_adjust : bool
        Whether bias adjustment should be applied to WRF models. Must be True for
        filtering to occur.
    historical_data : str
        Type of historical data being used. Filtering occurs when this is not
        "Historical Reconstruction".

    Returns
    -------
    xr.DataArray
        Filtered data array containing only bias-adjusted WRF model simulations
        when conditions are met, otherwise returns the original data unchanged.

    Notes
    -----
    The function uses WRF_BA_MODELS constant to identify which simulations
    correspond to bias-adjusted WRF models by checking if any of the model
    names appear as substrings in the simulation names.
    """

    if (
        downscaling_method == "Dynamical"
        and wrf_bias_adjust
        and historical_data != "Historical Reconstruction"
    ):
        # Filter only for BC-WRF models
        # This check is to see which simulations have these BC models as the root part
        # of their simulation name, which is to accomodate for simulation names being
        # the same across SSPs.
        data = data.sel(
            simulation=[
                sim
                for sim in data.simulation.values
                if any(s in sim for s in WRF_BA_MODELS)
            ]
        )
        """
        Not sure why this is selecting the right sims for batch mode but not leaving the
        lat/lons on their names...

        What Should Happen:
            In batch mode, after get_closest_gridcells concatenates the data along the 
            points dimension, there should be a step that transforms the simulation 
            names to include location information (similar to what 
            stack_sims_across_locs does), so that each simulation-location combination 
            gets a unique identifier like "model_name_lat_lon".

        The Root Cause:
            The _filter_ba_models function expects simulation names that distinguish 
            between different locations when working with batch mode data, but the 
            current batch processing pipeline doesn't create these location-specific 
            simulation names before the filtering step.
        """
        print("Filtering WRF data for bias-adjusted simulations.")
    return data


def _filter_hist_gwl_models(
    data: xr.DataArray, downscaling_method: str, warming_level: str
) -> xr.DataArray:
    """
    Filter climate model simulations based on their availability for specific global warming levels.

    This function removes simulations that do not have data available for the specified
    warming level (typically 0.8°C) based on the downscaling method used. Different
    downscaling methods have different sets of models that lack data for certain
    warming levels.

    Parameters
    ----------
    data : xr.DataArray
        Input climate data array containing simulation dimension with model data.
    downscaling_method : str
        The downscaling method used. Must be either "Dynamical" or "Statistical".
        - "Dynamical": Uses WRF downscaling, filters against WRF_NO_0PT8_GWL_MODELS
        - "Statistical": Uses LOCA downscaling, filters against LOCA_NO_0PT8_GWL_MODELS
    warming_level : str
        The global warming level being analyzed (e.g., "0.8" for 0.8°C warming).
        Used for informational messages about filtering.

    Returns
    -------
    xr.DataArray
        Filtered data array with only simulations that have data available for the
        specified warming level.

    Raises
    ------
    ValueError
        If downscaling_method is not "Dynamical" or "Statistical".

    Notes
    -----
    This function prints an informational message about the filtering process
    and refers users to guidance materials for more information about which
    models are excluded.
    """

    # Filter for viable 0.8°C simulations
    match downscaling_method:
        case "Dynamical":
            data = data.sel(
                simulation=[
                    sim
                    for sim in data.simulation.values
                    if sim not in WRF_NO_0PT8_GWL_MODELS
                ]
            )
        case "Statistical":
            data = data.sel(
                simulation=[
                    sim
                    for sim in data.simulation.values
                    if sim not in LOCA_NO_0PT8_GWL_MODELS
                ]
            )
        case _:
            raise ValueError(
                'downscaling_method needs to be either "Dynamical" or "Statistical"'
            )
    print(
        f"Not all {downscaling_method} simulations have data for {warming_level}°C. Removing invalid simulations. Please see Guidance materials for more information."
    )  # guidance forthcoming
    return data


def _metric_agg(
    da: xr.DataArray,
    variable: str,
    months: list[int],
    approach: str,
    metric: str,
    name_of_calc: str,
    heat_idx_threshold: float,
    percentile: int,
    one_in_x: list[int] | None,  # Update type hint
    event_duration: tuple[int, str],
    distr: str,
):
    """
    Calculate a metric on the given DataArray (da) using the specified metric type and heat index threshold.

    Parameters
    ----------
    da : xarray.DataArray
        Input DataArray to compute metric and treshold on.
    variable : str
        Type of climate variable to retrieve and calculate.
    months: list[int]
        list of months by month index in year
    approach : str
        Approach to follow, default is "Time".
    metric : str
        The metric type to calculate. Supported options are "min", "max".
    name_of_calc: str
        Name to apply to calucation results
    heat_idx_threshold : float
        The heat index threshold for comparison.
    percentile : int
        Percentile / quantile for calculating "likely".
    one_in_x : list[int] | None
        Return period for 1-in-X events.
    event_duration: tuple[int, str]
        Duration of event and time unit (e.g. (1, "day"))
    distr : str
        Name of distribution to use

    Returns
    -------
    xarray.DataArray
        Resulting DataArray from the calculated metric and heat index threshold applied.
    """
    metric_map = {
        "min": np.min,
        "max": np.max,
    }

    def _apply_metric(da):
        """
        Apply a statistical metric to climate data based on specified parameters.
        This function processes climate data to calculate various metrics including heat index
        thresholds, return period events (1-in-X year events), or percentile values.

        Parameters
        ----------
        da : xarray.DataArray
            Input data array containing climate data with time, simulation, lat, and lon dimensions.

        Returns
        -------
        xarray.DataArray or xarray.Dataset
            Processed data with applied metric. Returns Dataset for 1-in-X calculations
            (containing 'return_value' and 'p_values'), DataArray otherwise.

        Notes
        -----
        The function operates in three main modes based on the parameters provided:
        1. Heat index threshold mode: Calculates average number of days per year exceeding threshold
        2. 1-in-X return period mode: Computes return values for extreme events with goodness-of-fit
        3. Percentile mode: Calculates specified percentile of daily values
        The function relies on several external variables and functions:
        - heat_idx_threshold: Threshold value for heat index calculations
        - one_in_x: List of return periods for extreme value analysis
        - event_duration: Tuple of (number, unit) for event duration
        - variable: String indicating the climate variable being analyzed
        - months: List of months to include in analysis
        - metric: Type of extreme value metric to apply
        - metric_map: Dictionary mapping metric names to reduction functions
        - percentile: Percentile value to calculate (0-100)
        - distr: Statistical distribution for fitting extreme values
        - name_of_calc: Name to assign to output DataArray
        Special handling is applied for precipitation data to remove trace amounts
        and noise before analysis.
        """

        if (
            heat_idx_threshold or heat_idx_threshold == 0
        ):  # If a threshold is passed and/or if the `heat_idx_threshold` passed in is 0, since that is a valid threshold.
            calc_val = (
                (da.resample(time="1D").reduce(metric_map[metric]) > heat_idx_threshold)
                .resample(time="1YE")
                .sum()
                .mean(dim="time")
            )  # This line counts the average number of high heat days per year over this time interval

        elif not any(
            x is None for x in one_in_x
        ):  # This is done since `one_in_x` param could hold multiple X values

            x_vals = [f"1-in-{x}" for x in one_in_x]
            print(
                f"Goodness of fit for {', '.join(x_vals)}, {'-'.join(map(str, event_duration))} {variable} events: "
            )

            # Get 1-in-x event values
            return_vals, p_vals = [], []
            for (
                sim
            ) in (
                da.simulation.values
            ):  # Can use xr.apply_ufunc, not just a for loop here

                one_da = da.sel(simulation=sim)
                print(f"Running on simulation: {sim}")

                # Removing small amounts of noise and 0-precip days from precipitation data
                if variable == "Precipitation (total)":
                    one_da = one_da.resample(time="1D").sum()

                    # Resampling can cause initially filtered out dates to reappear. Re-filtered in case that occurs.
                    one_da = one_da.sel(time=one_da.time.dt.month.isin(months))

                    # Remove trace precipitation, using a small finite limit
                    one_da = one_da.where(one_da > 10e-10, drop=True)

                # Computing block maxima
                groupby, duration = UNSET, UNSET
                if event_duration == (1, "day"):
                    groupby = event_duration
                elif event_duration[1] == "hour":
                    duration = event_duration

                ams = get_block_maxima(
                    one_da,
                    extremes_type=metric,
                    duration=duration,
                    groupby=groupby,
                    check_ess=False,
                ).squeeze()

                # Calculating 1-in-X return value
                return_value_result = get_return_value(
                    ams,
                    return_period=one_in_x,  # This should be a list
                    multiple_points=False,
                    distr=distr,
                )["return_value"]
                return_vals.append(return_value_result)

                # Determining goodness-of-fit for the given distribution
                ks_result = get_ks_stat(ams, distr=distr, multiple_points=False)
                _, p_value = list(ks_result.data_vars.values())
                p_vals.append(p_value)

                # Printing out p-values
                if p_value.size == 1:
                    p_val_print = (
                        format(p_value.item(), ".3e")
                        if p_value.item() < 0.05
                        else round(p_value.item(), 4)
                    )
                    to_print = f"The simulation {sim} fitted with a {distr} distribution has a p-value of {p_val_print}.\n"
                    if p_value.item() < 0.05:
                        to_print += " Since the p-value is <0.05, the selected distribution does not fit the data well and therefore is not a good fit (see guidance)."
                else:
                    p_val_print = [round(float(v), 4) for v in p_value.values]
                    to_print = f"The simulation {sim} fitted with a {distr} distribution has p-values of {p_val_print}.\n"
                    if any(v < 0.05 for v in p_val_print):
                        to_print += " At least one p-value is <0.05, the selected distribution does not fit the data well and therefore is not a good fit (see guidance)."

                print(to_print)

            # Creating output 1-in-X object with p-values
            ret_vals = xr.concat(return_vals, dim="simulation")
            p_vals = xr.concat(p_vals, dim="simulation")

            # Removing any potential `None` attributes in the 1 in X or p-value DataArrays (i.e. from duration=None)
            ret_vals.attrs = {k: v for k, v in ret_vals.attrs.items() if v is not None}
            p_vals.attrs = {k: v for k, v in p_vals.attrs.items() if v is not None}

            calc_val = xr.Dataset({"return_value": ret_vals, "p_values": p_vals})

            # Changing attributes for thresholds DataArray because of export issue
            num, size = event_duration
            calc_val.attrs["groupby"] = f"{num} {size}"

            # Add distribution to attributes
            calc_val = calc_val.assign_attrs(
                fitted_distr=distr,
                sample_size=len(ams),
            )

        elif percentile or percentile == 0:

            calc_val = (
                da.resample(time="1D")
                .reduce(metric_map[metric])
                .quantile(percentile / 100, dim="time")
                .assign_coords(
                    lat=da.lat,
                    lon=da.lon,
                    quantile=percentile,  # Coords are dropped during quantile method
                )
                .rename({"quantile": "percentile"})
            )

        if isinstance(calc_val, xr.DataArray):
            calc_val.name = name_of_calc

        return calc_val

    # If the data passed in is warming level data, then a dummy timestamp must be added
    # to act as the current WL 'time' dimension
    if approach == "Warming Level":
        da = add_dummy_time_to_wl(da)

    metric_data = _apply_metric(da)
    return metric_data


class CavaParams(param.Parameterized):
    """
    Climate Analysis and Vulnerability Assessment Parameters Class.

    This class defines and validates parameters for climate vulnerability analysis,
    supporting various climate variables, metrics, and analysis approaches.

    Attributes
    ----------
    input_locations : pd.DataFrame
        Input coordinates that must have 'lat' and 'lon' columns with numeric data.
    time_start_year : int, default=1981
        Start year for the analysis period. Must be between 1900 and 2100.
    time_end_year : int, default=2010
        End year for the analysis period. Must be between 1900 and 2100.
    units : str, default="Celsius"
        Units for temperature measurement.
    variable : str, default="Air Temperature at 2m"
        Climate variable to analyze. Options include "Air Temperature at 2m",
        "Precipitation (total)", "NOAA Heat Index", "Effective Temperature".
    metric_calc : str, default="max"
        Statistical metric calculation method. Options: "min", "max", "mean", "median".
    percentile : float or None, default=None
        Percentile value for calculation. Must be between 0 and 100 if specified.
    heat_idx_threshold : float or None, default=None
        Threshold value for heat index calculations.
    one_in_x : int, float, list or None, default=None
        Return period(s) for extreme event analysis (e.g., 1-in-100 year event).
        Can be a single value or list of values.
    season : str, default="all"
        Season to analyze. Options: "summer", "winter", "all".
    downscaling_method : str, default="Dynamical"
        Climate model downscaling method. Options: "Dynamical", "Statistical".
    approach : str, default="Time"
        Analysis approach. Options: "Time" (time period), "Warming Level" (temperature target).
    warming_level : float, default=1.0
        Global warming level in degrees Celsius. Must be between 0 and 7.
    wrf_bias_adjust : bool, default=True
        Whether to apply bias adjustment to WRF model data.
    historical_data : str, default="Historical Climate"
        Type of historical data. Options: "Historical Climate", "Historical Reconstruction".
    ssp_data : list, default=["SSP 3-7.0"]
        Shared Socioeconomic Pathway scenarios to use.
        Options: "SSP 2-4.5", "SSP 3-7.0", "SSP 5-8.5".
    export_method : str, default="both"
        Data export method. Options: "raw", "calculate", "both", "None".
    separate_files : bool, default=False
        Whether to save climate variables in separate files.
    file_format : str, default="NetCDF"
        Output file format.
    batch_mode : bool, default=False
        Whether to run in batch processing mode.
    distr : str, default="gev"
        Statistical distribution for extreme value analysis.
        Options: "gev" (Generalized Extreme Value), "genpareto" (Generalized Pareto), "gamma".
    event_duration : tuple, default=(1, "day")
        Duration and unit for event analysis. Format: (duration, unit).
        Unit options: "hour", "day".

    Methods
    -------
    validate_params()
        Validate all parameters for consistency and compatibility.
    get_names()
        Generate standardized names and metadata for data processing.
        If parameter validation fails. Common validation errors include:
        - Missing or non-numeric lat/lon columns in input_locations
        - Invalid time range (start year > end year)
        - Incompatible parameter combinations (e.g., multiple threshold parameters)
        - Unsupported variable-downscaling method combinations
        - Invalid approach-data type combinations

    Notes
    -----
    The class enforces several validation rules:
    - Only one threshold parameter (heat_idx_threshold, percentile, or one_in_x) can be specified
    - Historical Reconstruction data requires time-based approach and end year <= 2022
    - NOAA Heat Index and Effective Temperature cannot use Statistical downscaling
    - Dynamical downscaling with time-based approach only supports SSP 3-7.0
    - Event duration currently supports only "hour" or "day" units

    Examples
    --------
    >>> import pandas as pd
    >>> locations = pd.DataFrame({'lat': [34.05, 36.17], 'lon': [-118.25, -115.14]})
    >>> params = CavaParams(
    ...     input_locations=locations,
    ...     time_start_year=2020,
    ...     time_end_year=2050,
    ...     variable="Air Temperature at 2m",
    ...     percentile=95,
    ...     metric_calc="max"
    ... )
    """

    input_locations = param.DataFrame(
        doc="Input coordinates must have 'lat' and 'lon' columns."
    )
    time_start_year = param.Integer(bounds=(1900, 2100), doc="Start year", default=1981)
    time_end_year = param.Integer(bounds=(1900, 2100), doc="End year", default=2010)

    # Need to find all valid units by variable name
    units = param.String(default="Celsius", doc="Units for temperature measurement")

    variable = param.ObjectSelector(
        default="Air Temperature at 2m",
        objects=[
            "Air Temperature at 2m",
            "Precipitation (total)",
            "NOAA Heat Index",
            "Effective Temperature",
        ],
        doc="Variable to analyze",
    )
    metric_calc = param.ObjectSelector(
        default="max",
        objects=["min", "max", "mean", "median"],
        doc="Metric calculation method",
    )
    percentile = param.Number(
        default=None, bounds=(0, 100), allow_None=True, doc="Percentile for calculation"
    )
    heat_idx_threshold = param.Number(
        default=None, allow_None=True, doc="Threshold for heat index"
    )
    one_in_x = param.Parameter(
        default=None, allow_None=True, doc="1-in-x year event(s) — number or list"
    )
    season = param.ObjectSelector(
        default="all", objects=["summer", "winter", "all"], doc="Season to analyze"
    )
    downscaling_method = param.ObjectSelector(
        default="Dynamical",
        objects=["Dynamical", "Statistical"],
        doc="Downscaling method",
    )
    approach = param.ObjectSelector(
        default="Time", objects=["Time", "Warming Level"], doc="Approach to use"
    )
    warming_level = param.Number(
        default=1.0, bounds=(0, 7)
    )  # Bounds will be further enforced once `DataParameters` is replaced with `get_data`
    wrf_bias_adjust = param.Boolean(default=True, doc="Flag for WRF bias adjustment")
    historical_data = param.ObjectSelector(
        default="Historical Climate",
        objects=["Historical Climate", "Historical Reconstruction"],
        doc="Type of historical data",
    )
    ssp_data = param.ListSelector(
        default=["SSP 3-7.0"],
        objects=["SSP 2-4.5", "SSP 3-7.0", "SSP 5-8.5"],
        doc="SSP data scenarios to use",
    )
    export_method = param.ObjectSelector(
        default="both",
        objects=["raw", "calculate", "both", "None"],
        doc="Export method",
    )
    separate_files = param.Boolean(
        default=False,
        doc="Whether to separate climate variable information into separate files",
    )
    file_format = param.String(default="NetCDF", doc="Format of the output files")
    batch_mode = param.Boolean(default=False, doc="Flag for batch mode operation")
    distr = param.ObjectSelector(
        default="gev",
        objects=["gev", "genpareto", "gamma"],
        doc="Distribution function",
    )
    event_duration = param.Tuple(default=(1, "day"))

    def __init__(self, **params):
        super().__init__(**params)
        self.validate_params()

    def validate_params(self):
        """
        Validate the parameters for vulnerability analysis.

        Parameters
        ----------
        None

        Returns
        -------
        None

        Raises
        ------
        ValueError
            If any validation check fails. The error message lists all validation
            failures found during the validation process.

        Notes
        -----
        The method validates the following conditions:
        - Input locations DataFrame contains required 'lat' and 'lon' columns with
          numeric data types (float64 or int64)
        - Time range validity (start year must be <= end year)
        - Historical reconstruction data constraints:
            - End year must be <= 2022
            - Only time-based approach is supported
        - Mutual exclusivity of threshold parameters (only one of heat_idx_threshold,
          percentile, or one_in_x can be specified)
        - At least one threshold parameter must be specified
        - Metric calculation ('min' or 'max') compatibility with percentile calculations
        - Variable and downscaling method compatibility (NOAA Heat Index and Effective
          Temperature cannot use Statistical downscaling)
        - SSP data constraints for WRF/Dynamical downscaling (only SSP 3-7.0 allowed
          for time-based approach)

        Side Effects
        ------------
        Converts self.one_in_x to a list if it's not None and not already a list.
        """

        errors = []

        # Change `one_in_x` type before param validation and arg generation
        one_in_x = self.one_in_x
        if one_in_x is not None:
            if not isinstance(one_in_x, list):
                self.one_in_x = [one_in_x]
            # Keep as list, don't convert to numpy array here

        # Check if input_locations is a DataFrame with lat/lon columns
        if isinstance(self.input_locations, pd.DataFrame):
            if (
                "lat" not in self.input_locations.columns
                or "lon" not in self.input_locations.columns
            ):
                errors.append("Input coordinates must have `lat` and `lon` columns.")
            elif not pd.api.types.is_numeric_dtype(
                self.input_locations["lat"]
            ) or not pd.api.types.is_numeric_dtype(self.input_locations["lon"]):
                errors.append("Input lat/lon columns must be float64 or int64 types.")

        # Validating that time start year < time end year
        if self.time_start_year > self.time_end_year:
            errors.append(
                "Start year must come before, or be the same year as, the end year."
            )

        # Validate year range for historical reconstruction
        if (
            self.historical_data == "Historical Reconstruction"
            and self.time_end_year > 2022
        ):
            errors.append(
                "End year for Historical Reconstruction data must be 2022 or earlier."
            )

        # Validate time approach with Historical Reconstruction
        if (
            self.historical_data == "Historical Reconstruction"
            and self.approach != "Time"
        ):
            errors.append(
                "Historical Reconstruction data can only be retrieved using a time-based approach."
            )

        # Validating that only one of heat_idx_threshold, percentile, or one_in_x is passed
        if (
            (self.heat_idx_threshold is not None and self.percentile is not None)
            or (self.heat_idx_threshold is not None and self.one_in_x is not None)
            or (self.percentile is not None and self.one_in_x is not None)
        ):
            errors.append(
                "Only one of heat_idx_threshold, one_in_x, and percentile can be non-None."
            )

        # Check that not all 3 customizable metric arguments are none.
        if (
            self.heat_idx_threshold is None
            and self.percentile is None
            and self.one_in_x is None
        ):
            errors.append(
                "`heat_idx_threshold`, `percentile`, and `one_in_x` cannot all be None."
            )

        # Make sure the metric is 'min' or 'max' if percentile is passed in
        if self.percentile is not None and self.metric_calc not in ["min", "max"]:
            errors.append(
                "Metric calculation must be 'min' or 'max' for percentile calculations."
            )

        # Validating variable and downscaling method combo
        if (
            self.variable in ["NOAA Heat Index", "Effective Temperature"]
            and self.downscaling_method == "Statistical"
        ):
            errors.append(
                f"`{self.variable}` cannot be used with Statistical downscaling method."
            )

        # Only allow for SSP 3-7.0 for WRF data
        if self.downscaling_method == "Dynamical" and self.approach == "Time":
            if len(self.ssp_data) > 1 or self.ssp_data[0] != "SSP 3-7.0":
                errors.append(
                    "Can only use SSP 3-7.0 data for a time-based approach using WRF data."
                )

        # Only allow `hour` or `daily` frequencies for 1-in-X event counts
        _, dur_type = self.event_duration
        if dur_type != "hour" and dur_type != "day":
            raise ValueError(
                "Current specifications not implemented. `event_duration` options only implemented for `hour` or `daily` frequency."
            )

        # Raise errors if any
        if errors:
            raise ValueError("Parameter validation errors:\n\n" + "\n".join(errors))

    def get_names(self):
        """
        Generate names and metadata for climate data processing.

        Returns
        -------
        dict
            A dictionary containing the following keys:
            ssp_selected : str or list
                The SSP (Shared Socioeconomic Pathway) data selected for analysis.
            variable : str
                The climate variable being analyzed, potentially adjusted based on
                downscaling method and metric calculation.
            variable_type : str
                Type of variable, either "Variable" for raw climate variables or
                "Derived Index" for calculated indices.
            var_name : str
                Human-readable description of the calculation being performed,
                including variable, metric, time period/warming level, and season.
            raw_name : str
                Standardized name for raw data storage.
            calc_name : str
                Standardized name for calculated data storage.

        Notes
        -----
        The function handles three main calculation types based on instance attributes:
        - Percentile calculations (when self.percentile is not None)
        - Heat index threshold calculations (when self.heat_idx_threshold is not None)
        - Return period calculations (when self.one_in_x is not None)
        For LOCA2 statistical downscaling with air temperature, the variable name is
        adjusted based on whether maximum or minimum metric calculation is selected.
        The approach can be either "Time" (using start/end years) or "Warming Level"
        (using a specific warming level in degrees Celsius).

        Raises
        ------
        ValueError
            If metric_calc is not "max" or "min" for LOCA2 air temperature data.
        ValueError
            If approach is not "Time" or "Warming Level".
        ValueError
            If an unsupported variable is used for 1-in-X calculations.
        """

        # Re-assigning `variable` in case it needs to be re-instantiated later.
        variable = self.variable

        # Determine variable type
        if variable in ["Air Temperature at 2m", "Precipitation (total)"]:
            variable_type = "Variable"
        else:
            variable_type = "Derived Index"

        # Adjust variable name for LOCA2 data
        if (
            variable == "Air Temperature at 2m"
            and self.downscaling_method == "Statistical"
        ):
            match self.metric_calc:
                case "max":
                    variable = "Maximum air temperature at 2m"
                case "min":
                    variable = "Minimum air temperature at 2m"
                case _:
                    raise ValueError('metric_calc needs to be either "max" or "min"')

        # Reducing potential decimal places for warming level
        self.warming_level = round(self.warming_level, 3)

        # Adding time period or warming level to filename
        match self.approach:
            case "Time":
                approach_str = f"{self.time_start_year}_to_{self.time_end_year}"
                approach_var_name = f"from {approach_str}"
            case "Warming Level":
                if float(self.warming_level).is_integer():
                    wl_str = str(int(self.warming_level))
                else:
                    wl_str = str(self.warming_level).replace(".", "pt")

                approach_str = f"{wl_str}degreeWL"
                approach_var_name = f"for Warming Level {self.warming_level}°C"
            case _:
                raise ValueError(
                    'approach needs to be either "Time" or "Warming Level"'
                )
        # Retrieving the name of the calculation about to occur
        if self.percentile is not None:

            def ordinal(n):
                """Find ordinal name for a number (1 -> 1st, 2 -> 2nd, 3 -> 3rd)"""
                return f"{n}{('th' if 11 <= n % 100 <= 13 else {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th'))}"

            var_name = f"{ordinal(self.percentile)} percentile of Daily {self.metric_calc.capitalize()} of {variable} for {self.season} seasons {approach_var_name}"
            raw_name = "likely_seasonal_temperature_raw_data"
            calc_name = (
                f"likely_seasonal_{self.season}_{self.metric_calc}_{approach_str}"
            )

        elif self.heat_idx_threshold is not None:
            var_name = f"Days per year with {self.metric_calc.capitalize()} Daily {variable} above a heat index threshold of {self.heat_idx_threshold} {self.units} for {self.season} seasons {approach_var_name}"
            raw_name = "heat_index_raw_data"
            calc_name = f"heat_index_thresh{self.heat_idx_threshold}_{approach_str}"

        elif self.one_in_x is not None:

            x_names = [
                f"1-in-{x} year, {'-'.join(map(str, self.event_duration))} {self.metric_calc.capitalize()} {variable} for {self.season} seasons {approach_var_name}"
                for x in self.one_in_x
            ]
            var_name = " and ".join(x_names)

            # Making names for raw and calculated data
            x_vals_str = "-".join(map(str, self.one_in_x))

            dur_len, dur_type = self.event_duration
            shortname_event = f"{dur_len}_{dur_type if dur_type == 'day' else 'hr'}"
            match variable:
                case "Precipitation (total)":
                    raw_name = f"one_in_{x_vals_str}_precipitation_raw_data"
                    calc_name = f"one_in_{x_vals_str}_{shortname_event}_precipitation_{approach_str}"
                case (
                    "Air Temperature at 2m"
                    | "Maximum air temperature at 2m"
                    | "Minimum air temperature at 2m"
                ):
                    raw_name = f"one_in_{x_vals_str}_temperature_raw_data"
                    calc_name = f"one_in_{x_vals_str}_{shortname_event}_temperature_{approach_str}"
                case "NOAA Heat Index" | "Effective Temperature":
                    raw_name = f"one_in_{x_vals_str}_derived_index_raw_data"
                    calc_name = f"one_in_{x_vals_str}_{shortname_event}_derived_index_{approach_str}"
                case _:
                    raise ValueError(
                        f'variable "{variable}" is not supported for 1-in-X calculations'
                    )

        return {
            "ssp_selected": self.ssp_data,
            "variable": variable,
            "variable_type": variable_type,
            "var_name": var_name,
            "raw_name": raw_name,
            "calc_name": calc_name,
        }


def cava_data(
    input_locations,  # csv file
    variable,
    units=None,
    approach="Time",  # default
    downscaling_method="Dynamical",  # default
    time_start_year=1981,  # default, optional
    time_end_year=2010,  # default, optional
    historical_data="Historical Climate",  # default
    ssp_data=UNSET,  # default
    warming_level=1.5,  # default
    metric_calc="max",  # default
    heat_idx_threshold=None,
    one_in_x=None,
    event_duration=(1, "day"),
    percentile=None,
    season="all",  # default to all so no subsetting occurs unless called
    wrf_bias_adjust=True,
    export_method="both",  # raw, full calculate, both
    separate_files=True,  # toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format="NetCDF",
    batch_mode=False,
    distr="gev",
):
    """Retrieve, process, and export climate data based on inputs.

    Designed for CAVA reports.

    Parameters
    ----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    variable : str
        Type of climate variable to retrieve and calculate.
    units : str, optional
        Units for the retrieved data.
    approach : str
        Approach to follow, default is "Time".
    downscaling_method : str
        Method of downscaling, default is "Dynamical".
    time_start_year : int, optional
        Starting year for data selection.
    time_end_year : int, optional
        Ending year for data selection.
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP 3-7.0".
    warming_level : str, optional
        Global warming levels, default is 1.5°C.
    metric_calc : str, optional
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics. Default is "max"
    heat_idx_threshold : float
        Heat index threshold for counting events.
    one_in_x : int, optional
        Return period for 1-in-X events.
    event_duration: tuple[int, str], optional
        Duration of event and time unit (e.g. (1, "day"))
    percentile : int, optional
        Percentile for calculating "likely" event occurrence.
    season : str, optional
        Season to subset time dimension on (e.g., 'summer', 'winter', 'all'). Default is 'all'.
    wrf_bias_adjust : str, optional
        Flag to subset the WRF data for the bias-adjusted models. Default is True.
    export_method : str, optional
        Export method, options are 'raw', 'calculate', 'both', default is 'both'.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.
    file_format : str, optional
        Export file format options.
    batch_mode : bool
        Whether to process data with batch mode or through iterating through the points.
    distr : str
        Name of distribution to use

    Returns
    -------
    xarray.DataArray
        Computed climate metrics for input locations.

    Raises
    ------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    if ssp_data is UNSET:
        ssp_data = ["SSP 3-7.0"]

        # with param.exceptions_summarized():
        # Convert `one_in_x` param to list if needed
    if one_in_x is not None:
        if not isinstance(one_in_x, list):
            one_in_x = [one_in_x]

    params = CavaParams(**locals())
    clean_params = params.get_names()

    # Convert string input to list
    # ssp_data input requires a list even if the user is just inputting a single value
    # i.e. ssp_data = "SSP 3-7.0" is NOT valid; needs to be ssp_data = ["SSP 3-7.0]
    # Here, we catch that error for the user and just convert the string to a list :D
    if isinstance(ssp_data, str):
        ssp_data = [ssp_data]

    months_map = {
        "winter": [12, 1, 2],
        "summer": [6, 7, 8],
        "all": [int(x) for x in np.arange(1, 13)],
    }

    locations = input_locations[["lat", "lon"]].values

    print(f"Calculating {clean_params['var_name']}")

    print("--- Selecting Data Points --- \n")

    selections = DataParameters()
    selections.approach = approach
    selections.data_type = "Gridded"
    selections.downscaling_method = downscaling_method
    selections.timescale = "hourly" if downscaling_method == "Dynamical" else "daily"
    selections.variable_type = clean_params["variable_type"]
    selections.variable = clean_params["variable"]
    selections.resolution = "3 km"
    selections.units = units

    # Setting WL vs time-based attributes
    if approach == "Warming Level":
        selections.warming_level = [float(warming_level)]
        selections.warming_level_months = months_map[season]
        selections.scenario_ssp = ["n/a"]
        selections.scenario_historical = ["n/a"]

    else:
        selections.scenario_ssp = (
            clean_params["ssp_selected"]
            if historical_data != "Historical Reconstruction"
            else []
        )
        selections.scenario_historical = [historical_data]
        selections.time_slice = (time_start_year, time_end_year)

    if batch_mode and downscaling_method == "Statistical":
        print(
            """
            Batch mode for Statistical data in Warming Level approach is not optimized 
            for multiple locations (in development). This code could break unexpectedly.
            If it does, please report a bug on the climakitae GitHub repository.
            \n
            """
        )

    if batch_mode:
        separate_files = False
        data_pts = batch_select(approach, selections, locations)

        data = _filter_ba_models(
            data_pts, downscaling_method, wrf_bias_adjust, historical_data
        )

        if approach == "Warming Level" and warming_level == 0.8:
            # Filter out inappropriate models for historical GWL baseline
            data = _filter_hist_gwl_models(data, downscaling_method, str(warming_level))

    else:
        data_pts = []
        for _, loc in input_locations.iterrows():

            lat, lon = float(loc["lat"]), float(loc["lon"])
            print(f"Selecting data for {lat, lon}")

            selections.latitude = (lat - 0.02, lat + 0.02)
            selections.longitude = (lon - 0.02, lon + 0.02)
            data = selections.retrieve()

            if approach == "Time":
                # Remove leap days, if applicable
                data = data.sel(
                    time=~((data.time.dt.month == 2) & (data.time.dt.day == 29))
                )

            data = get_closest_gridcell(data, lat, lon, print_coords=False)

            data = _filter_ba_models(
                data, downscaling_method, wrf_bias_adjust, historical_data
            )

            if approach == "Warming Level" and warming_level == 0.8:
                # Filter out inappropriate models for historical GWL baseline
                data = _filter_hist_gwl_models(
                    data, downscaling_method, str(warming_level)
                )
            data_pts.append(data)

    if historical_data == "Historical Reconstruction":
        data = data.squeeze(dim="scenario")

    # Determining how to concatenate or load data, if necessary, depending on `approach` and `separate_files` parameters.
    print("\n--- Loading Data into Memory ---\n")

    if separate_files:
        loaded_data = []
        for point in data_pts:
            print(f"Point ({point.lat.compute().item()}, {point.lon.compute().item()})")
            data = load(point, progress_bar=True)
            loaded_data.append(data)

    else:
        data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
        loaded_data = load(data_pts, progress_bar=True)

    # Export raw data if desired
    if export_method == "raw" or export_method == "both":

        print("\n--- Exporting Raw Data ---")

        if downscaling_method == "Statistical":
            print(
                "\nExporting Statistical data in most granular time availability (daily)\n"
            )

        if separate_files:
            for pt_idx, data_point in enumerate(loaded_data):
                lat_str = str(round(data_point.lat.item(), 3)).replace(".", "")
                lon_str = str(round(data_point.lon.item(), 3)).replace(".", "")
                filename_str = f"{clean_params['raw_name']}_" f"{lat_str}N_{lon_str}W"
                _export_no_e(
                    data_point,
                    filename=filename_str,
                    file_format=file_format,
                )
        else:
            _export_no_e(
                loaded_data, filename="combined_raw_data", file_format=file_format
            )

        if export_method == "raw":
            return loaded_data

    print("\n--- Calculating Metrics ---")
    print("Calculating...\n")  # , end="", flush=True)

    if separate_files:
        metric_data = []
        # Calculate and export into separate files
        for point in loaded_data:
            calc_val = _metric_agg(
                point,
                variable,
                months_map[season],
                approach,
                metric_calc,
                clean_params["var_name"],
                heat_idx_threshold,
                percentile,
                one_in_x,
                event_duration,
                distr,
            )
            metric_data.append(calc_val)

    # Export a combined file of all metrics
    else:
        metric_data = _metric_agg(
            loaded_data,
            variable,
            months_map[season],
            approach,
            metric_calc,
            clean_params["var_name"],
            heat_idx_threshold,
            percentile,
            one_in_x,
            event_duration,
            distr,
        )

    print("\nComplete!")

    # Export the data
    print("\n--- Exporting Metric Data ---")

    match export_method:
        case "calculate" | "both":
            if separate_files:
                for _, data_point in enumerate(metric_data):
                    lat_str = str(round(data_point.lat.item(), 3)).replace(".", "")
                    lon_str = str(round(data_point.lon.item(), 3)).replace(".", "")
                    _export_no_e(
                        data_point,
                        filename=f"{clean_params['calc_name']}_{lat_str}N_{lon_str}W",
                        file_format=file_format,
                    )  # Will need to include naming convention for calculated file too.
            else:
                _export_no_e(
                    metric_data, filename="metric_data", file_format=file_format
                )
            # Returning values to user
            if len(metric_data) == 1:
                metric_data = metric_data[0]
            if len(loaded_data) == 1:
                loaded_data = loaded_data[0]

            match export_method:
                case "calculate":
                    return {"calc_data": metric_data}
                case "both":
                    return {"calc_data": metric_data, "raw_data": loaded_data}

        case "None":  # Specific for table generation
            print("Data export selection set to 'None'; no data is exported!")
            return {"calc_data": metric_data}
