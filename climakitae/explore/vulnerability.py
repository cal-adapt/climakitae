"""Tools for CAVA vulnerability assessment pilot"""
import warnings
import xarray as xr
import numpy as np
import pandas as pd

from climakitae.util.utils import get_closest_gridcell
from climakitae.core.data_load import load
from climakitae.core.data_interface import DataParameters
from climakitae.core.data_export import export
from climakitae.explore import warming_levels
from climakitae.explore.threshold_tools import get_block_maxima

# Ignore specific warnings about division errors to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")


def export_no_e(da, filename, format):
    """Exports a file but doesn't throw an exception if a file already exists."""
    try:
        export(
            da,
            filename=filename,
            format=format,
        )
    except Exception as e:
        print(e)


def _clean_wl_data(data, downscaling_method):
    """Cleans WL data to conform with time-based data for rest of function."""
    # Renaming simulation dimension (Calvin- will only work when models are unique across SSPs, Q: will this work with LOCA data too?)
    data["all_sims"] = ["_".join(x.split("_")[:3]) for x in data.all_sims.to_numpy()]
    data = data.rename({"all_sims": "simulation"})

    # Elevate dimensions, if needed
    if "x" not in data.dims and downscaling_method == "Dynamical":
        data = data.expand_dims(dim="x")
    if "y" not in data.dims and downscaling_method == "Dynamical":
        data = data.expand_dims(dim="y")

    return data


def metric_agg(da, approach, metric, heat_idx_threshold, one_in_x, percentile):
    """
    Calculate a metric on the given DataArray (da) using the specified metric type
    and heat index threshold.

    Parameters:
    -----------
    da : xarray.DataArray
        Input DataArray to compute metric and treshold on.
    metric : str
        The metric type to calculate. Supported options are "min", "max",
        "mean", and "median".
    heat_idx_threshold : float
        The heat index threshold for comparison.

    Returns:
    --------
    metric_data : xarray.DataArray
        Resulting DataArray from the calculated metric and heat index threshold applied.
    """
    metric_map = {
        "min": np.min,
        "max": np.max,
    }

    def apply_metric(da):
        """Applies the hard-coded heat index threshold or percentile to the DataArray"""
        if (
            heat_idx_threshold or heat_idx_threshold == 0
        ):  # If a threshold is passed and/or if the `heat_idx_threshold` passed in is 0, since that is a valid threshold.
            calc_val = (
                (da.resample(time="1D").reduce(metric_map[metric]) > heat_idx_threshold)
                .resample(time="1Y")
                .sum()
                .mean(dim="time")
            )  # This line counts the average number of high heat days per year over this time interval
            calc_val.name = f"Days per year with {metric.capitalize()} Daily {da.name} above a heat index threshold of {heat_idx_threshold} {da.units}"
            return calc_val

        elif one_in_x:  # Calculate 1-in-X events

            # Get 1-in-x event values
            event_vals = []
            for (
                sim
            ) in (
                da.simulation.values
            ):  # Can use xr.apply_ufunc, not just a for loop here
                event_val = get_block_maxima(  # Victoria: is this function right and/or am I using it right? I think these counts are a bit low.
                    da.sel(simulation=sim),
                    duration=(24, "hour"),
                    extremes_type=metric,
                    block_size=one_in_x,
                ).reduce(
                    np.mean
                )
                event_vals.append(event_val)

            sim_counts = xr.concat(event_vals, dim="simulation")

            # Calvin- Add renaming of DataArray title

            return sim_counts

        elif percentile or percentile == 0:

            # Calvin- Add renaming of DataArray title

            return (
                da.resample(time="1D")
                .reduce(metric_map[metric])
                .quantile(percentile / 100, dim="time")
                .assign_coords(
                    lat=da.lat, lon=da.lon
                )  # Coords are dropped during quantile method application
            )

    if approach == "time":
        metric_data = apply_metric(da)

    # If the data passed in is warming level data, the data must be manipulated to re-create a dummy time index
    elif approach == "warming_level":

        ### Creating dummy timestamps and renaming time dimension to replace current WL "time" dimension

        # Finding the name of the dimension that is not `simulation`, that should be named `hours/days/months_from_center`
        time_dim_name = [dim for dim in da.dims if dim != "simulation"][0]
        time_freq_name = time_dim_name.split("_")[0]

        # Creating dummy timestamps and assigning to the DataArray
        name_to_freq = {"hours": "H", "days": "D", "months": "M"}
        timestamps = pd.date_range(
            "2000-01-01",
            periods=len(da[time_dim_name]),
            freq=name_to_freq[time_freq_name],
        )
        da = da.assign_coords(hours_from_center=timestamps).rename(
            hours_from_center="time"
        )

        # Computing metric
        metric_data = apply_metric(da)

        # CALVIN- This is only useful if we want to keep the time dimension after applying the metric

        # Reverting time dimension back to WL framework and rename dimension
        # metric_data = metric_data.assign_coords(
        #     time=np.concatenate(
        #         [
        #             np.arange(-len(metric_data.time) / 2, 0),
        #             np.arange(1, len(metric_data.time) / 2 + 1),
        #         ]
        #     )
        # ).rename(time="years_from_center")

    return metric_data


def check_and_set_params(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    variable,  ## mandatory, must eventually accept temp, precip, or heat index
    metric_calc,
    percentile,
    heat_idx_threshold,
    one_in_x,
    season,
    downscaling_method,  # default for now ## mandatory
    approach,  # GWL to follow on ## mandatory
    warming_level,
    wrf_bc,
    historical_data,  # or "historical reconstruction"
    ssp_data,
    export_method,  # off-ramp, full calculate, both
    separate_files,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format,
    preloaded_data,
):
    """
    Validates parameters for the `cava_data` function, and returns transformed parameters, if needed.
    Transformed parameters returned: `ssp_selected`, `variable_type`.
    """
    # Validating lat/lon
    if "lat" not in input_locations.columns or "lon" not in input_locations.columns:
        raise ValueError("Input coordinates must have `lat` and `lon` columns.")
    elif (
        (input_locations.dtypes.lat != "float64")
        and (input_locations.dtypes.lat != "int64")
    ) or (
        (input_locations.dtypes.lon != "float64")
        and (input_locations.dtypes.lon != "int64")
    ):
        raise ValueError("Input lat/lon columns must be float64 or int64 types.")

    # Validating metric
    if metric_calc not in ["min", "max"]:
        raise ValueError("Passed in metric must be min, max")

    # Validating export method
    if export_method not in ["off-ramp", "calculate", "both", "None"]:
        raise ValueError(
            "Passed in export method must be 'off-ramp', 'calculate', 'both', or 'None'"
        )

    # Validating historical data type
    if historical_data not in ["Historical Climate", "Historical Reconstruction"]:
        raise ValueError(
            "Passed in historical data method must be 'Historical Climate' or 'Historical Reconstruction'"
        )

    # Validating warming level inputs
    if approach == "warming_level" and warming_level not in [
        "1.5",
        "2.0",
        "3.0",
        "4.0",
    ]:
        raise ValueError("Passed in warming level must be '1.5', '2.0', '3.0', '4.0'")

    # Validating that only heat_idx_threshold or percentile is being passed in
    if (
        (heat_idx_threshold != None and percentile != None)
        or (heat_idx_threshold != None and one_in_x != None)
        or (percentile != None and one_in_x != None)
    ):
        raise ValueError(
            "Only one of heat_idx_threshold, one_in_x, and percentile arguments can both be passed in. The others must be `None`."
        )

    # Make sure the metric is 'min' or 'max' if `percentile` is passed in
    if percentile or percentile == 0:
        if metric_calc != "min" and metric_calc != "max":
            raise ValueError(
                "Metric aggregation must either be min or max for a percentile calculation (i.e. likely summer day high or low)."
            )

    if season != "summer" and season != "winter" and season != "all":
        raise ValueError("Season must either be `summer`, `winter`, or `all`.")

    # Validating downscaling method
    downscaling_methods = ["Dynamical", "Statistical"]
    if downscaling_method not in downscaling_methods:
        raise ValueError(
            f"Passed in downscaling method must be in {downscaling_methods}"
        )

    # Validating approach
    if approach not in ["time", "warming_level"]:
        raise ValueError(f"Passed in approach must either be 'time' or 'warming_level'")

    # Temp throw error with WL showing future support
    if approach == "warming_level":
        raise ValueError(
            "Warming level integration still under construction! Only time-based approach available for now."
        )

    # Validating variable
    if variable not in [
        "Air Temperature at 2m",
        "Precipitation (total)",
        "NOAA Heat Index",
    ]:
        raise ValueError(
            "Passed in variable must be `Air Temperature at 2m`, `Precipitation (total)`, or `NOAA Heat Index`"
        )

    # Validating SSP
    ssp_mapping = {
        "SSP2-4.5": "SSP 2-4.5 -- Middle of the Road",
        "SSP3-7.0": "SSP 3-7.0 -- Business as Usual",
        "SSP5-8.5": "SSP 5-8.5 -- Burn it All",
    }

    # Add validation for SSPs

    # Map SSP names from user inputs to selections SSPs
    ssp_selected = [ssp_mapping[ssp] for ssp in ssp_data]

    # Determining if passed in variable is a climate variable or a derived index
    if variable == "Air Temperature at 2m" or variable == "Precipitation (total)":
        variable_type = "Variable"
    elif variable == "NOAA Heat Index":
        variable_type = "Derived Index"

    return {"ssp_selected": ssp_selected, "variable_type": variable_type}


def cava_data(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    variable,  ## mandatory, must eventually accept temp, precip, or heat index
    metric_calc,
    percentile,
    heat_idx_threshold,
    one_in_x=1,
    season="summer",
    downscaling_method="Dynamical",  # default for now ## mandatory
    approach="time",  # GWL to follow on ## mandatory, Q: Why is anom required for non BC WRF data?
    warming_level="1.5",
    wrf_bc=True,
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=["SSP 3-7.0 -- Business as Usual"],
    export_method="both",  # off-ramp, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format="NetCDF",
    preloaded_data=None,
):
    """Retrieve, process, and export climate data based on inputs.

    Designed for CAVA reports.

    Parameters
    ----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    time_start_year : int
        Starting year for data selection.
    time_end_year : int
        Ending year for data selection.
    units : str
        Units for the retrieved data.
    variable : str
        Type of climate variable to retrieve and calculate.
    metric_calc : str
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics.
    heat_idx_threshold : float
        Heat index threshold for counting events.
    downscaling_method : str, optional
        Method of downscaling, default is "Dynamical".
    approach : str, optional
        Approach to follow, default is "time".
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    export_method : str, optional
        Export method, options are 'off-ramp', 'calculate', 'both', default is 'both'.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.

    Returns
    -------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises
    ------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    clean_params = check_and_set_params(
        **locals()
    )  # This may not be best practice. Also to note, this line MUST be the first line (or at least before other local vars are defined), or else they will also be passed to `check_and_set_params`.

    time_new_line = "\n" if approach == "time" else ""
    warm_new_line = "\n" if approach == "warming_level" else ""
    months_map = {"winter": [12, 1, 2], "summer": [6, 7, 8], "all": np.arange(1, 13)}

    print(f"--- Selecting Data Points --- {time_new_line}")

    data_pts = []
    for index, loc in input_locations.iterrows():
        lat, lon = loc["lat"], loc["lon"]

        print(f"{warm_new_line}Selecting data for {lat, lon}")

        # Using preloaded data if it is passed in
        if preloaded_data and season in preloaded_data:
            data = preloaded_data[season]

            if approach == "warming_level":
                data = _clean_wl_data(data, downscaling_method)

        else:

            if approach == "time":  # not preloaded or season not there

                selections = DataParameters()
                selections.data_type = "Gridded"
                selections.downscaling_method = downscaling_method
                selections.scenario_historical = [historical_data]
                selections.scenario_ssp = clean_params["ssp_selected"]
                selections.timescale = "hourly"
                selections.variable_type = clean_params["variable_type"]
                selections.variable = variable
                selections.latitude = (
                    lat - 0.02,
                    lat + 0.02,
                )
                selections.longitude = (lon - 0.02, lon + 0.02)
                selections.time_slice = (time_start_year, time_end_year)
                selections.resolution = "3 km"
                selections.units = units

                data = selections.retrieve()

            elif approach == "warming_level":  # not preloaded or season not there

                wl = warming_levels()
                wl.wl_params.timescale = "hourly"
                wl.wl_params.downscaling_method = downscaling_method
                wl.wl_params.variable_type = clean_params["variable_type"]
                wl.wl_params.variable = variable
                wl.wl_params.latitude = (lat - 0.02, lat + 0.02)
                wl.wl_params.longitude = (lon - 0.02, lon + 0.02)
                wl.wl_params.warming_levels = [
                    warming_level
                ]  # Calvin- default, only allow for 1 warming level to be passed in.
                wl.wl_params.units = units
                wl.wl_params.resolution = "3 km"
                wl.wl_params.anom = "No"
                wl.wl_params.months = months_map[season]

                wl.calculate()

                # Clean WL data before using
                data = _clean_wl_data(wl.sliced_data[warming_level], downscaling_method)

        # Toggle for only BC or not BC WRF models
        if downscaling_method == "Dynamical":
            if wrf_bc:
                data = data.sel(
                    simulation=list(
                        set(
                            [
                                "WRF_EC-Earth3_r1i1p1f1",
                                "WRF_MPI-ESM1-2-HR_r3i1p1f1",
                                "WRF_TaiESM1_r1i1p1f1",
                                "WRF_MIROC6_r1i1p1f1",
                            ]
                        ).intersection(
                            data.simulation.values
                        )  # This intersection check is here because some WRF models don't reach higher levels of warming, so this line only selects models that already exist in the data.
                    )
                )

        # Find the closest gridcell from the data to the lat/lon point
        closest_cell = get_closest_gridcell(
            data, lat, lon, print_coords=False
        ).squeeze()

        # Storing data differently if all data points should be in one file vs each data point in its own file
        if not separate_files:

            # Renaming gridcell so that it can be concatenated with other lat/lon gridcells
            closest_cell["simulation"] = [
                "{}_{}_{}".format(
                    sim_name, closest_cell.lat.item(), closest_cell.lon.item()
                )
                for sim_name in closest_cell.simulation
            ]

        data_pts.append(closest_cell)

    # Determining how to concatenate or load data, if necessary, depending on `approach` and `separate_files` parameters.
    if approach == "time":

        print("\n--- Loading Data into Memory ---\n")

        if separate_files:
            loaded_data = []
            for point in data_pts:
                print(f"Point ({point.lat.item()}, {point.lon.item()})")
                data = load(point, progress_bar=True)
                print("\n")

                # Filter for specific months
                data = data.sel(time=data.time.dt.month.isin(months_map[season]))
                loaded_data.append(data)

        else:
            data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
            loaded_data = load(data_pts, progress_bar=True)

            # Filter for specific months
            loaded_data = loaded_data.sel(
                time=loaded_data.time.dt.month.isin(months_map[season])
            )

    elif approach == "warming_level":

        # Don't load or print anything since warming levels are already loaded
        loaded_data = data_pts

    # Export raw data if desired
    if export_method == "off-ramp" or export_method == "both":

        print("\n--- Offramp Exporting Raw Data ---")

        if separate_files:
            for pt_idx in range(len(loaded_data)):
                export_no_e(
                    loaded_data[pt_idx],
                    filename=f"{variable.replace(' ', '_')}_{pt_idx}",
                    format=file_format,
                )
        else:
            export_no_e(loaded_data, filename="combined_raw_data", format=file_format)

        if export_method == "off-ramp":
            return loaded_data

    print("\n--- Calculating Metrics ---")

    if separate_files:

        metric_data = []
        # Calculate and export into separate files
        for point in loaded_data:
            calc_val = metric_agg(
                point, approach, metric_calc, heat_idx_threshold, one_in_x, percentile
            )
            metric_data.append(calc_val)

    # Export a combined file of all metrics
    else:
        metric_data = metric_agg(
            loaded_data, approach, metric_calc, heat_idx_threshold, one_in_x, percentile
        )

    # Export the data
    print("\n--- Exporting Metric Data ---")

    if export_method == "calculate" or export_method == "both":

        if separate_files:
            for pt_idx in range(len(metric_data)):
                export_no_e(
                    metric_data[pt_idx],
                    filename=f"{metric_calc.capitalize()}_{variable.replace(' ', '_')}_{pt_idx}",
                    format=file_format,
                )  # Will need to include naming convention for calculated file too.

        else:
            export_no_e(metric_data[pt_idx], filename="metric_data", format=file_format)

        # Calvin- Look into how to return data (dictionaries of indices to points)
        if export_method == "calculate":
            return metric_data
        elif export_method == "both":
            return loaded_data, metric_data

    elif export_method == "None":
        return metric_data
