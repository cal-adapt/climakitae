"""Tools for CAVA vulnerability assessment pilot"""

from climakitae.util.utils import get_closest_gridcell
from climakitae.core.data_load import load
from climakitae.core.data_interface import Select
from climakitae.core.data_export import export
import xarray as xr
import numpy as np
import pandas as pd
import warnings
from climakitae.explore import warming_levels
from climakitae.explore.threshold_tools import get_block_maxima

# Ignore specific warnings about division errors to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")


def metric_agg(da, approach, metric, threshold, one_in_x, percentile):
    """
    Calculate a metric on the given DataArray (da) using the specified metric type
    and threshold.

    Parameters:
    -----------
    da : xarray.DataArray
        Input DataArray to compute metric and treshold on.
    metric : str
        The metric type to calculate. Supported options are "min", "max",
        "mean", and "median".
    threshold : float
        The threshold value for comparison.

    Returns:
    --------
    metric_data : xarray.DataArray
        Resulting DataArray from the calculated metric and threshold applied.
    """
    metric_map = {
        "min": np.min,
        "max": np.max,
        "mean": np.mean,
        "median": np.median,
    }

    def apply_metric(da):
        """Applies the hard-coded threshold or percentile to the DataArray"""
        if (
            threshold or threshold == 0
        ):  # If a threshold is passed and/or if the threshold passed in is 0, since that is a value threshold.
            return (
                (da.resample(time="1D").reduce(metric_map[metric]) > threshold)
                .resample(time="1Y")
                .sum()
                .mean(
                    dim="time"
                )  # This line counts the average number of high heat days per year over this time interval
            )

        elif one_in_x:  # Calculate 1-in-X events

            # Get 1-in-x event values
            event_vals = []
            for (
                sim
            ) in (
                da.simulation.values
            ):  # Can use xr.apply_ufunc, not just a for loop here
                event_val = get_block_maxima(
                    da.sel(simulation=sim),
                    duration=(24, "hour"),
                    extremes_type=metric,
                    block_size=one_in_x,
                ).reduce(
                    np.mean
                )  # Q: How do we determine 1-x event after `get_block_maxima`?
                event_vals.append(event_val)

            sim_counts = xr.concat(event_vals, dim="simulation")
            return sim_counts

        elif percentile or percentile == 0:
            return (
                da.resample(time="1D")
                .reduce(metric_map[metric])
                .quantile(percentile / 100, dim="time")
                .assign_coords(
                    lat=da.lat, lon=da.lon
                )  # Coords are dropped during quantile method application
            )

    if approach == "time":
        metric_data = apply_metric(da)

    # If the data passed in is warming level data, the data must be manipulated to re-create a dummy time index
    elif approach == "warming_level":

        ### Creating dummy timestamps and renaming time dimension to replace current WL "time" dimension

        # Finding the name of the dimension that is not `simulation`, that should be named `hours/days/months_from_center`
        time_dim_name = [dim for dim in da.dims if dim != "simulation"][0]
        time_freq_name = time_dim_name.split("_")[0]

        # Creating dummy timestamps and assigning to the DataArray
        name_to_freq = {"hours": "H", "days": "D", "months": "M"}
        timestamps = pd.date_range(
            "2000-01-01",
            periods=len(da[time_dim_name]),
            freq=name_to_freq[time_freq_name],
        )
        da = da.assign_coords(hours_from_center=timestamps).rename(
            hours_from_center="time"
        )

        # Computing metric
        metric_data = apply_metric(da)

        # Reverting time dimension back to WL framework and rename dimension
        # metric_data = metric_data.assign_coords(
        #     time=np.concatenate(
        #         [
        #             np.arange(-len(metric_data.time) / 2, 0),
        #             np.arange(1, len(metric_data.time) / 2 + 1),
        #         ]
        #     )
        # ).rename(time="years_from_center")

    return metric_data


def check_and_set_params(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    variable,  ## mandatory, must eventually accept temp, precip, or heat index
    metric_calc,
    threshold,
    one_in_x,
    percentile,
    season,
    downscaling_method,  # default for now ## mandatory
    approach,  # GWL to follow on ## mandatory
    warming_level,
    wrf_bc,
    historical_data,  # or "historical reconstruction"
    ssp_data,
    export_method,  # off-ramp, full calculate, both
    separate_files,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format,
):
    """
    Validates parameters for the `cava_data` function, and returns transformed parameters, if needed.
    Transformed parameters returned: `ssp_selected`, `variable_type`.
    """
    # Validating lat/lon
    if "lat" not in input_locations.columns or "lon" not in input_locations.columns:
        raise ValueError("Input coordinates must have `lat` and `lon` columns.")
    elif (
        (input_locations.dtypes.lat != "float64")
        and (input_locations.dtypes.lat != "int64")
    ) or (
        (input_locations.dtypes.lon != "float64")
        and (input_locations.dtypes.lon != "int64")
    ):
        raise ValueError("Input lat/lon columns must be float64 or int64 types.")

    # Validating metric
    if metric_calc not in ["min", "max", "mean", "median"]:
        raise ValueError("Passed in metric must be min, max, median, or mean.")

    # Validating export method
    if export_method not in ["off-ramp", "calculate", "both", "None"]:
        raise ValueError(
            "Passed in export method must be 'off-ramp', 'calculate', 'both', or 'None'"
        )

    # Validating historical data type
    if historical_data not in ["Historical Climate", "Historical Reconstruction"]:
        raise ValueError(
            "Passed in historical data method must be 'Historical Climate' or 'Historical Reconstruction'"
        )

    # Validating warming level inputs
    if warming_level not in ["1.5", "2.0", "3.0", "4.0"]:
        raise ValueError("Passed in warming level must be '1.5', '2.0', '3.0', '4.0'")

    # Validating that only threshold or percentile is being passed in
    if (
        (threshold != None and percentile != None)
        or (threshold != None and one_in_x != None)
        or (percentile != None and one_in_x != None)
    ):
        raise ValueError(
            "Only one of threshold, one_in_x, and percentile arguments can both be passed in. The others must be `None`."
        )

    # Make sure the metric is 'min' or 'max' if `percentile` is passed in
    if percentile or percentile == 0:
        if metric_calc != "min" and metric_calc != "max":
            raise ValueError(
                "Metric aggregation must either be min or max for a percentile calculation (i.e. likely summer day high or low)."
            )

    if season != "summer" and season != "winter" and season != "all":
        raise ValueError("Season must either be `summer`, `winter`, or `all`.")

    # Validating downscaling method
    downscaling_methods = ["Dynamical", "Statistical", "Dynamical+Statistical"]
    if downscaling_method not in downscaling_methods:
        raise ValueError(
            f"Passed in downscaling method must be in {downscaling_methods}"
        )

    # Validating approach
    if approach not in ["time", "warming_level"]:
        raise ValueError(f"Passed in approach must either be 'time' or 'warming_level'")

    # Validating variable
    if variable not in [
        "Air Temperature at 2m",
        "Precipitation (total)",
        "NOAA Heat Index",
    ]:
        raise ValueError(
            "Passed in variable must be `Air Temperature at 2m`, `Precipitation (total)`, or `NOAA Heat Index`"
        )

    # Validating SSP
    ssp_mapping = {
        "SSP2-4.5": "SSP 2-4.5 -- Middle of the Road",
        "SSP3-7.0": "SSP 3-7.0 -- Business as Usual",
        "SSP5-8.5": "SSP 5-8.5 -- Burn it All",
    }
    # if ssp_data not in ssp_mapping:
    #     raise ValueError("Passed in SSP must be `SSP2-4.5`, `SSP3-7.0`, or `SSP5-8.5`.")

    # Calvin- Add unit validation for variable (can't pull degF for Precip)

    # Map SSP names from user inputs to selections SSPs
    ssp_selected = [ssp_mapping[ssp] for ssp in ssp_data]

    # Determining if passed in variable is a climate variable or a derived index
    if variable == "Air Temperature at 2m" or variable == "Precipitation (total)":
        variable_type = "Variable"
    elif variable == "NOAA Heat Index":
        variable_type = "Derived Index"

    return {"ssp_selected": ssp_selected, "variable_type": variable_type}


def cava_data(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    variable,  ## mandatory, must eventually accept temp, precip, or heat index
    metric_calc,
    threshold,
    percentile,
    one_in_x=1,
    season="summer",
    downscaling_method="Dynamical",  # default for now ## mandatory
    approach="time",  # GWL to follow on ## mandatory, Q: Why is anom required for non BC WRF data?
    warming_level="1.5",
    wrf_bc=True,
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=["SSP 3-7.0 -- Business as Usual"],
    export_method="both",  # off-ramp, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format="NetCDF",
):
    """Retrieve, process, and export climate data based on inputs.

    Designed for CAVA reports.

    Parameters
    ----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    time_start_year : int
        Starting year for data selection.
    time_end_year : int
        Ending year for data selection.
    units : str
        Units for the retrieved data.
    variable : str
        Type of climate variable to retrieve and calculate.
    metric_calc : str
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics.
    threshold : float
        Threshold value for counting events.
    downscaling_method : str, optional
        Method of downscaling, default is "Dynamical".
    approach : str, optional
        Approach to follow, default is "time".
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    export_method : str, optional
        Export method, options are 'off-ramp', 'calculate', 'both', default is 'both'.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.

    Returns
    -------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises
    ------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    clean_params = check_and_set_params(
        **locals()
    )  # This may not be best practice. Also to note, this line MUST be the first line (or at least before other local vars are defined), or else they will also be passed to `check_and_set_params`.

    time_new_line = "\n" if approach == "time" else ""
    warm_new_line = "\n" if approach == "warming_level" else ""
    months_map = {"winter": [12, 1, 2], "summer": [6, 7, 8], "all": np.arange(1, 13)}

    print(f"--- Selecting Data Points --- {time_new_line}")

    data_pts = []
    for index, loc in input_locations.iterrows():
        lat, lon = loc["lat"], loc["lon"]

        print(f"{warm_new_line}Selecting data for {lat, lon}")

        if approach == "time":

            selections = Select()
            selections.data_type = "Gridded"
            selections.downscaling_method = downscaling_method
            selections.scenario_historical = [historical_data]
            selections.scenario_ssp = clean_params["ssp_selected"]
            selections.timescale = "hourly"
            selections.variable_type = clean_params["variable_type"]
            selections.variable = variable
            selections.latitude = (
                lat - 0.02,
                lat + 0.02,
            )
            selections.longitude = (lon - 0.02, lon + 0.02)
            selections.time_slice = (time_start_year, time_end_year)
            selections.resolution = "3 km"
            selections.units = units

            data = selections.retrieve()

        elif approach == "warming_level":

            wl = warming_levels()
            wl.wl_params.timescale = "hourly"
            wl.wl_params.downscaling_method = downscaling_method
            wl.wl_params.variable_type = "Derived Index"
            wl.wl_params.variable = "NOAA Heat Index"
            wl.wl_params.latitude = (lat - 0.02, lat + 0.02)
            wl.wl_params.longitude = (lon - 0.02, lon + 0.02)
            wl.wl_params.warming_levels = [
                warming_level
            ]  # Calvin- default, only allow for 1 warming level to be passed in.
            wl.wl_params.units = units
            wl.wl_params.resolution = "3 km"
            wl.wl_params.anom = "No"  # Q: When do we want this anomaly to be 'Yes'?
            wl.wl_params.months = months_map[season]

            wl.calculate()

            data = wl.sliced_data[warming_level]

            # Renaming simulation dimension (Calvin- will only work when models are unique across SSPs, Q: will this work with LOCA data too?)
            data["all_sims"] = [
                "_".join(x.split("_")[:3])
                for x in wl.sliced_data[warming_level].all_sims.to_numpy()
            ]
            data = data.rename({"all_sims": "simulation"})

            # Elevate dimensions, if needed
            if "x" not in data.dims and downscaling_method == "Dynamical":
                data = data.expand_dims(dim="x")
            if "y" not in data.dims and downscaling_method == "Dynamical":
                data = data.expand_dims(dim="y")

        # Toggle for only BC or not BC WRF models
        if downscaling_method == "Dynamical":
            if wrf_bc:
                data = data.sel(
                    simulation=list(
                        set(
                            [
                                "WRF_EC-Earth3_r1i1p1f1",
                                "WRF_MPI-ESM1-2-HR_r3i1p1f1",
                                "WRF_TaiESM1_r1i1p1f1",
                                "WRF_MIROC6_r1i1p1f1",
                            ]
                        ).intersection(data.simulation.values)
                    )  # These intersection checks are because some WRF models don't reach higher levels of warming, so this only selects models that already exist in the data.
                )
            else:
                data = data.sel(
                    simulation=list(
                        set(
                            [
                                "WRF_CESM2_r11i1p1f1",
                                "WRF_CNRM-ESM2-1_r1i1p1f2",
                                "WRF_FGOALS-g3_r1i1p1f1",
                                "WRF_EC-Earth3-Veg_r1i1p1f1",
                            ]
                        ).intersection(data.simulation.values)
                    )
                )
        # Find the closest gridcell from the data to the lat/lon point
        closest_cell = get_closest_gridcell(
            data, lat, lon, print_coords=False
        ).squeeze()

        # Storing data differently if all data points should be in one file vs each data point in its own file
        if not separate_files:
            # Renaming gridcell so that it can be concatenated with other lat/lon gridcells
            closest_cell["simulation"] = [
                "{}_{}_{}".format(
                    sim_name, closest_cell.lat.item(), closest_cell.lon.item()
                )
                for sim_name in closest_cell.simulation
            ]

        data_pts.append(closest_cell)

    # Determining how to concatenate or load data, if necessary, depending on `approach` and `separate_files` parameters.
    if approach == "time":

        print("\n--- Loading Data into Memory ---\n")

        # if separate_files:
        #     loaded_data = [
        #         (
        #             lambda point: (
        #                 print(f"Point ({point.lat.item()}, {point.lon.item()})"),
        #                 load(point, progress_bar=True),
        #                 print("\n"),
        #             )
        #         )(data_pt)[
        #             1
        #         ]  # Prints the data, loads the data, and only stores the loaded data into `loaded_data`
        #         for data_pt in data_pts
        #     ]

        if separate_files:
            loaded_data = []
            for point in data_pts:
                print(f"Point ({point.lat.item()}, {point.lon.item()})")
                data = load(point, progress_bar=True)
                print("\n")

                # Filter for specific months
                data = data.sel(time=data.time.dt.month.isin(months_map[season]))
                loaded_data.append(data)

        else:
            data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
            loaded_data = load(data_pts, progress_bar=True)

            # Filter for specific months
            loaded_data = loaded_data.sel(
                time=loaded_data.time.dt.month.isin(months_map[season])
            )

    elif approach == "warming_level":

        # Don't load or print anything since warming levels are already loaded
        if separate_files:
            loaded_data = data_pts

        else:
            # Combine data points into a single xr.Dataset to load in and calculate metrics on
            data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
            loaded_data = load(data_pts, progress_bar=True)

    # Export raw data if desired
    if export_method == "off-ramp" or export_method == "both":

        print("\n--- Offramp Exporting Raw Data ---")

        if separate_files:
            for pt_idx in range(len(loaded_data)):
                export(
                    loaded_data[pt_idx],
                    filename=f"{variable.replace(' ', '_')}_{pt_idx}",
                    format=file_format,
                )
        else:
            export(loaded_data, filename="combined_raw_data", format=file_format)

        if export_method == "off-ramp":
            return loaded_data

    print("\n--- Calculating Metrics ---")

    #     # Filtering for specific seasons
    #     if approach == 'time':
    #         if separate_files:
    #             for pt_idx in range(len(loaded_data)): # Have to use indexing because of weird behavior modifying DataArrays in place within a for-loop
    #                 loaded_data[pt_idx]= loaded_data[pt_idx].sel(time=loaded_data[pt_idx].time.dt.month.isin(months_map[season]))
    #         else:
    #             loaded_data = loaded_data.sel(time=loaded_data.time.dt.month.isin(months_map[season]))

    #     elif approach == 'warming_level':
    #         pass # Explicitly writing this so that it's clear that the warming levels data is already pulled from `WarmingLevels` with the months filter applied to it.

    if separate_files:
        metric_data = []
        # Calculate and export into separate files
        for point in loaded_data:
            calc_val = metric_agg(
                point, approach, metric_calc, threshold, one_in_x, percentile
            )
            # calc_val.name = f"Days per year with {metric_calc.capitalize()} Daily {variable} above threshold of {threshold} {units}" # Calvin- redo these names and add the data history to the attributes instead
            metric_data.append(calc_val)

    # Export a combined file of all metrics
    else:
        metric_data = metric_agg(
            loaded_data, approach, metric_calc, threshold, one_in_x, percentile
        )
        # metric_data.name = f"Days per year with {metric_calc.capitalize()} Daily {variable} above threshold of {threshold} {units}"

    # Export the data
    print("\n--- Exporting Metric Data ---")

    # Run calculations or threshold counts if desired
    if (
        export_method == "calculate" or export_method == "both"
    ):  # Calvin- I think this condition is always true if you get to this part of the code

        if separate_files:
            for pt_idx in range(len(metric_data)):
                tmp = metric_data[
                    pt_idx
                ].copy()  # This is because of some weird attribute in threshold exports, not sure how to resolve that yet. Q: Victoria- will you know which attribute it could be?
                tmp.attrs = {}
                export(
                    tmp,
                    filename=f"{metric_calc.capitalize()}_{variable.replace(' ', '_')}_{threshold}_{pt_idx}",
                    format=file_format,
                )

        else:
            tmp = metric_data[
                pt_idx
            ].copy()  # This is because of some weird attribute in threshold exports, not sure how to resolve that yet. Q: Victoria- will you know which attribute it could be?
            tmp.attrs = {}
            export(metric_data, filename="metric_data", format=file_format)

        # Return data based on export method, Q: How should we return the points? Dictionary of points to DataArrays?
        if export_method == "calculate":
            return metric_data
        elif export_method == "both":
            return loaded_data, metric_data

    elif export_method == "None":
        return metric_data
