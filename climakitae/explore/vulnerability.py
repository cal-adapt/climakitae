"""Tools for CAVA vulnerability assessment pilot"""

import warnings
import xarray as xr
import numpy as np
import pandas as pd

from climakitae.util.utils import (
    get_closest_gridcell,
    add_dummy_time_to_wl,
)
from climakitae.core.data_load import load
from climakitae.core.data_interface import DataParameters
from climakitae.core.data_export import export
from climakitae.explore import warming_levels
from climakitae.explore.threshold_tools import get_block_maxima, get_return_value
from IPython.display import Markdown, display

# Ignore specific warnings about division errors to create more elegant logging
warnings.filterwarnings("ignore", message="invalid value encountered in sqrt")
warnings.filterwarnings("ignore", message="divide by zero encountered in divide")


def _export_no_e(da, filename, format):
    """Exports a file but doesn't throw an exception if a file already exists."""
    try:
        export(
            da,
            filename=filename,
            format=format,
        )
    except Exception as e:
        print(e)


def _clean_wl_data(data, downscaling_method):
    """Cleans WL data to conform with time-based data for rest of function."""
    # Remove SSP descriptions
    data["all_sims"] = [
        sim_name.item().split("--")[0].strip() for sim_name in data.all_sims
    ]
    data = data.rename({"all_sims": "simulation"})

    # Elevate dimensions, if needed
    if "x" not in data.dims and downscaling_method == "Dynamical":
        data = data.expand_dims(dim="x")
    if "y" not in data.dims and downscaling_method == "Dynamical":
        data = data.expand_dims(dim="y")
    if "lat" not in data.dims and downscaling_method == "Statistical":
        data = data.expand_dims(dim="lat")
    if "lon" not in data.dims and downscaling_method == "Statistical":
        data = data.expand_dims(dim="lon")
    return data


def _metric_agg(
    da, approach, metric, heat_idx_threshold, one_in_x, percentile, name_of_calc
):
    """
    Calculate a metric on the given DataArray (da) using the specified metric type
    and heat index threshold.

    Parameters:
    -----------
    da : xarray.DataArray
        Input DataArray to compute metric and treshold on.
    metric : str
        The metric type to calculate. Supported options are "min", "max".
    heat_idx_threshold : float
        The heat index threshold for comparison.
    one_in_x : int
        Return period for 1-in-X events.
    percentile : int
        Percentile / quantile for calculating "likely".

    Returns:
    --------
    metric_data : xarray.DataArray
        Resulting DataArray from the calculated metric and heat index threshold applied.
    """
    metric_map = {
        "min": np.min,
        "max": np.max,
    }

    def _apply_metric(da):
        """Applies the hard-coded heat index threshold or percentile to the DataArray"""
        if (
            heat_idx_threshold or heat_idx_threshold == 0
        ):  # If a threshold is passed and/or if the `heat_idx_threshold` passed in is 0, since that is a valid threshold.
            calc_val = (
                (da.resample(time="1D").reduce(metric_map[metric]) > heat_idx_threshold)
                .resample(time="1Y")
                .sum()
                .mean(dim="time")
            )  # This line counts the average number of high heat days per year over this time interval

        elif one_in_x:  # Calculate 1-in-X events

            # Get 1-in-x event values
            return_vals = []
            for (
                sim
            ) in (
                da.simulation.values
            ):  # Can use xr.apply_ufunc, not just a for loop here
                ams = get_block_maxima(
                    da.sel(simulation=sim),
                    extremes_type=metric,
                    groupby=(1, "day"),
                    check_ess=False,
                )
                return_values = get_return_value(
                    ams, return_period=one_in_x, multiple_points=False
                )["return_value"]
                return_vals.append(return_values)
            calc_val = xr.concat(return_vals, dim="simulation")

            # Changing attributes for thresholds DataArray because of export issue later on
            old_group = calc_val.attrs["groupby"]
            calc_val.attrs["groupby"] = f"{old_group[0]} {old_group[1]}"

        elif percentile or percentile == 0:

            calc_val = (
                da.resample(time="1D")
                .reduce(metric_map[metric])
                .quantile(percentile / 100, dim="time")
                .assign_coords(
                    lat=da.lat, lon=da.lon
                )  # Coords are dropped during quantile method application
            )

        calc_val.name = name_of_calc
        return calc_val

    # If the data passed in is warming level data, then a dummy timestamp must be added to act as the current WL 'time' dimension
    if approach == "warming_level":
        da = add_dummy_time_to_wl(da)

    metric_data = _apply_metric(da)
    return metric_data


def _check_params_get_name(
    input_locations,  # csv file or pandas dataframe?
    time_start_year,
    time_end_year,
    units,
    variable,  ## mandatory, must eventually accept temp, precip, or heat index
    metric_calc,
    percentile,
    heat_idx_threshold,
    one_in_x,
    season,
    downscaling_method,  # default for now ## mandatory
    approach,  # GWL to follow on ## mandatory
    warming_level,
    wrf_bc,
    historical_data,  # or "historical reconstruction"
    ssp_data,
    export_method,  # off-ramp, full calculate, both
    separate_files,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format,
):
    """
    Validates parameters for the `cava_data` function, and returns transformed parameters, if needed.
    Transformed parameters returned: `ssp_selected`, `variable_type`.
    """
    # Validating lat/lon
    if "lat" not in input_locations.columns or "lon" not in input_locations.columns:
        raise ValueError("Input coordinates must have `lat` and `lon` columns.")
    elif (
        (input_locations.dtypes.lat != "float64")
        and (input_locations.dtypes.lat != "int64")
    ) or (
        (input_locations.dtypes.lon != "float64")
        and (input_locations.dtypes.lon != "int64")
    ):
        raise ValueError("Input lat/lon columns must be float64 or int64 types.")

    # Validating metric
    if metric_calc not in ["min", "max", "mean", "median"]:
        raise ValueError("Passed in metric must be min, max, median, or mean.")

    # Validating export method
    if export_method not in ["off-ramp", "calculate", "both", "None"]:
        raise ValueError(
            "Passed in export method must be 'off-ramp', 'calculate', 'both', or 'None'"
        )

    # Validating historical data type
    if historical_data not in ["Historical Climate", "Historical Reconstruction"]:
        raise ValueError(
            "Passed in historical data method must be 'Historical Climate' or 'Historical Reconstruction'"
        )

    # Validating warming level inputs
    if approach == "warming_level" and warming_level not in [
        "1.5",
        "2.0",
        "3.0",
        "4.0",
    ]:
        raise ValueError("Passed in warming level must be '1.5', '2.0', '3.0', '4.0'")

    # Validating that only heat_idx_threshold or percentile is being passed in
    if (
        (heat_idx_threshold != None and percentile != None)
        or (heat_idx_threshold != None and one_in_x != None)
        or (percentile != None and one_in_x != None)
    ):
        raise ValueError(
            "Only one of heat_idx_threshold, one_in_x, and percentile arguments can both be passed in. The others must be `None`."
        )

    # Check that not all 3 customizable metric arguments are none.
    if heat_idx_threshold == None and percentile == None and one_in_x == None:
        raise ValueError(
            "`heat_idx_threshold`, `percentile`, and `one_in_x` cannot all be None. Assign a parameter to one of them."
        )

    # Make sure the metric is 'min' or 'max' if `percentile` is passed in
    if percentile or percentile == 0:
        if metric_calc != "min" and metric_calc != "max":
            raise ValueError(
                "Metric aggregation must either be min or max for a percentile calculation (i.e. likely summer day high or low)."
            )

    if season != "summer" and season != "winter" and season != "all":
        raise ValueError("Season must either be `summer`, `winter`, or `all`.")

    # Validating downscaling method
    downscaling_methods = ["Dynamical", "Statistical"]
    if downscaling_method not in downscaling_methods:
        raise ValueError(
            f"Passed in downscaling method must be in {downscaling_methods}"
        )
        
    # Validate variable by downscaling method
    if downscaling_method == 'Statistical' and variable != 'Maximum air temperature at 2m':
        raise ValueError(
            f"Passed in variable for statistical downscaling must be `Maximum air temperature at 2m`"
        )

    # Validating approach
    if approach not in ["time", "warming_level"]:
        raise ValueError(f"Passed in approach must either be 'time' or 'warming_level'")

    # Validating variable
    if variable not in [
        "Air Temperature at 2m",
        "Precipitation (total)",
        "NOAA Heat Index",
        "Maximum air temperature at 2m",
    ]:
        raise ValueError(
            "Passed in variable must be `Air Temperature at 2m`, `Precipitation (total)`, `Maximum air temperature at 2m`, or `NOAA Heat Index`"
        )

    # Validating SSP
    ssp_mapping = {
        "SSP2-4.5": "SSP 2-4.5 -- Middle of the Road",
        "SSP3-7.0": "SSP 3-7.0 -- Business as Usual",
        "SSP5-8.5": "SSP 5-8.5 -- Burn it All",
    }

    # Add validation for SSPs

    # Map SSP names from user inputs to selections SSPs
    ssp_selected = [ssp_mapping[ssp] for ssp in ssp_data]

    # Determining if passed in variable is a climate variable or a derived index
    if (
        variable == "Air Temperature at 2m"
        or variable == "Precipitation (total)"
        or variable == "Maximum air temperature at 2m"
    ):
        variable_type = "Variable"
    elif variable == "NOAA Heat Index":
        variable_type = "Derived Index"
        
    # Adding time period or warming level to filename
    if approach == "time":
        approach_str = f"{time_start_year} to {time_end_year}"
        approach_var_name = f"from {approach_str}"
    elif approach == "warming_level":
        if warming_level == "1.5":
            wl_str = "15"
        else:
            wl_str = str(int(float(warming_level)))
        approach_str = f"{wl_str}degreeWL"
        approach_var_name = f"for Warming Level {warming_level}Â°C"

    # Retrieving the name of the calculation about to occur
    if percentile != None and heat_idx_threshold == None and one_in_x == None:

        def ordinal(n):
            """Find ordinal name for a number (1 -> 1st, 2 -> 2nd, 3 -> 3rd)"""
            return f"{n}{('th' if 11 <= n % 100 <= 13 else {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th'))}"

        var_name = f"{ordinal(percentile)} percentile of Daily {metric_calc.capitalize()} of {variable} for {season} seasons {approach_var_name}"
        offramp_name = "likely_seasonal_temperature_raw_data"
        calc_name = f"likely_seasonal_{season}_{metric_calc}_{approach_str}"

    elif percentile == None and heat_idx_threshold != None and one_in_x == None:
        var_name = f"Days per year with {metric_calc.capitalize()} Daily {variable} above a heat index threshold of {heat_idx_threshold} {units} for {season} seasons {approach_var_name}"
        offramp_name = "heat_index_raw_data"
        calc_name = f"heat_index_thresh{heat_idx_threshold}_{approach_str}"

    elif percentile == None and heat_idx_threshold == None and one_in_x != None:
        var_name = f"1-in-{one_in_x} year {metric_calc.capitalize()} for {variable} for {season} seasons {approach_var_name}"
        offramp_name = f"one_in_{one_in_x}_temperature_raw_data"
        calc_name = f"one_in_{one_in_x}_temperature_{approach_str}"

    return (
        {"ssp_selected": ssp_selected, "variable_type": variable_type},
        {"var_name": var_name, "offramp_name": offramp_name, "calc_name": calc_name},
    )


def cava_data(
    input_locations,  # csv file or pandas dataframe?
    variable,  # must eventually accept temp, precip, or heat index
    approach="time",  # GWL to follow on ## mandatory, Q: Why is anom required for non BC WRF data?
    downscaling_method="Dynamical",  # default for now, statistical to follow ## mandatory
    time_start_year=1981,  # default setting, optional
    time_end_year=2010,  # default setting, optional
    historical_data="Historical Climate",  # or "historical reconstruction"
    ssp_data=["SSP3-7.0"],
    warming_level="1.5",
    metric_calc="max",
    heat_idx_threshold=None,
    one_in_x=None,
    percentile=None,
    season="all",  ## default to all so no subsetting occurs unless called
    units="degF",  ## default for now
    wrf_bc=True,
    export_method="both",  # off-ramp, full calculate, both
    separate_files=True,  # Toggle to determine whether or not the user wants to separate climate variable information into separate files
    file_format="NetCDF",
):
    """Retrieve, process, and export climate data based on inputs.

    Designed for CAVA reports.

    Parameters
    ----------
    input_locations : pandas.DataFrame
        Input locations containing 'lat' and 'lon' columns.
    variable : str
        Type of climate variable to retrieve and calculate.
    approach : str
        Approach to follow, default is "time".
    downscaling_method : str
        Method of downscaling, default is "Dynamical".
    time_start_year : int, optional
        Starting year for data selection.
    time_end_year : int, optional
        Ending year for data selection.
    historical_data : str, optional
        Type of historical data, default is "Historical Climate".
    ssp_data : str, optional
        Shared Socioeconomic Pathway data, default is "SSP3-7.0".
    metric_calc : str, optional
        Metric calculation type (e.g., 'mean', 'max', 'min') for supported metrics. Default is "max"
    heat_idx_threshold : float
        Heat index threshold for counting events.
    one_in_x : int, optional
        Return period for 1-in-X events.
    percentile : int, optional
        Percentile for calculating "likely" event occurrence.
    season : str, optional
        Season to subset time dimension on (e.g., 'summer', 'winter', 'all'). Default is 'all'.
    units : str, optional
        Units for the retrieved data.
    wrf_bc : str, optional
        Flag to subset the WRF data for the bias-adjusted models. Default is True.
    export_method : str, optional
        Export method, options are 'off-ramp', 'calculate', 'both', default is 'both'.
    file_format : str, optional
        Export file format options.
    separate_files : bool, optional
        Whether to separate climate variable information into separate files, default is True.

    Returns
    -------
    metric_data : xarray.DataArray
        Computed climate metrics for input locations.

    Raises
    ------
    ValueError
        If input coordinates lack 'lat' and 'lon' columns or if 'lat'/'lon' columns are not of type float64 or int64.
    """
    clean_params, names = _check_params_get_name(
        **locals()
    )  # This may not be best practice. Also to note, this line MUST be the first line (or at least before other local vars are defined), or else they will also be passed to `check_and_set_params`.

    months_map = {"winter": [12, 1, 2], "summer": [6, 7, 8], "all": np.arange(1, 13)}

    display(Markdown(f"**Calculating {names['var_name']}**"))

    print(f"--- Selecting Data Points --- \n")
    data_pts = []
    for index, loc in input_locations.iterrows():

        print(f"Selecting data for {loc['lat'], loc['lon']}")
        lat, lon = loc["lat"], loc["lon"]

        if approach == "time":

            selections = DataParameters()
            selections.data_type = "Gridded"
            selections.downscaling_method = downscaling_method
            selections.scenario_historical = [historical_data]
            selections.scenario_ssp = clean_params["ssp_selected"]
            selections.timescale = (
                "hourly" if downscaling_method == "Dynamical" else "daily"
            )  # Toggling time-resolution for WRF and LOCA data respectively
            selections.variable_type = clean_params["variable_type"]
            selections.variable = variable
            selections.latitude = (
                lat - 0.02,
                lat + 0.02,
            )  # Q: What padding to put for lat/lon values?
            selections.longitude = (lon - 0.02, lon + 0.02)
            selections.time_slice = (time_start_year, time_end_year)
            selections.resolution = "3 km"
            selections.units = units

            data = selections.retrieve()

            # Remove leap days, if applicable
            data = data.sel(
                time=~((data.time.dt.month == 2) & (data.time.dt.day == 29))
            )

        elif approach == "warming_level":

            wl = warming_levels()
            wl.wl_params.timescale = (
                "hourly" if downscaling_method == "Dynamical" else "daily"
            )  # Toggling time-resolution for WRF and LOCA data respectively
            wl.wl_params.downscaling_method = downscaling_method
            wl.wl_params.variable_type = clean_params["variable_type"]
            wl.wl_params.variable = variable
            wl.wl_params.latitude = (lat - 0.02, lat + 0.02)
            wl.wl_params.longitude = (lon - 0.02, lon + 0.02)
            wl.wl_params.warming_levels = [
                warming_level
            ]  # Calvin- default, only allow for 1 warming level to be passed in.
            wl.wl_params.units = units
            wl.wl_params.resolution = "3 km"
            wl.wl_params.anom = "No"
            # wl.wl_params.months = months_map[season] # Commenting out months because they are integrated separately in https://github.com/cal-adapt/climakitae/pull/363/.

            wl.calculate()

            # Clean WL data before using
            data = _clean_wl_data(wl.sliced_data[warming_level], downscaling_method)

        if downscaling_method == "Dynamical" and wrf_bc:
            # Filter only for BC-WRF models
            bc_wrf = [
                "WRF_EC-Earth3_r1i1p1f1",
                "WRF_MPI-ESM1-2-HR_r3i1p1f1",
                "WRF_TaiESM1_r1i1p1f1",
                "WRF_MIROC6_r1i1p1f1",
            ]
            # This check is to see which simulations have these BC models as the root part of their simulation name, which is to accomodate for simulation names being the same across SSPs.
            data = data.sel(
                simulation=[
                    sim
                    for sim in data.simulation.values
                    if any(
                        s in sim
                        for s in [
                            "WRF_EC-Earth3_r1i1p1f1",
                            "WRF_MPI-ESM1-2-HR_r3i1p1f1",
                            "WRF_TaiESM1_r1i1p1f1",
                            "WRF_MIROC6_r1i1p1f1",
                        ]
                    )
                ]
            )

        # Find the closest gridcell from the data to the lat/lon point
        if downscaling_method == "Dynamical":
            closest_cell = get_closest_gridcell(
                data, lat, lon, print_coords=False
            ).squeeze()
        else:
            closest_cell = data.sel(lat=lat, lon=lon, method="nearest")

        # Storing data differently if all data points should be in one file vs each data point in its own file
        if not separate_files:
            # Renaming gridcell so that it can be concatenated with other lat/lon gridcells
            closest_cell["simulation"] = [
                "{}_{}_{}".format(
                    sim_name, closest_cell.lat.item(), closest_cell.lon.item()
                )
                for sim_name in closest_cell.simulation
            ]

        data_pts.append(closest_cell)

    # Determining how to concatenate or load data, if necessary, depending on `approach` and `separate_files` parameters.
    if approach == "time":
        print("\n--- Loading Data into Memory ---\n")

        if separate_files:
            loaded_data = []
            for point in data_pts:
                print(f"Point ({point.lat.item()}, {point.lon.item()})")
                data = load(point, progress_bar=True)

                # Filter for specific months
                data = data.sel(time=data.time.dt.month.isin(months_map[season]))
                loaded_data.append(data)

        else:
            data_pts = xr.concat(data_pts, dim="simulation").chunk(chunks="auto")
            loaded_data = load(data_pts, progress_bar=True)

    elif approach == "warming_level":

        # Don't load or print anything since warming levels are already loaded
        loaded_data = data_pts

    # Export raw data if desired
    if export_method == "off-ramp" or export_method == "both":

        print("\n--- Offramp Exporting Raw Data ---")

        if separate_files:
            for pt_idx in range(len(loaded_data)):
                _export_no_e(
                    loaded_data[pt_idx],
                    filename=names["offramp_name"],
                    format=file_format,
                )
        else:
            _export_no_e(loaded_data, filename="combined_raw_data", format=file_format)

        if export_method == "off-ramp":
            return loaded_data

    print("\n--- Calculating Metrics ---")
    print("Calculating... ", end="", flush=True)

    if separate_files:
        metric_data = []
        # Calculate and export into separate files
        for point in loaded_data:
            calc_val = _metric_agg(
                point,
                approach,
                metric_calc,
                heat_idx_threshold,
                one_in_x,
                percentile,
                names["var_name"],
            )
            metric_data.append(calc_val)

    # Export a combined file of all metrics
    else:
        metric_data = _metric_agg(
            loaded_data,
            approach,
            metric_calc,
            heat_idx_threshold,
            one_in_x,
            percentile,
            names["var_name"],
        )
    print("Complete!")

    # Export the data
    print("\n--- Exporting Metric Data ---")

    if export_method == "calculate" or export_method == "both":

        if separate_files:
            for pt_idx in range(len(metric_data)):
                _export_no_e(
                    metric_data[pt_idx],
                    filename=f"{names['calc_name']}_{str(lat).replace('.', '')}N_{str(lon).replace('.', '')}W",
                    format=file_format,
                )  # Will need to include naming convention for calculated file too.

        else:
            _export_no_e(metric_data, filename="metric_data", format=file_format)

        # Calvin- Look into how to return data (dictionaries of indices to points)
        if export_method == "calculate":
            return {"calc_data": metric_data}
        elif export_method == "both":
            return {"calc_data": metric_data, "offramp_data": loaded_data}

    elif export_method == "None":  # Specific for table generation
        return {"calc_data": metric_data}
