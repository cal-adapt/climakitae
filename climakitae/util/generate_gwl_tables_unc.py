"""
Utility for generating Global Warming Level (GWL) reference data files.

This script generates CSV files (e.g., "gwl_1981-2010ref_EC-Earth3_ssp370.csv")
in the ../data directory. These files are intended for use in uncertainty analysis,
specifically within ../explore/uncertainty.py.

Each CSV file contains, for every specified ensemble member of a given GCM (e.g., EC-Earth3),
the calculated times when five major warming levels (1.5, 2.0, 2.5, 3.0, 4.0 degrees C)
are projected to be reached under a specific SSP scenario (e.g., SSP3-7.0), relative
to a defined reference period.

To run this script:
    python generate_gwl_tables_unc.py
Monitor the console output for progress updates.
"""

from typing import Dict, List, Tuple, Union

import numpy as np
import pandas as pd
import s3fs
import xarray as xr

from climakitae.util.utils import write_csv_file


def make_weighted_timeseries(temp: xr.DataArray) -> xr.DataArray:
    """
    Creates a spatially-weighted single-dimension time series of global temperature.

    Weights latitude grids by the square root of the cosine of their latitude
    (proportional to grid cell area) and averages across all longitudes.

    Parameters
    ----------
    temp : xr.DataArray
        Global temperature data with latitude and longitude coordinates.
        Coordinates can be named 'lat', 'lon' or 'latitude', 'longitude'.

    Returns
    -------
    xr.DataArray
        Spatially weighted and longitudinally averaged global temperature time series.
    """
    # Determine coordinate names
    lat, lon = "lat", "lon"
    if "latitude" in temp.coords and "longitude" in temp.coords:
        lat, lon = "latitude", "longitude"
    elif "lat" not in temp.coords or "lon" not in temp.coords:
        raise ValueError(
            "Input DataArray must have latitude and longitude coordinates."
        )

    # Weight by latitude and average over longitude
    weightlat = np.sqrt(np.cos(np.deg2rad(temp[lat])))
    weightlat = weightlat / weightlat.sum()  # Normalize weights
    timeseries = (temp * weightlat).sum(dim=lat).mean(dim=lon)
    return timeseries


class GWLGenerator:
    """
    Generates Global Warming Level (GWL) reference data tables.

    Orchestrates the process of fetching CMIP6 data, calculating anomalies,
    smoothing time series, and determining the timing of GWL crossings for
    various models, scenarios, and ensemble members.

    Attributes
    ----------
    df : pd.DataFrame
        DataFrame containing metadata for CMIP6 simulations, loaded from the
        Pangeo CMIP6 catalog.
    sims_on_aws : pd.DataFrame
        DataFrame listing available simulations on AWS S3, filtered for 'tas'
        variable in 'Amon' table, indexed by model ('source_id'), with columns
        for scenarios ('experiment_id'). Values are lists of available ensemble
        members ('member_id').
    fs : s3fs.S3FileSystem
        S3 file system object for anonymous access to AWS S3 data.

    Methods
    -------
    get_sims_on_aws()
        Filters the main CMIP6 catalog to find relevant 'tas' simulations.
    build_timeseries(model_config)
        Loads and concatenates historical and scenario time series for a model member.
    get_gwl(smoothed, degree)
        Finds the first time a smoothed anomaly series crosses a specific GWL.
    get_gwl_table_for_single_model_and_ensemble(model_config, reference_period)
        Calculates GWL crossing times for one model ensemble member.
    get_gwl_table(model_config, reference_period)
        Aggregates GWL crossing times across all available ensemble members for a model.
    generate_gwl_file(models, scenarios, reference_periods)
        Main method to generate and save the GWL reference CSV files.
    """

    def __init__(self, df: pd.DataFrame, sims_on_aws: pd.DataFrame = None):
        """
        Initializes the GWLGenerator.

        Parameters
        ----------
        df : pd.DataFrame
            DataFrame containing metadata for CMIP6 simulations.
        sims_on_aws : pd.DataFrame, optional
            Pre-computed DataFrame listing available simulations on AWS.
            If None (default), it will be generated by calling get_sims_on_aws().
        """
        self.df: pd.DataFrame = df
        self.fs: s3fs.S3FileSystem = s3fs.S3FileSystem(anon=True)
        self.sims_on_aws: pd.DataFrame = (
            sims_on_aws if sims_on_aws is not None else self.get_sims_on_aws()
        )

    def get_sims_on_aws(self) -> pd.DataFrame:
        """
        Filters the CMIP6 catalog DataFrame to identify available simulations.

        Selects simulations for the 'tas' variable from the 'Amon' table,
        identifies models with both historical and at least one SSP run,
        and ensures consistency of ensemble members across historical and SSP runs.

        Returns
        -------
        pd.DataFrame
            Filtered DataFrame indexed by model ('source_id') with columns for
            scenarios, containing lists of valid, consistent ensemble members.
        """
        df_subset = self.df[
            (self.df.table_id == "Amon")
            & (self.df.variable_id == "tas")
            & (self.df.experiment_id == "historical")
        ]
        models: List[str] = sorted(list(set(df_subset.source_id)))

        scenarios: List[str] = ["historical", "ssp585", "ssp370", "ssp245", "ssp126"]
        sims_on_aws = pd.DataFrame(index=models, columns=scenarios)

        # Populate DataFrame with available ensemble members for each model/scenario
        for model in models:
            for scenario in scenarios:
                df_scenario = self.df[
                    (self.df.table_id == "Amon")
                    & (self.df.variable_id == "tas")
                    & (self.df.experiment_id == scenario)
                    & (self.df.source_id == model)
                ]
                ensMembers: List[str] = list(set(df_scenario.member_id))
                sims_on_aws.loc[model, scenario] = ensMembers

        # Filter out models lacking historical runs or any SSP runs
        models_to_drop: List[str] = []
        for model in sims_on_aws.index:
            has_historical = len(sims_on_aws.loc[model, "historical"]) > 0
            has_ssp = any(
                len(sims_on_aws.loc[model, ssp]) > 0
                for ssp in scenarios
                if ssp != "historical"
            )
            if not has_historical or not has_ssp:
                models_to_drop.append(model)
        sims_on_aws = sims_on_aws.drop(index=models_to_drop)

        # Ensure historical ensemble members are also present in at least one SSP scenario
        for model in sims_on_aws.index:
            historical_members = sims_on_aws.loc[model, "historical"]
            ssp_members_sets = [
                set(sims_on_aws.loc[model, ssp])
                for ssp in scenarios
                if ssp != "historical"
            ]
            # Find historical members present in the union of all SSP members for this model
            valid_historical_members = [
                member
                for member in historical_members
                if any(member in ssp_set for ssp_set in ssp_members_sets)
            ]
            sims_on_aws.loc[model, "historical"] = list(
                set(valid_historical_members)
            )  # Store unique valid members

        sims_on_aws.index.name = "source_id"
        return sims_on_aws

    def build_timeseries(
        self, model_config: Dict[str, Union[str, List[str]]]
    ) -> xr.Dataset:
        """
        Builds concatenated historical + scenario time series for a model member.

        Loads data from S3 using zarr, applies spatial weighting, and concatenates
        the historical run with each specified future scenario run.

        Parameters
        ----------
        model_config : dict
            Configuration dictionary specifying 'variable', 'model', 'ens_mem',
            and 'scenarios' (list of SSPs).

        Returns
        -------
        xr.Dataset
            Dataset containing the concatenated time series for each requested
            scenario, with time as the dimension. Returns an empty Dataset if
            data cannot be loaded or processed.
        """
        variable: str = model_config["variable"]
        model: str = model_config["model"]
        ens_mem: str = model_config["ens_mem"]
        scenarios: List[str] = model_config["scenarios"]

        data_historical: xr.DataArray = None  # Initialize as None

        # Load historical data
        df_hist = self.df[
            (self.df.table_id == "Amon")
            & (self.df.variable_id == variable)
            & (self.df.experiment_id == "historical")
            & (self.df.source_id == model)
            & (self.df.member_id == ens_mem)
        ]
        if not df_hist.empty:
            try:
                with xr.open_zarr(
                    self.fs.get_mapper(df_hist.zstore.values[0])
                ) as temp_hist:
                    data_historical = make_weighted_timeseries(temp_hist[variable])
                    data_historical = data_historical.sortby("time")
            except Exception as e:
                print(f"Error loading historical data for {model} {ens_mem}: {e}")
                return xr.Dataset()  # Return empty if historical fails

        if data_historical is None:
            print(f"No valid historical data found for {model} {ens_mem}. Skipping.")
            return xr.Dataset()  # Return empty if no historical data

        # Load and concatenate scenario data
        data_one_model = xr.Dataset()
        for scenario in scenarios:
            if (
                scenario in self.sims_on_aws.columns  # Check if scenario column exists
                and model in self.sims_on_aws.index  # Check if model row exists
                and ens_mem
                in self.sims_on_aws.loc[
                    model, scenario
                ]  # Check if member is listed for this model/scenario
            ):
                df_scenario = self.df[
                    (self.df.table_id == "Amon")
                    & (self.df.variable_id == variable)
                    & (self.df.experiment_id == scenario)
                    & (self.df.source_id == model)
                    & (self.df.member_id == ens_mem)
                ]
                if not df_scenario.empty:
                    try:
                        # Use decode_times=False and manual slicing/decoding for robustness
                        with xr.open_zarr(
                            self.fs.get_mapper(df_scenario.zstore.values[0]),
                            decode_times=False,
                        ) as temp_scenario_raw:
                            # Limit time steps and decode
                            temp_scenario = temp_scenario_raw.isel(time=slice(0, 1032))
                            temp_scenario = xr.decode_cf(temp_scenario)

                            timeseries_scenario = make_weighted_timeseries(
                                temp_scenario[variable]
                            )
                            timeseries_scenario = timeseries_scenario.sortby("time")

                            # Concatenate historical and scenario data
                            data_var = xr.concat(
                                [data_historical, timeseries_scenario], dim="time"
                            )
                            data_var.name = (
                                scenario  # Ensure the DataArray has the correct name
                            )
                            data_one_model[scenario] = data_var
                    except Exception as e:
                        print(
                            f"Error processing scenario {scenario} for {model} {ens_mem}: {e}"
                        )
                        # Continue to next scenario if one fails
            else:
                print(
                    f"Scenario {scenario} or member {ens_mem} not available for model {model}. Skipping scenario."
                )

        return data_one_model

    @staticmethod
    def get_gwl(smoothed: pd.DataFrame, degree: float) -> pd.Series:
        """
        Computes the timestamp when each scenario first reaches a given GWL.

        Parameters
        ----------
        smoothed : pd.DataFrame
            DataFrame where columns are scenarios and rows are smoothed temperature anomalies indexed by time.
        degree : float
            The global warming level (anomaly in degrees C) to detect.

        Returns
        -------
        pd.Series
            Series indexed by scenario name, containing the first timestamp
            when the warming level `degree` is crossed. Returns pd.NaT for
            scenarios that never cross the level.
        """

        def get_wl_timestamp(
            scenario_series: pd.Series, degree_threshold: float
        ) -> pd.Timestamp:
            """Find the first timestamp crossing the degree threshold."""
            crossing_times = scenario_series[scenario_series >= degree_threshold].index
            if not crossing_times.empty:
                return crossing_times[0]
            else:
                return pd.NaT  # Use NaT for not reached

        # Apply the helper function to each column (scenario)
        gwl_series = smoothed.apply(
            lambda scenario_col: get_wl_timestamp(scenario_col, degree)
        )
        return gwl_series  # Return a Series, not a DataFrame

    def get_gwl_table_for_single_model_and_ensemble(
        self,
        model_config: Dict[str, Union[str, List[str]]],
        reference_period: Dict[str, str],
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Generates GWL table and smoothed anomaly data for one model ensemble member.

        Calculates temperature anomalies relative to the reference period, smooths them,
        finds GWL crossing times, and returns both the GWL table and the smoothed data.

        Parameters
        ----------
        model_config : dict
            Configuration dictionary specifying 'variable', 'model', 'ens_mem', 'scenarios'.
        reference_period : dict
            Dictionary with 'start_year' and 'end_year' (YYYYMMDD format) for the baseline.

        Returns
        -------
        tuple[pd.DataFrame, pd.DataFrame]
            - gwlevels: DataFrame indexed by scenario, columns are GWLs (1.5, 2.0, ...),
                        values are timestamps of first crossing (or NaT).
            - final_model: DataFrame of smoothed temperature anomalies, indexed by time,
                           columns named '{ens_mem}_{scenario}'.
            Returns (empty DataFrame, empty DataFrame) if processing fails.
        """
        model: str = model_config["model"]
        ens_mem: str = model_config["ens_mem"]
        start_year: str = reference_period["start_year"]
        end_year: str = reference_period["end_year"]

        data_one_model = self.build_timeseries(model_config)
        if not data_one_model.data_vars:  # Check if dataset is empty
            print(
                f"Build timeseries returned empty dataset for {model} {ens_mem}. Cannot generate GWL table."
            )
            return pd.DataFrame(), pd.DataFrame()

        try:
            # Calculate anomaly relative to reference period mean
            ref_slice = slice(start_year, end_year)
            try:
                ref_mean = data_one_model.sel(time=ref_slice).mean("time")
            except Exception:  # Handle potential calendar issues with end date
                end_date_adjusted = str(
                    (pd.to_datetime(end_year) - pd.DateOffset(days=1)).date()
                ).replace("-", "")
                ref_slice_adjusted = slice(start_year, end_date_adjusted)
                ref_mean = data_one_model.sel(time=ref_slice_adjusted).mean("time")
            anom = data_one_model - ref_mean
        except Exception as e:
            print(f"Error calculating anomaly for {model} {ens_mem}: {e}")
            return pd.DataFrame(), pd.DataFrame()

        # Apply 20-year centered rolling mean for smoothing
        smoothed = anom.rolling(time=20 * 12, center=True).mean("time")

        # Convert smoothed data to pandas DataFrame
        one_model_smoothed_df = (
            smoothed.to_array(
                dim="scenario", name=model
            )  # Combine scenarios into one DataArray
            .dropna(
                "time", how="all"
            )  # Drop times where all scenarios are NaN (usually ends due to rolling mean)
            .to_pandas()  # Convert to DataFrame [time x scenario]
        )

        if one_model_smoothed_df.empty:
            print(
                f"Smoothed data is empty for {model} {ens_mem}. Cannot calculate GWLs."
            )
            return pd.DataFrame(), pd.DataFrame()

        # Calculate GWL crossing times for standard levels
        gwlevels = pd.DataFrame(
            index=one_model_smoothed_df.columns
        )  # Index by scenario
        gwlevels.index.name = "scenario"  # Explicitly name the index
        try:
            for level in [1.5, 2.0, 2.5, 3.0, 4.0]:
                # Pass transposed DataFrame [scenario x time] to get_gwl
                gwlevels[level] = self.get_gwl(one_model_smoothed_df, level)
        except Exception as e:
            print(f"Error calculating GWL crossings for {model} {ens_mem}: {e}")
            # Return potentially partial results or empty frame if error was early
            return gwlevels, pd.DataFrame()  # Return empty for final_model on error

        # Prepare the final smoothed anomaly DataFrame with renamed columns
        final_model = one_model_smoothed_df.copy()
        final_model.columns = [f"{ens_mem}_{col}" for col in final_model.columns]

        return gwlevels, final_model

    def get_gwl_table(
        self,
        model_config: Dict[str, Union[str, List[str]]],
        reference_period: Dict[str, str],
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Generates aggregated GWL tables across all available ensemble members for a model.

        Iterates through ensemble members listed in `sims_on_aws`, calls
        `get_gwl_table_for_single_model_and_ensemble` for each, and concatenates
        the results.

        Parameters
        ----------
        model_config : dict
            Configuration dictionary specifying 'variable', 'model', 'scenarios'.
            'ens_mem' will be added internally for each member.
        reference_period : dict
            Dictionary with 'start_year' and 'end_year' for the baseline.

        Returns
        -------
        tuple[pd.DataFrame, pd.DataFrame]
            - gwlevels_concat: DataFrame containing GWL crossing times, indexed by
                                MultiIndex (ensemble_member, scenario), columns are GWLs.
            - wl_data_tbl_sim: DataFrame of concatenated smoothed anomalies, indexed by time,
                                columns named by scenario only (duplicates possible).
            Returns (empty DataFrame, empty DataFrame) if no valid members are found or processed.
        """
        model: str = model_config["model"]
        scenarios: List[str] = model_config["scenarios"]

        # Determine the list of ensemble members to process based on a representative scenario (e.g., ssp370)
        # Assumes member availability is consistent across relevant scenarios used in generate_gwl_file
        lookup_scenario = "ssp370"  # Or another common scenario expected to be present
        if (
            model in self.sims_on_aws.index
            and lookup_scenario in self.sims_on_aws.columns
        ):
            ens_mem_list: List[str] = self.sims_on_aws.loc[model, lookup_scenario]
            if not isinstance(ens_mem_list, list):
                ens_mem_list = []  # Handle case where it might not be a list
        else:
            ens_mem_list: List[str] = []

        print(
            f"Model: {model}. Found {len(ens_mem_list)} ensemble members for scenario {lookup_scenario}."
        )
        if not ens_mem_list:
            print(
                f"No ensemble members found for {model} based on {lookup_scenario}. Cannot generate GWL table."
            )
            return pd.DataFrame(), pd.DataFrame()

        # Process each ensemble member
        gwlevels_results: List[pd.DataFrame] = []
        wl_data_results: List[pd.DataFrame] = []
        successful_ens_mems: List[str] = []

        for i, ens_mem in enumerate(ens_mem_list):
            print(f"Processing ensemble member {i+1}/{len(ens_mem_list)}: {ens_mem}...")
            try:
                member_config = model_config.copy()
                member_config["ens_mem"] = ens_mem

                gwlevels_single, wl_data_single = (
                    self.get_gwl_table_for_single_model_and_ensemble(
                        member_config, reference_period
                    )
                )

                # Check if results are valid before appending
                if not gwlevels_single.empty and not wl_data_single.empty:
                    gwlevels_results.append(gwlevels_single)
                    wl_data_results.append(wl_data_single)
                    successful_ens_mems.append(ens_mem)
                else:
                    print(f"Skipping member {ens_mem} due to empty results.")

            except Exception as e:
                print(f"Error processing ensemble member {ens_mem}: {e}. Skipping.")

        # Concatenate results if any were successful
        if successful_ens_mems:
            try:
                # Concatenate GWL tables, using successful members as keys for the outer index level
                gwlevels_concat = pd.concat(
                    gwlevels_results,
                    keys=successful_ens_mems,
                    names=["ensemble_member", "scenario"],
                )

                # Concatenate smoothed anomaly tables
                # The keys ensure correct alignment if indices differ slightly
                wl_data_tbl_sim_raw = pd.concat(
                    wl_data_results, axis=1, keys=successful_ens_mems
                )

                # Rename columns to just the scenario name (level 1 of the MultiIndex)
                wl_data_tbl_sim = wl_data_tbl_sim_raw.copy()
                wl_data_tbl_sim.columns = wl_data_tbl_sim_raw.columns.get_level_values(
                    1
                )

                print(
                    f"Successfully processed {len(successful_ens_mems)} ensemble members for {model}."
                )
                return gwlevels_concat, wl_data_tbl_sim
            except Exception as e:
                print(f"Error during final concatenation for {model}: {e}")
                return pd.DataFrame(), pd.DataFrame()  # Return empty on concat error
        else:
            print(f"No ensemble members processed successfully for model {model}.")
            return pd.DataFrame(), pd.DataFrame()

    def generate_gwl_file(
        self,
        models: List[str],
        scenarios: List[str],
        reference_periods: List[Dict[str, str]],
    ) -> None:
        """
        Generates and saves the GWL reference CSV files for specified configurations.

        Iterates through models and reference periods, calls `get_gwl_table` to
        aggregate results across ensemble members, and writes the final GWL
        crossing time table to a CSV file.

        Parameters
        ----------
        models : list[str]
            List of model names (source_id) to process.
        scenarios : list[str]
            List of SSP scenario names (experiment_id) to include in the analysis.
            Note: The current implementation primarily uses 'ssp370' to find members
            and expects data for the listed scenarios to exist for those members.
        reference_periods : list[dict]
            List of dictionaries, each defining a reference period with
            'start_year' and 'end_year' (YYYYMMDD format).
        """
        variable: str = "tas"  # Hardcoded to 'tas' for temperature anomalies

        for period in reference_periods:
            start_year: str = period["start_year"]
            end_year: str = period["end_year"]
            ref_period_str = f"{start_year[:4]}-{end_year[:4]}"
            print(
                f"\n--- Generating GWL table for reference period: {ref_period_str} ---"
            )

            all_gw_tables_for_period: List[pd.DataFrame] = []
            processed_models: List[str] = (
                []
            )  # Keep track of models successfully processed for this period

            # Process each model
            for i, model in enumerate(models):
                print(f"\nProcessing Model {i+1}/{len(models)}: {model}...")

                model_config = {
                    "variable": variable,
                    "model": model,
                    "scenarios": scenarios,  # Pass all requested scenarios
                }

                # Get aggregated GWL table for the model across its members
                gw_table_model, _ = self.get_gwl_table(
                    model_config, period
                )  # Ignore the smoothed data here

                if not gw_table_model.empty:
                    all_gw_tables_for_period.append(gw_table_model)
                    processed_models.append(
                        model
                    )  # Add model name for final concatenation keys
                else:
                    print(
                        f"No GWL data generated for model {model} for period {ref_period_str}."
                    )

            # Concatenate results from all models for this reference period
            if all_gw_tables_for_period:
                # Concatenate using model names as the outermost index level
                final_gwl_table = pd.concat(
                    all_gw_tables_for_period,
                    keys=processed_models,
                    names=["GCM", "ensemble_member", "scenario"],
                )

                # Define output filename (consider making scenario part dynamic if needed)
                # Assuming a primary scenario focus for naming, e.g., ssp370
                primary_scenario = scenarios[0] if scenarios else "unknown_scenario"
                filename = f"data/gwl_{ref_period_str}ref_{models[0]}_{primary_scenario}.csv"  # Naming based on first model/scenario

                # Write to CSV
                try:
                    write_csv_file(final_gwl_table, filename)
                    print(f"\nSuccessfully wrote GWL file: {filename}")
                except Exception as e:
                    print(f"\nError writing CSV file {filename}: {e}")
            else:
                print(
                    f"\nNo GWL data generated for any model for reference period {ref_period_str}."
                )


def main() -> None:
    """
    Main execution function.

    Loads the CMIP6 catalog, initializes the GWLGenerator, defines the
    models, scenarios, and reference periods to process, and calls
    generate_gwl_file to produce the output CSV.
    """
    # Load CMIP6 data catalog
    try:
        df = pd.read_csv("https://cmip6-pds.s3.amazonaws.com/pangeo-cmip6.csv")
    except Exception as e:
        print(f"Error loading CMIP6 catalog: {e}")
        return

    # Initialize the GWL generator
    try:
        gwl_generator = GWLGenerator(df)
    except Exception as e:
        print(f"Error initializing GWLGenerator: {e}")
        return

    # --- Configuration ---
    models_to_process: List[str] = ["EC-Earth3"]
    scenarios_to_process: List[str] = [
        "ssp370"
    ]  # Primarily used for member lookup and output naming convention
    reference_periods_to_process: List[Dict[str, str]] = [
        {"start_year": "19810101", "end_year": "20101231"}
    ]
    # --- End Configuration ---

    # Generate GWL files
    try:
        gwl_generator.generate_gwl_file(
            models_to_process, scenarios_to_process, reference_periods_to_process
        )
    except Exception as e:
        print(f"Error generating GWL file: {e}")
        return


if __name__ == "__main__":
    main()
