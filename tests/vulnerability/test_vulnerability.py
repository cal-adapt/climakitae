import pytest
import xarray as xr
import pandas as pd
import numpy as np
import os
from climakitae.util.utils import (
    read_csv_file,
    get_closest_gridcell,
    add_dummy_time_to_wl,
)
from climakitae.core.paths import stations_csv_path
from climakitae.core.data_export import export
from climakitae.explore.threshold_tools import get_block_maxima, get_return_value
from climakitae.explore.vulnerability import (
    _export_no_e,
    _clean_wl_data,
    _filter_ba_models,
    _metric_agg,
    CavaParams,
    cava_data,
)
from unittest.mock import MagicMock, patch

##### DONE #####


# # Test `export_no_e` on a series of potential exports.
# def test_export_no_e(test_data_2022_monthly_45km):
#     """
#     Test `_export_no_e` function to ensure it correctly handles exporting data to a file.

#     The test verifies that:
#     1. The function creates a file when it doesn't exist.
#     2. The function does not modify an existing file when called again.
#     3. The file is deleted correctly after the test.

#     Parameters:
#     test_data_2022_monthly_45km (xarray.Dataset): Test dataset used for export.
#     """
#     # Export dataset
#     export_path = os.path.join(
#         os.path.dirname(os.path.abspath(__file__)), "random_file.nc"
#     )
#     _export_no_e(test_data_2022_monthly_45km, export_path, "NetCDF")

#     # Assert path exists
#     assert os.path.exists(export_path), "The file should be created."

#     # Try exporting it again
#     _export_no_e(test_data_2022_monthly_45km, export_path, "NetCDF")

#     # Check that the file still exists and has not been modified
#     assert os.path.exists(export_path), "The file should still exist."

#     # Delete exported file
#     os.remove(export_path)
#     assert not os.path.exists(export_path), "The file should have been deleted."


# def test_clean_wl_data(test_dataset_WL_Alamedacounty_45km_hourly):
#     """
#     Test `_clean_wl_data` function to ensure it correctly cleans and renames the simulation dimension.

#     The test asserts that:
#     1. The `all_sims` dimension is renamed to `simulation`.
#     2. Descriptions for SSPs are removed when `separate_files` is True.
#     3. The original number of simulations is preserved.

#     Parameters:
#     test_dataset_WL_Alamedacounty_45km_hourly (xarray.Dataset): Test dataset used for cleaning.
#     """
#     orig_ds = test_dataset_WL_Alamedacounty_45km_hourly
#     num_sims = len(orig_ds["all_sims"])

#     # Assert that the simulation names are in their original form
#     assert "all_sims" in orig_ds.dims, "Simulation dimension should be `all_sims`."
#     assert (
#         "WRF_CESM2_r11i1p1f1_Historical + SSP 3-7.0 -- Business as Usual"
#         in orig_ds.all_sims
#     ), "The format of all simulation names should include additional SSP text."

#     # Assert that the simulation dimension gets changed correctly and test across both potential inputs for `separate_files`.
#     # Re-assigining orig_ds to the original dataset because it gets mutated during each iteration of `_clean_wl_data`
#     orig_ds = test_dataset_WL_Alamedacounty_45km_hourly.copy()
#     ds = _clean_wl_data(orig_ds, "Dynamical", True)
#     assert (
#         "all_sims" not in ds.dims and "simulation" in ds.dims
#     ), "Simulation dimension should be renamed."
#     assert (
#         len(ds["simulation"]) == num_sims
#     ), "Number of simulations should not be changed."

#     # Make sure descriptions for SSPs are removed
#     def substring_exists(substring, lst):
#         return any(substring in string for string in lst)

#     assert not substring_exists("Business as Usual", ds["simulation"])
#     assert (
#         "WRF_CESM2_r11i1p1f1_Historical + SSP 3-7.0" in ds.simulation
#     ), "The format of the simulation name should no longer include additional SSP text."


# def test_filter_ba_models(test_dataset_WL_Alamedacounty_45km_hourly):
#     """
#     Test `_filter_ba_models` function to ensure it correctly filters simulations based on input parameters.

#     The test asserts that:
#     1. Simulations are not filtered out under non-filtering conditions.
#     2. The correct simulation is filtered out under filtering conditions.

#     Parameters:
#     test_dataset_WL_Alamedacounty_45km_hourly (xarray.Dataset): Test dataset used for filtering.
#     """
#     # Rename `all_sims` to `simulation`
#     orig_ds = test_dataset_WL_Alamedacounty_45km_hourly.copy()
#     orig_ds = orig_ds.rename({"all_sims": "simulation"})

#     # Assert that simulations are not filtered on other param inputs
#     orig_ds_len = len(orig_ds.simulation)
#     ds = _filter_ba_models(orig_ds, "Statistical", True, "Historical Climate")
#     assert len(ds.simulation) == orig_ds_len, "No simulations should be changed."
#     ds = _filter_ba_models(orig_ds, "Dynamical", False, "Historical Climate")
#     assert len(ds.simulation) == orig_ds_len, "No simulations should be changed."
#     ds = _filter_ba_models(orig_ds, "Dynamical", True, "Historical Reconstruction")
#     assert len(ds.simulation) == orig_ds_len, "No simulations should be changed."

#     # Assert that simulations are actually filtered out
#     ds = _filter_ba_models(orig_ds, "Dynamical", True, "Historical Climate")
#     wrf_bc_sims_45km = (
#         "WRF_EC-Earth3_r1i1p1f1_Historical + SSP 3-7.0 -- Business as Usual"
#     )
#     assert (
#         wrf_bc_sims_45km in ds.simulation.values and len(ds.simulation.values) == 1
#     ), "BC-WRF EC-Earth3 simulation should be in the post-filtered data AND the only simulation left for 45km WRF data."


# def test_metric_agg(test_dataset_time_wrf_single_cell_3km_hourly_2030_2035):
#     """
#     Test `_metric_agg` function to ensure it correctly aggregates metrics over time or space.

#     The test verifies that:
#     1. The output type is an xarray DataArray.
#     2. The output shape and dimensions are as expected.
#     3. Values are within expected ranges.
#     4. The new name of the DataArray is correctly assigned.

#     Parameters:
#     test_dataset_time_wrf_single_cell_3km_hourly_2030_2035 (xarray.Dataset): Test dataset used for metric aggregation.
#     """
#     # Grab dataset and number of simulations
#     ds = test_dataset_time_wrf_single_cell_3km_hourly_2030_2035.copy()
#     num_sims = len(ds.simulation)
#     new_name = "test_name"

    # Heat Index test
    calc_val = _metric_agg(
        ds,
        "Air Temperature at 2m",
        np.arange(1, 13),
        "time",
        "max",
        new_name,
        91,
        None,
        None,
        (1, "day"),
        "gev",
    )

    assert (
        type(calc_val) == xr.core.dataarray.DataArray
    ), "The output type of `metric_agg` should be an xarray DataArray."
    assert calc_val.shape == (
        1,
        num_sims,
    ), "This is not an expected output shape of `metric_agg`."
    assert (
        "simulation" in calc_val.dims
    ), "Simulation should remain in the expected output of `metric_agg`."
    assert (
        "time" not in calc_val.dims
    ), "Time should not be in the dimensions after `metric_agg` is calculated."
    assert np.all(
        (calc_val >= 0) & (calc_val < 366)
    ), "All values should be between 0 and 366 (no above heat index days to only above heat index days)"
    assert (
        calc_val.name == new_name
    ), "New name of DataArray should be carried on from argument."

    # Percentile test
    calc_val = _metric_agg(
        ds,
        "Air Temperature at 2m",
        np.arange(1, 13),
        "time",
        "max",
        new_name,
        None,
        95,
        None,
        (1, "day"),
        "gev",
    )

    assert (
        type(calc_val) == xr.core.dataarray.DataArray
    ), "The output type of `metric_agg` should be an xarray DataArray."
    assert calc_val.shape == (
        1,
        num_sims,
    ), "This is not an expected output shape of `metric_agg`."
    assert (
        "simulation" in calc_val.dims
    ), "Simulation should remain in the expected output of `metric_agg`."
    assert (
        "time" not in calc_val.dims
    ), "Time should not be in the dimensions after `metric_agg` is calculated."
    assert (
        "lat" in calc_val.coords and "lon" in calc_val.coords
    ), "Lat and lon dimensions should exist after percentile `metric_calc`."
    assert np.all(
        calc_val < ds.max()
    ), "95th yearly percentiles should all be below the max of the entire DataArray."
    assert (
        calc_val.name == new_name
    ), "New name of DataArray should be carried on from argument."

## Need to test:

# variable: Air Temp, Heat Index, Precip
# approach: time, WL
# downscaling_method: Dynamical, Statistical
# season? only 1
# wrf_bias_adj
# separate_files: True, False
# batch_mode: True, False

### CALVIN: Need to mock DataParameters so that tests don't take forever

inputs = [
    (
        "Air Temperature at 2m",
        "Dynamical",
        "time",
        "Historical Climate",
        ["SSP3-7.0"],
        "1.5",
        "min",
        "summer",
        "both",
        False,
        "test_dataset_time_wrf_single_cell_3km_hourly_2030_2035",
    ),
    (
        "Air Temperature at 2m",
        "Statistical",
        "time",
        "Historical Climate",
        ["SSP3-7.0"],
        "1.5",
        "max",
        "all",
        "both",
        False,
        "test_dataset_time_loca_single_cell_3km_hourly_2030_2035",
    ),
    # ("Air Temperature at 2m", "Dynamical", "warming_level", "Historical Climate", ["SSP3-7.0"], "2.0", "min", "all", "calculate", False, "test_dataset_wl_wrf_single_cell_3km_hourly_2030_2035"),
    # ("Air Temperature at 2m", "Statistical", "warming_level", "Historical Climate", ["SSP3-7.0"], "2.0", "max", "summer", "raw", True, "test_dataset_time_single_cell_3km_hourly_2030_2035"),
    (
        "NOAA Heat Index",
        "Dynamical",
        "time",
        "Historical Climate",
        ["SSP3-7.0"],
        "1.5",
        "min",
        "summer",
        "both",
        False,
        "test_dataset_time_wrf_single_cell_noaa_heat_index_3km_hourly_2030_2035",
    ),
    # ("NOAA Heat Index", "Dynamical", "warming_level", "Historical Climate", ["SSP3-7.0"], "2.0", "min", "winter", "both", False, test_dataset_time_single_cell_3km_hourly_2030_2035),
]


@pytest.mark.parametrize(
    "variable, downscaling_method, approach, historical_data, ssp_data, warming_level, metric_calc, season, export_method, batch_mode, test_dataset_fixture",
    inputs,
)
def test_cava_data_params(
    request,
    variable,
    downscaling_method,
    approach,
    historical_data,
    ssp_data,
    warming_level,
    metric_calc,
    season,
    export_method,
    batch_mode,
    test_dataset_fixture,
):  # , downscaling_method, approach, metric_calc):

    test_dataset = request.getfixturevalue(test_dataset_fixture)
    if approach == "time":
        patch_target = "climakitae.core.data_interface.DataParameters.retrieve"
        return_value = test_dataset
    elif approach == "warming_level":
        patch_target = "climakitae.explore.warming.WarmingLevels.calculate"
        return_value = None  # {warming_level: test_dataset}

    with (
        patch(patch_target, return_value=return_value),
        # patch.object(climakitae.explore.warming.WarmingLevels(), 'sliced_data', {warming_level: test_dataset}),
        patch("climakitae.explore.vulnerability._export_no_e", return_value=None),
    ):
        input_locations = pd.DataFrame({"lat": [35.43424], "lon": [-119.05524]})

        start_year, end_year = 2030, 2035
        result = cava_data(
            ## Set-up
            input_locations.iloc[:1],  # select a single location
            time_start_year=start_year,
            time_end_year=end_year,
            warming_level=warming_level,
            downscaling_method=downscaling_method,  # WRF data
            approach=approach,
            historical_data=historical_data,
            ssp_data=ssp_data,
            wrf_bias_adjust=False,  # return all WRF models
            ## Likely seaonal event specific arguments
            variable=variable,
            metric_calc=metric_calc,  # daily high temperature
            percentile=75,  # likeliness percentile
            season=season,  # season
            units="degF",  # change units
            ## Export
            export_method="both",  # export only calculated metric data
            file_format="NetCDF",
        )

        def check_raw_output(result):
            days_by_season = {
                "winter": 90,
                "summer": 92,
                "all": 365,
            }
            assert "raw_data" in result
            raw_res = result["raw_data"]
            if downscaling_method == "Dynamical":
                assert raw_res.shape == (
                    1,
                    8,
                    (end_year - start_year + 1) * days_by_season[season] * 24,
                )  # Finding the total number of hours after filtering for specific seasons
                assert raw_res.name == variable
            elif downscaling_method == "Statistical":
                assert raw_res.shape == (
                    1,
                    62,
                    (end_year - start_year + 1) * days_by_season[season],
                )  # Finding the total number of hours after filtering for specific seasons

                # Make sure that raw dataset pulls the correct, re-defined LOCA variable for `Air Temperature at 2m`
                if metric_calc == "max" and variable == "Air Temperature at 2m":
                    assert raw_res.name == "Maximum air temperature at 2m"
                elif metric_calc == "min" and variable == "Air Temperature at 2m":
                    assert raw_res.name == "Minimum air temperature at 2m"

        def check_calculate_output(result):
            assert "calc_data" in result
            calc_res = result["calc_data"]
            # Check for shape and coords depending on downscaling method
            if downscaling_method == "Dynamical":
                if ssp_data == ["SSP 3-7.0"]:
                    assert (
                        calc_res.scenario.item()
                        == "Historical + SSP 3-7.0 -- Business as Usual"
                    )
                    assert calc_res.shape == (1, 8)
                assert calc_res.frequency == "hourly"
                assert np.isclose(calc_res.lat.item(), 35.45, atol=0.01)
                assert np.isclose(calc_res.lon.item(), -119.06, atol=0.01)
                assert np.isclose(calc_res.x.item(), -4089113.66, atol=1)
                assert np.isclose(calc_res.y.item(), 1015913.12, atol=1)

            elif downscaling_method == "Statistical":
                if ssp_data == ["SSP 3-7.0"]:
                    assert (
                        calc_res.scenario.item()
                        == "Historical + SSP 3-7.0 -- Business as Usual"
                    )
                    assert calc_res.shape == (1, 62)
                assert calc_res.frequency == "daily"
                assert np.isclose(calc_res.lat.item(), 35.42, atol=0.01)
                assert np.isclose(calc_res.lon.item(), -119.05, atol=0.01)

            if ssp_data == ["SSP 3-7.0"]:
                assert (
                    calc_res.scenario.item()
                    == "Historical + SSP 3-7.0 -- Business as Usual"
                )

            assert calc_res.resolution == "3 km"
            assert calc_res.units == "degF"
            # assert calc_res.name == '75th percentile of Daily Min of Air Temperature at 2m for summer seasons from 2030_to_2035'
            assert calc_res.percentile.item() == 75

        # Check for appropriate data structures
        assert isinstance(result, dict)

        if export_method == "raw":
            check_raw_output(result)

        elif export_method == "calculate":
            check_calculate_output(result)

        elif export_method == "both":
            check_raw_output(result)
            check_calculate_output(result)

    # One-in-X test
    calc_val = _metric_agg(
        ds,
        "Air Temperature at 2m",
        np.arange(1, 13),
        "time",
        "max",
        new_name,
        None,
        None,
        10,
        (1, "day"),
        "gev",
    )

    assert (
        type(calc_val) == xr.core.dataarray.DataArray
    ), "The output type of `metric_agg` should be an xarray DataArray."
    assert calc_val.shape == (
        num_sims,
    ), "This is not an expected output shape of `metric_agg`."
    assert (
        "simulation" in calc_val.dims
    ), "The expected output dimension of `metric_agg` 1-in-X has `simulation`."
    assert (
        calc_val.name == new_name
    ), "New name of DataArray should be carried on from argument."

    # if batch mode:
        # Check batch_mode size
        # Add tests on CAVAParams
        # Check 'lat', 'lon' data var names


##### NOT DONE #####

# @pytest.fixture
# def mock_input_locations():
#     """Fixture for creating a mock input locations DataFrame."""
#     return read_csv_file(stations_csv_path, index_col=0)[
#         ["station", "LAT_Y", "LON_X"]
#     ].rename(columns={"LAT_Y": "lat", "LON_X": "lon"})


# def test_clean_wl_data(test_dataset_WL_Alamedacounty_45km_hourly):
#     """Test on `_clean_wl_data` with separate_files=False"""
#     return


# Test cases

# downscaling method (2), approach (2), WL (4), ssp (3), historical (2), wrf_bias_adjust (2), variable (3), metric_calc (4), export_method (3), batch_mode (2)

# At least 10 ^^
