import pytest
import os
import numpy as np
import pandas as pd
import xarray as xr
from climakitae.explore.vulnerability import (
    _export_no_e,
    _filter_ba_models,
    _metric_agg,
    cava_data,
)
from unittest.mock import patch


# Test `export_no_e` on a series of potential exports.
def test_export_no_e(test_data_2022_monthly_45km):
    """
    Test `_export_no_e` function to ensure it correctly handles exporting data to a file.

    The test verifies that:
    1. The function creates a file when it doesn't exist.
    2. The function does not modify an existing file when called again.
    3. The file is deleted correctly after the test.

    Parameters:
    test_data_2022_monthly_45km (xarray.Dataset): Test dataset used for export.
    """
    # Export dataset
    export_path = os.path.join(
        os.path.dirname(os.path.abspath(__file__)), "random_file.nc"
    )
    _export_no_e(test_data_2022_monthly_45km, export_path, "NetCDF")

    # Assert path exists
    assert os.path.exists(export_path), "The file should be created."

    # Try exporting it again
    _export_no_e(test_data_2022_monthly_45km, export_path, "NetCDF")

    # Check that the file still exists and has not been modified
    assert os.path.exists(export_path), "The file should still exist."

    # Delete exported file
    os.remove(export_path)
    assert not os.path.exists(export_path), "The file should have been deleted."


def test_filter_ba_models(test_dataarray_wl_20_all_season_wrf_3km_hourly_temp):
    """
    Test `_filter_ba_models` function to ensure it correctly filters simulations based on input parameters.

    The test asserts that:
    1. Simulations are not filtered out under non-filtering conditions.
    2. The correct simulation is filtered out under filtering conditions.

    Parameters:
    test_dataarray_wl_20_all_season_wrf_3km_hourly_temp (xarray.Dataset): Test dataset used for filtering.
    """
    # Rename `all_sims` to `simulation`
    orig_ds = test_dataarray_wl_20_all_season_wrf_3km_hourly_temp.copy()

    # Assert that simulations are not filtered on other param inputs
    orig_ds_len = len(orig_ds.simulation)
    ds = _filter_ba_models(orig_ds, "Statistical", True, "Historical Climate")
    assert len(ds.simulation) == orig_ds_len, "No simulations should be changed."
    ds = _filter_ba_models(orig_ds, "Dynamical", False, "Historical Climate")
    assert len(ds.simulation) == orig_ds_len, "No simulations should be changed."
    ds = _filter_ba_models(orig_ds, "Dynamical", True, "Historical Reconstruction")
    assert len(ds.simulation) == orig_ds_len, "No simulations should be changed."

    # Assert that simulations are actually filtered out
    ds = _filter_ba_models(orig_ds, "Dynamical", True, "Historical Climate")
    wrf_bc_sims_45km = np.array(
        [
            "WRF_EC-Earth3_r1i1p1f1_historical+ssp370",
            "WRF_EC-Earth3-Veg_r1i1p1f1_historical+ssp370",
            "WRF_MIROC6_r1i1p1f1_historical+ssp370",
            "WRF_MPI-ESM1-2-HR_r3i1p1f1_historical+ssp370",
            "WRF_TaiESM1_r1i1p1f1_historical+ssp370",
        ]
    )
    assert np.array_equal(
        ds.simulation.values, wrf_bc_sims_45km
    ), "Bias adjusted models are not equal"


def test_metric_agg(test_dataarray_time_2030_2035_wrf_3km_hourly_temp):
    """
    Test `_metric_agg` function to ensure it correctly aggregates metrics over time or space.

    The test verifies that:
    1. The output type is an xarray DataArray.
    2. The output shape and dimensions are as expected.
    3. Values are within expected ranges.
    4. The new name of the DataArray is correctly assigned.

    Parameters:
    test_dataarray_time_2030_2035_wrf_3km_hourly_temp (xarray.Dataset): Test dataset used for metric aggregation.
    """
    # Grab dataset and number of simulations
    ds = test_dataarray_time_2030_2035_wrf_3km_hourly_temp.copy()
    num_sims = len(ds.simulation)
    new_name = "test_name"

    # Heat Index test
    calc_val = _metric_agg(
        ds,
        "Air Temperature at 2m",
        np.arange(1, 13),
        "time",
        "max",
        new_name,
        91,
        None,
        np.array([None]),
        (1, "day"),
        "gev",
    )

    assert (
        type(calc_val) == xr.core.dataarray.DataArray
    ), "The output type of `metric_agg` should be an xarray DataArray."

    assert calc_val.shape == (
        1,  # scenario
        num_sims,  # simulation
        1,  # y
        1,  # x
    ), "This is not an expected output shape of `metric_agg`."
    assert (
        "simulation" in calc_val.dims
    ), "Simulation should remain in the expected output of `metric_agg`."
    assert (
        "time" not in calc_val.dims
    ), "Time should not be in the dimensions after `metric_agg` is calculated."
    assert np.all(
        (calc_val >= 0) & (calc_val < 366)
    ), "All values should be between 0 and 366 (no above heat index days to only above heat index days)"
    assert (
        calc_val.name == new_name
    ), "New name of DataArray should be carried on from argument."

    # Percentile test
    calc_val = _metric_agg(
        ds,
        "Air Temperature at 2m",
        np.arange(1, 13),
        "time",
        "max",
        new_name,
        None,
        95,
        np.array([None]),
        (1, "day"),
        "gev",
    )

    assert (
        type(calc_val) == xr.core.dataarray.DataArray
    ), "The output type of `metric_agg` should be an xarray DataArray."
    assert calc_val.shape == (
        1,  # scenario
        num_sims,  # simulation
        1,  # y
        1,  # x
    ), "This is not an expected output shape of `metric_agg`."
    assert (
        "simulation" in calc_val.dims
    ), "Simulation should remain in the expected output of `metric_agg`."
    assert (
        "time" not in calc_val.dims
    ), "Time should not be in the dimensions after `metric_agg` is calculated."
    assert (
        "lat" in calc_val.coords and "lon" in calc_val.coords
    ), "Lat and lon dimensions should exist after percentile `metric_calc`."
    assert np.all(
        calc_val < ds.max()
    ), "95th yearly percentiles should all be below the max of the entire DataArray."
    assert (
        calc_val.name == new_name
    ), "New name of DataArray should be carried on from argument."

    # One-in-X test
    calc_val = _metric_agg(
        ds,
        "Air Temperature at 2m",
        np.arange(1, 13),
        "time",
        "max",
        new_name,
        None,
        None,
        np.array([10]),
        (1, "day"),
        "gev",
    )

    assert (
        type(calc_val) == xr.core.dataarray.Dataset
    ), "The output type of `metric_agg` should be an xarray Dataset."
    assert calc_val["return_value"].shape == (
        num_sims,
        1,
    ), "This is not an expected output shape of the `return_value` data variable for `calc_val`."
    assert calc_val["p_values"].shape == (
        num_sims,
    ), "This is not an expected output shape of the `p_values` data variable for `calc_val`."
    assert (
        "simulation" in calc_val.dims
    ), "The expected output dimension of `metric_agg` 1-in-X has `simulation`."

    ### This last assertion does not apply to a returned xr.Dataset
    # assert (
    #     calc_val.name == new_name
    # ), "New name of DataArray should be carried on from argument."

    # One-in-X test for multiple 1-in-X values
    one_in_x_vals = np.array([10, 100])
    calc_val = _metric_agg(
        ds,
        "Air Temperature at 2m",
        np.arange(1, 13),
        "time",
        "max",
        new_name,
        None,
        None,
        one_in_x_vals,
        (1, "day"),
        "gev",
    )

    assert (
        type(calc_val) == xr.core.dataarray.Dataset
    ), "The output type of `metric_agg` should be an xarray Dataset."
    assert calc_val["return_value"].shape == (
        num_sims,
        len(one_in_x_vals),
    ), "This is not an expected output shape of the `return_value` data variable for `calc_val`."
    assert calc_val["p_values"].shape == (
        num_sims,
    ), "This is not an expected output shape of the `p_values` data variable for `calc_val`."
    assert (
        "simulation" in calc_val.dims
    ), "The expected output dimension of `metric_agg` 1-in-X has `simulation`."


### Listing out different parameters to test `cava_data` on
inputs = [
    ## WRF Time
    (
        "Air Temperature at 2m",
        "Dynamical",
        "Time",
        "Historical Climate",
        ["SSP 3-7.0"],
        False,
        1.5,
        "min",
        "all",
        "both",
        False,
        "test_dataarray_time_2030_2035_wrf_3km_hourly_temp",
    ),
    ### LOCA Time
    (
        "Air Temperature at 2m",
        "Statistical",
        "Time",
        "Historical Climate",
        ["SSP 3-7.0"],
        False,
        1.5,
        "max",
        "all",
        "both",
        False,
        "test_dataarray_time_2030_2035_loca_3km_daily_temp",
    ),
    ### WRF WL
    (
        "Air Temperature at 2m",
        "Dynamical",
        "Warming Level",
        "Historical Climate",
        ["SSP 3-7.0"],
        False,
        2.0,
        "min",
        "all",
        "calculate",
        False,
        "test_dataarray_wl_20_all_season_wrf_3km_hourly_temp",
    ),
    ### LOCA WL all seasons
    (
        "Air Temperature at 2m",
        "Statistical",
        "Warming Level",
        "Historical Climate",
        ["SSP 3-7.0"],
        False,
        2.0,
        "max",
        "all",
        "raw",
        False,
        "test_dataarray_wl_20_all_season_loca_3km_daily_temp",
    ),
    # LOCA WL summer
    (
        "Air Temperature at 2m",
        "Statistical",
        "Warming Level",
        "Historical Climate",
        ["SSP 3-7.0"],
        False,
        2.0,
        "max",
        "summer",
        "raw",
        False,
        "test_dataarray_wl_20_summer_season_loca_3km_daily_temp",
    ),
    # ## NOAA Heat Index test
    (
        "NOAA Heat Index",
        "Dynamical",
        "Time",
        "Historical Climate",
        ["SSP 3-7.0"],
        True,
        1.5,
        "min",
        "all",
        "both",
        False,
        "test_dataarray_time_2030_2035_wrf_3km_hourly_heat_index",
    ),
    # ### Historical Reconstruction, no batch
    # (
    #     "Air Temperature at 2m",
    #     "Dynamical",
    #     "Time",
    #     "Historical Reconstruction",
    #     ["SSP 3-7.0"],
    #     False,
    #     1.5,
    #     "max",
    #     "all",
    #     "both",
    #     False,
    #     "test_dataarray_time_2010_2015_histrecon_wrf_3km_hourly_temp_single_cell",
    # ),
    ## Historical Reconstruction, batch
    (
        "Air Temperature at 2m",
        "Dynamical",
        "Time",
        "Historical Reconstruction",
        ["SSP 3-7.0"],
        False,
        1.5,
        "max",
        "all",
        "both",
        True,
        "test_dataarray_time_2010_2015_histrecon_wrf_3km_hourly_temp_gridded_area",
    ),
    ### Precipitation test
    (
        "Precipitation (total)",
        "Dynamical",
        "Time",
        "Historical Climate",
        ["SSP 3-7.0"],
        False,
        2.0,
        "max",
        "all",
        "both",
        False,
        "test_dataarray_time_2030_2035_wrf_3km_hourly_prec",
    ),
]


@pytest.mark.parametrize(
    "variable, downscaling_method, approach, historical_data, ssp_data, wrf_bias_adjust, warming_level, metric_calc, season, export_method, batch_mode, test_dataset_fixture",
    inputs,
)
def test_cava_data_params(
    request,
    variable,
    downscaling_method,
    approach,
    historical_data,
    ssp_data,
    wrf_bias_adjust,
    warming_level,
    metric_calc,
    season,
    export_method,
    batch_mode,
    test_dataset_fixture,
):
    """
    Test the `cava_data` function under various inputs to ensure it
    returns correctly structured output data for different parameters.

    This test covers a variety of scenarios based on downscaling methods, historical data types,.
    approaches, batch mode, variables, and seasonality. It performs assertions on data shapes,
    names, and values based on the inputs, verifies both raw and calculated outputs
    and checks export export configurations.

    Parameters:
    -----------
    request : fixture
        A pytest fixture request used to retrieve test data.
    variable : str
        The climate variable being tested, (e.g. "Air Temperature at 2m", "Precipitation (total)" or "NOAA Heat Index"
    downscaling_method : str
        Method of downscaling, either "Dynamical" or "Statistical."
    approach : str
        Climate modeling approach, either "Time" or "Warming Level."
    historical_data : str
        Type of historical data used, e.g., "Historical Climate" or
        "Historical Reconstruction."
    ssp_data : list of str
        List of SSP scenarios, e.g., ["SSP 3-7.0"].
    wrf_bias_adjust : bool
        Whether bias adjustment is applied to WRF data.
    warming_level : float
        Warming level used in the model, if applicable.
    metric_calc : str
        Metric calculation method, e.g., "max" or "min."
    season : str
        Season filter for the data, e.g., "summer," "winter," or "all."
    export_method : str
        Export method for the output data, either "raw," "calculate," or "both."
    batch_mode : bool
        Whether to process data with batch mode or through iterating through the points.
    test_dataset_fixture : str
        Name of the fixture that provides the test DataArray to mock the data retrieval function.

    Returns:
    --------
    None

    Overview of checks:
    ----------------
    1. Checks raw data shape based on downscaling method and approach.
    2. Checks calculated data for attributes such as shape, latitude, longitude,
       scenario, resolution, units, and percentile.
    3. Validates data names for specific variables under certain metrics.
    """
    test_dataset = request.getfixturevalue(test_dataset_fixture)

    with (
        patch(
            "climakitae.core.data_interface.DataParameters.retrieve",
            return_value=test_dataset,
        ),
        patch("climakitae.explore.vulnerability._export_no_e", return_value=None),
    ):
        input_locations = pd.DataFrame(
            {"lat": [35.43424, 35.4], "lon": [-119.05524, -119.02176]}
        )

        batch = 2 if batch_mode else 1

        if historical_data == "Historical Climate":
            start_year, end_year = 2030, 2035
        elif historical_data == "Historical Reconstruction":
            start_year, end_year = 2010, 2015

        result = cava_data(
            ## Set-up
            input_locations.iloc[:batch],  # select a single location
            time_start_year=start_year,
            time_end_year=end_year,
            warming_level=warming_level,
            downscaling_method=downscaling_method,  # WRF data
            approach=approach,
            historical_data=historical_data,
            ssp_data=ssp_data,
            wrf_bias_adjust=False,  # return all WRF models
            batch_mode=batch_mode,
            ## Likely seaonal event specific arguments
            variable=variable,
            metric_calc=metric_calc,  # daily high temperature
            percentile=75,  # likeliness percentile
            season=season,  # season
            units="degF",  # change units
            ## Export
            export_method="both",  # export only calculated metric data
            file_format="NetCDF",
        )

        def check_raw_output(result):
            days_by_season = {
                "winter": 90,
                "summer": 92,
                "all": 365,
            }
            assert "raw_data" in result
            raw_res = result["raw_data"]
            if downscaling_method == "Dynamical":
                if approach == "Time":
                    if historical_data == "Historical Reconstruction":
                        assert raw_res.shape == (
                            (end_year - start_year + 1) * days_by_season[season] * 24,
                            batch,
                        )  # Finding the total number of hours after filtering for specific seasons
                    elif wrf_bias_adjust == True:
                        num_sims = 5 * batch
                        assert raw_res.shape == (
                            1,
                            num_sims,
                            (end_year - start_year + 1) * days_by_season[season] * 24,
                        )  # Finding the total number of hours after filtering for specific seasons
                    elif wrf_bias_adjust == False:
                        num_sims = 8 * batch
                        assert raw_res.shape == (
                            1,
                            num_sims,
                            (end_year - start_year + 1) * days_by_season[season] * 24,
                        )  # Finding the total number of hours after filtering for specific seasons
                elif approach == "Warming Level":
                    assert raw_res.shape == (
                        1,
                        8 * batch,
                        30
                        * days_by_season[season]
                        * 24,  # Pre-determined as 30-year WL slices; finding the total number of hours
                    )
                assert raw_res.name == variable
            elif downscaling_method == "Statistical":
                if approach == "Time":
                    assert raw_res.shape == (
                        1,
                        62 * batch,
                        (end_year - start_year + 1) * days_by_season[season],
                    )  # Finding the total number of hours after filtering for specific seasons
                elif approach == "Warming Level":
                    assert raw_res.shape == (
                        1,
                        30
                        * days_by_season[
                            season
                        ],  # Pre-determined as 30-year WL slices; finding the total number of days
                        129 * batch,
                    )
                # Make sure that raw dataset pulls the correct, re-defined LOCA variable for `Air Temperature at 2m`
                if metric_calc == "max" and variable == "Air Temperature at 2m":
                    assert raw_res.name == "Maximum air temperature at 2m"
                elif metric_calc == "min" and variable == "Air Temperature at 2m":
                    assert raw_res.name == "Minimum air temperature at 2m"

        def check_calculate_output(result):
            assert "calc_data" in result
            calc_res = result["calc_data"]
            # Check for shape and coords depending on downscaling method
            if downscaling_method == "Dynamical":
                if (
                    approach == "Time"
                    and historical_data != "Historical Reconstruction"
                ):
                    if ssp_data == ["SSP 3-7.0"]:
                        assert calc_res.scenario.item() == "Historical + SSP 3-7.0"
                if historical_data == "Historical Reconstruction":
                    assert calc_res.shape == (1 * batch,)
                elif not wrf_bias_adjust:
                    assert calc_res.shape == (1, 8 * batch)
                elif wrf_bias_adjust:
                    assert calc_res.shape == (1, 5 * batch)
                assert calc_res.frequency == "hourly"
                if not batch_mode:
                    assert np.isclose(calc_res.lat.item(), 35.43, atol=0.02)
                    assert np.isclose(calc_res.lon.item(), -119.04, atol=0.02)
                    assert np.isclose(calc_res.x.item(), -4089113.66, atol=5000)
                    assert np.isclose(calc_res.y.item(), 1012911.12, atol=5000)
                else:
                    assert np.all(
                        np.isclose(calc_res.lat, [35.445045, 35.400005], atol=0.1)
                    )
                    assert np.all(
                        np.isclose(calc_res.lon, [-119.06061, -119.02176], atol=0.1)
                    )
                    assert np.all(
                        np.isclose(
                            calc_res.x,
                            [-4089113.66186097, -4089113.66186097],
                            atol=5000,
                        )
                    )
                    assert np.all(
                        np.isclose(
                            calc_res.y, [1015911.73069854, 1009911.73069854], atol=5000
                        )
                    )

            elif downscaling_method == "Statistical":
                if (
                    approach == "Time"
                    and historical_data != "Historical Reconstruction"
                ):
                    if ssp_data == ["SSP 3-7.0"]:
                        assert calc_res.scenario.item() == "Historical + SSP 3-7.0"
                    assert calc_res.shape == (1, 62)
                elif approach == "Warming Level":
                    assert calc_res.shape == (1, 129)
                assert calc_res.frequency == "daily"
                assert np.isclose(calc_res.lat.item(), 35.42, atol=0.01)
                assert np.isclose(calc_res.lon.item(), -119.05, atol=0.01)

            assert calc_res.resolution == "3 km"
            if (
                calc_res.variable_id == "t2"
                or calc_res.variable_id == "tasmax"
                or calc_res.variable_id == "noaa_heat_index_derived"
            ):
                assert calc_res.units == "degF"
            elif calc_res.variable_id == "prec":
                assert calc_res.units == "inches"
            # assert calc_res.name == '75th percentile of Daily Min of Air Temperature at 2m for summer seasons from 2030_to_2035'
            assert calc_res.percentile.item() == 75

        # Check for appropriate data structures
        assert isinstance(result, dict)

        if export_method == "raw":
            check_raw_output(result)

        elif export_method == "calculate":
            check_calculate_output(result)

        elif export_method == "both":
            check_raw_output(result)
            check_calculate_output(result)
